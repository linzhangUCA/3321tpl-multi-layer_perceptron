{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron Neural Network\n",
    "\n",
    "In this assignment, We will construct and train a multi-layer perceptron (MLP) model to categorize pictures into 10 classes.\n",
    "## Background\n",
    "\n",
    "![mlp](./mlp.png)\n",
    "\n",
    "An Multi-Layer Perceptron (MLP) is a classic type of feedforward artificial neural network (ANN). \n",
    "It serves as the foundation for many more complex deep learning architectures.\n",
    "An MLP is structured in a series of layers:\n",
    "- Input Layer: This is where the network receives your raw data or features.\n",
    "- Hidden Layer(s): One or more layers of neurons (or nodes) positioned between the input and output. \n",
    "This is where the \"learning\" and complex computation primarily occur. \n",
    "A network with multiple hidden layers is what makes it \"deep.\"\n",
    "- Output Layer: This final layer produces the network's prediction, such as a class probability or a continuous value.\n",
    "\n",
    "In an MLP, each neuron in one layer is typically fully connected to every neuron in the next. \n",
    "The power of the MLP comes from its non-linear activation functions (like ReLU or sigmoid) applied within the hidden neurons. \n",
    "These functions allow the network to learn and model complex, non-linear patterns in data. \n",
    "\n",
    "MLP itself has a wide an a variety of applications (e.g. image classification, fraud detection, spam filtering, etc..).\n",
    "It also serves as key components in today's larger scale AI models such as in Convolutional Neural Network (CNN), Deep Reinforcement Learning (DRL), Large Language Model (LLM), etc..\n",
    "  \n",
    "## Objectives:\n",
    "- Review data preprocessing techniques (rescale, flatten, one-hot encoding, etc.).\n",
    "- Practice MLP construction with arbitrary hidden layer dimensions.\n",
    "- Realize a variety of activation functions (`sigmoid`, `tanh`, `relu`, `leaky_relu`).\n",
    "- Realize the special softmax activation which formats features into categorical probability distributions.\n",
    "- Ultilize cross entropy function to evaluate a general classification model.  \n",
    "- Apply back propagation to a multi-layer gradients computation.  \n",
    "- Reinforce the of model training process.\n",
    "\n",
    "## Exercises:\n",
    "<font color=582c83>\n",
    "\n",
    "1. (5%) Parameters Initialization\n",
    "2. (5%) Activation Functions\n",
    "3. (10%) MLP Forward Pass Construction\n",
    "4. (10%) Cross Entropy Loss and Classification Accuracy\n",
    "5. (4%) Derivatives of Activation Functions\n",
    "6. (18%) Gradients of Loss Back-Propagation\n",
    "7. (33%) Stochastic Gradient Descent (SGD) Optimization\n",
    "8. (15%) Model Analyses\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Data\n",
    "The CIFAR-10 dataset (Canadian Institute For Advanced Research) is a collection of images that are commonly used to train machine learning and computer vision algorithms. \n",
    "It consists of 60000 32x32 color images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "Torchvision provides a descent pool of datasets and [CIFAR10](https://docs.pytorch.org/vision/main/generated/torchvision.datasets.CIFAR10.html) is one of them.\n",
    "Wee will investigate a multi-class classification problem using this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms.v2 import Compose, ToImage, ToDtype\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Construct pre-processing pipeline\n",
    "feature_transform_pipeline = Compose([\n",
    "    ToImage(),\n",
    "    ToDtype(torch.float32, scale=True),\n",
    "    torch.flatten,\n",
    "    torch.Tensor.numpy,\n",
    "])\n",
    "label_transform_pipeline = Compose([\n",
    "    lambda x:torch.tensor([x]),\n",
    "    lambda x:torch.nn.functional.one_hot(x,10),\n",
    "    torch.squeeze,\n",
    "    torch.Tensor.numpy,\n",
    "])\n",
    "\n",
    "# Download datasets\n",
    "dataset_train = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=feature_transform_pipeline,\n",
    "    target_transform = label_transform_pipeline,\n",
    ")\n",
    "\n",
    "dataset_val = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=feature_transform_pipeline,\n",
    "    target_transform = label_transform_pipeline,\n",
    ")\n",
    "print(f\"Number of training samples: {len(dataset_train)}\")\n",
    "print(f\"Number of validation samples: {len(dataset_val)}\")\n",
    "print(f\"Features shape: {dataset_train[0][0].shape}\")\n",
    "print(f\"Classes: {dataset_train.classes}\")\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 128\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=100000, shuffle=True)\n",
    "\n",
    "\n",
    "# Visualize data samples\n",
    "sample_batch_train = next(iter(dataloader_train))\n",
    "fig, axs = plt.subplots(1, 12, figsize=(12, 4))\n",
    "for i in range(12):\n",
    "    sample_img = np.transpose(sample_batch_train[0][i].numpy().reshape(3, 32, 32), (1, 2, 0))  # reconstruct image to (H, W, C) format\n",
    "    sample_cls = dataset_train.classes[sample_batch_train[1][i].numpy().argmax()]\n",
    "    axs[i] = plt.subplot(1, 12, i + 1)\n",
    "    axs[i].set_title(sample_cls)\n",
    "    axs[i].axis('off')\n",
    "    axs[i].imshow(sample_img)  # image is already in (H, W, C) format\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construct Multi-Layer Perceptron Model\n",
    "A Multi-Layer Perceptron (MLP) model is featured with multiple layers of features, $\\mathbf{X}^{[l]}$ s and $\\mathbf{Z}^{[l]}$ s. \n",
    "The features of the previous layers, ${[l-1]}$ will be transformed (by a stack of a linear function and a non-linear function) to the features in the next layer, ${[l]} \\text{, where } l \\in {1, 2, \\dots, L}$. \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\mathbf{X}^{[l]} = a(\\mathbf{X}^{[l-1]} \\cdot \\mathbf{W}^{[l]T} + \\mathbf{b}^{[l]})\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The input layer can be defined as $\\mathbf{X}^{[0]}$, and the output layer can be defined as $\\hat{\\mathbf{Y}} = \\mathbf{X}^{[L]}$, where $L$ is the index of the last layer.\n",
    "\n",
    "### 2.1. Initialize Parameters\n",
    "An MLP model is governed by $L$ sets of $\\mathbf{W}^{[l]}$ s and $\\mathbf{b}^{[l]}$ s.\n",
    "Assume $\\mathbf{X}^{[l-1]}$ has $N_{l-1}$ features and $\\mathbf{X}^{[l]}$ has $N_{l}$ features, $\\mathbf{W}^{[l]}$ needs to be an array/matrix with shape $(N_l, N_{l-1})$, and $\\mathbf{b}^{[l]}$ needs to be an array/vector with shape $(1, N_l)$.\n",
    "\n",
    "\n",
    "### <font color=#582c83> (5%) Exercise 1: Parameters Initialization </font>\n",
    "Define a function to initialize weights and biases parameters and save these parameters in a Python **dictionary**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_params(in_dims: int, hidden_dims: tuple, out_dims: int) -> dict:\n",
    "    \"\"\"\n",
    "    Initialize the parameters of the MLP model.\n",
    "    Args:\n",
    "        in_dims: number of input dimensions.\n",
    "        hidden_dims: tuple/list of hidden layer dimensions.\n",
    "        out_dims: number of output dimensions.\n",
    "    Returns:\n",
    "        params: dictionary containing the initialized weights and biases.\n",
    "    \"\"\"\n",
    "    layer_dims = (in_dims, *hidden_dims, out_dims)\n",
    "    params = {}\n",
    "    for l in range(len(layer_dims) - 1):\n",
    "        params[f'W{l+1}'] = np.random.normal(loc=0, scale=0.01, size=(layer_dims[l + 1], layer_dims[l]))\n",
    "        params[f'b{l+1}'] = np.random.normal(loc=0, scale=0.01, size=(1, layer_dims[l + 1]))\n",
    "    return params\n",
    "\n",
    "# Example usage\n",
    "np.random.seed(3321)  # for reproducibility\n",
    "params_dummy = init_params(in_dims=sample_batch_train[0].numpy().shape[1], hidden_dims=(4, 3), out_dims=sample_batch_train[1].numpy().shape[1])\n",
    "# print(f\"Dummy parameter: {params_dummy}\")\n",
    "for key in params_dummy:\n",
    "    print(f\"{key} shape: {params_dummy[key].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "W1 shape: (4, 3072)\n",
    "b1 shape: (1, 4)\n",
    "W2 shape: (3, 4)\n",
    "b2 shape: (1, 3)\n",
    "W3 shape: (10, 3)\n",
    "b3 shape: (1, 10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Activate Features\n",
    "A non-linear activation function will be applied on $\\mathbf{Z}^{[l]}$ to form new features $\\mathbf{X}^{[l]}$ as the $l$-th layer in an MLP model.\n",
    "Commonly use activations are:\n",
    "\n",
    "- Sigmoid\n",
    "$$x = \\sigma (z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "- Hyperbolic Tangent\n",
    "$$x = \\tanh (z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$$\n",
    "\n",
    "- Rectified Linear Unit\n",
    "$$\n",
    "x = \\text{ReLU}(z) = \n",
    "    \\begin{cases}\n",
    "        0   & z \\leq 0 \\\\\n",
    "        z   & z > 0\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "- Leaky ReLU\n",
    "$$\n",
    "x = \\text{LeakyReLU}(z; a \\geq 0) = \n",
    "    \\begin{cases}\n",
    "        az   & z \\leq 0 \\\\\n",
    "        z   & z > 0\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "- Softmax\n",
    "For a multi-class classification problem, we will want to encode $\\mathbf{Z}^{[L]}$ into probability distributions.\n",
    "\n",
    "$$\\hat{\\mathbf{y}}_i = \\frac{e^{\\mathbf{z}^{[L]}_i}}{\\sum^C_{i=1} e^{\\mathbf{z}^{[L]}_i}}$$\n",
    "\n",
    "The maxtrix $\\mathbf{Z}^{[L]}$ has shape: $(M, C)$, where $M$ is the number of samples, $C$ is the number of classes. \n",
    "When applying softmax activation, we only want it to deal with each row of $\\mathbf{Z}^{[L]}$, or perform it on the last dimension (axis) of $\\mathbf{Z}^{[L]}$.\n",
    "\n",
    "### <font color=#582c83> (5%) Exercise 2. Activation Functions </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 9 lines of code)\n",
    "def sigmoid(z):\n",
    "    \"\"\" Sigmoid function\n",
    "    Args:\n",
    "        z: independent variable, could be an arrary of any shape or a scalar. \n",
    "    Returns:\n",
    "        dependent variable, could be an arrary of any shape or a scalar. \n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def tanh(z):\n",
    "    \"\"\" Hyperbolic Tangent function\n",
    "    Args:\n",
    "        z: independent variable, could be an arrary of any shape or a scalar. \n",
    "    Returns:\n",
    "        dependent variable, could be an arrary of any shape or a scalar. \n",
    "    \"\"\"\n",
    "    return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\" ReLU function\n",
    "    Args:\n",
    "        z: independent variable, could be an arrary of any shape or a scalar. \n",
    "    Returns:\n",
    "        dependent variable, could be an arrary of any shape or a scalar. \n",
    "    \"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "\n",
    "def leaky_relu(z, a=0.05):\n",
    "    \"\"\" Leaky ReLU function\n",
    "    Args:\n",
    "        z: independent variable, could be an arrary of any shape or a scalar. \n",
    "        a: constant, a scalar. \n",
    "    Returns:\n",
    "        dependent variable, could be an arrary of any shape or a scalar. \n",
    "    \"\"\"\n",
    "    return np.maximum(a * z, z)\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\" Softmax function\n",
    "    Args:\n",
    "        z: independent variable, 2d array with shape (# samples, # features)\n",
    "    Returns:\n",
    "        probabilities: dependent variable, 2d array with shape (# samples, # features)\n",
    "    \"\"\"\n",
    "    probabilities = np.exp(z) / np.sum(np.exp(z), axis=-1, keepdims=True)\n",
    "    return probabilities\n",
    "\n",
    "# Sanity check\n",
    "z_dummy = np.linspace(-2, 2, 10).reshape(2, 5)\n",
    "print(f\"Input = \\n{z_dummy}\")\n",
    "print(f\"Sigmoid activation = \\n{sigmoid(z_dummy)}\")\n",
    "print(f\"Hyper Tangent activation = \\n{tanh(z_dummy)}\")\n",
    "print(f\"ReLU activation = \\n{relu(z_dummy)}\")\n",
    "print(f\"Leaky ReLU activation = \\n{leaky_relu(z_dummy, a=0.1)}\")\n",
    "print(f\"Softmax activation = \\n{softmax(z_dummy)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "Input = \n",
    "[[-2.         -1.55555556 -1.11111111 -0.66666667 -0.22222222]\n",
    " [ 0.22222222  0.66666667  1.11111111  1.55555556  2.        ]]\n",
    "Sigmoid activation = \n",
    "[[0.11920292 0.17428532 0.2476638  0.33924363 0.44467194]\n",
    " [0.55532806 0.66075637 0.7523362  0.82571468 0.88079708]]\n",
    "Hyper Tangent activation = \n",
    "[[-0.96402758 -0.9146975  -0.8044548  -0.58278295 -0.21863508]\n",
    " [ 0.21863508  0.58278295  0.8044548   0.9146975   0.96402758]]\n",
    "ReLU activation = \n",
    "[[0.         0.         0.         0.         0.        ]\n",
    " [0.22222222 0.66666667 1.11111111 1.55555556 2.        ]]\n",
    "Leaky ReLU activation = \n",
    "[[-0.2        -0.15555556 -0.11111111 -0.06666667 -0.02222222]\n",
    " [ 0.22222222  0.66666667  1.11111111  1.55555556  2.        ]]\n",
    "Softmax activation = \n",
    "[[0.06801606 0.10607944 0.16544399 0.25803034 0.40243017]\n",
    " [0.06801606 0.10607944 0.16544399 0.25803034 0.40243017]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Forward Propagation\n",
    "Refer to (1), The forward pass of an MLP model is looping the linear and activation transforms. \n",
    "\n",
    "### <font color=#582c83> (10%) Exercise 3. MLP Forward Pass Construction </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(in_features, weights, biases):\n",
    "    \"\"\" Linear function\n",
    "    Args:\n",
    "        in_features: input feature matrix, 2d array with shape (# samples, # input features)\n",
    "        weights: weight parameter matrix, 2d array with shape (# next layer features , # input features)\n",
    "        biases: bias parameter vector, 2d array with shape (1, # next layer features)\n",
    "    Returns:\n",
    "        linear_output: linear model output feature matrix, 2d array with shape (# samples, # next layer features)\n",
    "    \"\"\"\n",
    "    linear_output = in_features @ weights.T + biases\n",
    "    return linear_output\n",
    "\n",
    "def forward(in_features, params, hidden_activation=None, out_activation=None):\n",
    "    \"\"\" Forward function\n",
    "    Args:\n",
    "        in_features: feature matrix, 2d array with shape (# samples, # pixels)\n",
    "        params: a dictionary of weights and biases\n",
    "            params = {\n",
    "                'W1': weight matrix of the first layer, 2d array with shape (# hidden features in 1st layer, # input features)\n",
    "                'b1': bias vector of the first layer, 2d array with shape (1, # hidden features in 1st layer)\n",
    "                ...\n",
    "                'Wi': weight matrix of the i-th layer, 2d array with shape (# hidden features in i-th layer, # hidden features in (i-1)-th layer)\n",
    "                'bi': bias vector of the i-th layer, 2d array with shape (1, # hidden features in i-th layer)\n",
    "                ...\n",
    "                'WL': weight matrix of the last layer, 2d array with shape (# hidden features in output layer, # hidden features in (L-1)-th layer)\n",
    "                'bL': bias vector of the last layer, 2d array with shape (1, # output features)\n",
    "            }\n",
    "        hidden_activation: activation function for hidden layers, could be sigmoid, relu, etc. If None, no activation will be applied.\n",
    "        out_activation: activation function for output layer, could be sigmoid, softmax, etc. If None, no activation will be applied.\n",
    "    Returns:\n",
    "        predictions: predicted probabilities, a column vector or 2d array with shape (# samples, # output features)\n",
    "        cache: linear transformed and activated features\n",
    "    \"\"\"\n",
    "\n",
    "    num_layers = len(params) // 2\n",
    "    # Input layer\n",
    "    cache = {'X0': in_features}\n",
    "    # Hidden layers\n",
    "    for i in range(num_layers - 1):\n",
    "        cache[f'Z{i+1}'] = linear(cache[f'X{i}'], params[f'W{i+1}'], params[f'b{i+1}'])\n",
    "        if hidden_activation==\"relu\":\n",
    "            cache[f'X{i+1}'] = relu(cache[f'Z{i+1}'])\n",
    "        elif hidden_activation==\"leaky_relu\":\n",
    "            cache[f'X{i+1}'] = leaky_relu(cache[f'Z{i+1}'])\n",
    "        elif hidden_activation==\"sigmoid\":\n",
    "            cache[f'X{i+1}'] = sigmoid(cache[f'Z{i+1}'])\n",
    "        elif hidden_activation==\"tanh\":\n",
    "            cache[f'X{i+1}'] = tanh(cache[f'Z{i+1}'])\n",
    "        else:\n",
    "            cache[f'X{i+1}'] = cache[f'Z{i+1}']\n",
    "    # Output layer\n",
    "    cache[f'Z{num_layers}'] = linear(cache[f'X{num_layers-1}'], params[f'W{num_layers}'], params[f'b{num_layers}'])\n",
    "    if out_activation==\"relu\":\n",
    "        predictions = relu(cache[f'Z{num_layers}'])\n",
    "    elif out_activation==\"leaky_relu\":\n",
    "        predictions = leaky_relu(cache[f'Z{num_layers}'])\n",
    "    elif out_activation==\"sigmoid\":\n",
    "        predictions = sigmoid(cache[f'Z{num_layers}'])\n",
    "    elif out_activation==\"tanh\":\n",
    "        predictions = tanh(cache[f'Z{num_layers}'])\n",
    "    elif out_activation==\"softmax\":\n",
    "        predictions = softmax(cache[f'Z{num_layers}'])\n",
    "    else:\n",
    "        predictions = cache[f'Z{num_layers}']\n",
    "    return predictions, cache\n",
    "\n",
    "# Sanity check\n",
    "preds_dummy, cache_dummy = forward(sample_batch_train[0].numpy(), params_dummy, \"sigmoid\", \"softmax\")\n",
    "print(f\"Features: {list(cache_dummy.keys())[:]}\")\n",
    "print(f\"Shape of dummy predictions: {preds_dummy.shape}\")\n",
    "print(f\"Dummy prediction sample with sigmoid activations:\\n{preds_dummy[0]}\")\n",
    "preds_dummy, cache_dummy = forward(sample_batch_train[0].numpy(), params_dummy, \"tanh\", \"softmax\")\n",
    "print(f\"Dummy prediction sample with hyperbolic tangent activations:\\n{preds_dummy[0]}\")\n",
    "preds_dummy, cache_dummy = forward(sample_batch_train[0].numpy(), params_dummy, \"relu\", \"softmax\")\n",
    "print(f\"Dummy prediction sample with ReLU activations:\\n{preds_dummy[0]}\")\n",
    "preds_dummy, cache_dummy = forward(sample_batch_train[0].numpy(), params_dummy, \"leaky_relu\", \"softmax\")\n",
    "print(f\"Dummy prediction sample with Leaky ReLU activations:\\n{preds_dummy[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference Output**:\n",
    ">\n",
    "```console\n",
    "Features: ['X0', 'Z1', 'X1', 'Z2', 'X2', 'Z3']\n",
    "Shape of dummy predictions: (128, 10)\n",
    "Dummy prediction sample with sigmoid activations:\n",
    "[0.09528843 0.09125905 0.11383818 0.09590308 0.1024395  0.09614213\n",
    " 0.08904265 0.09979815 0.10857134 0.1077175 ]\n",
    "Dummy prediction sample with hyperbolic tangent activations:\n",
    "[0.10076069 0.09910421 0.09969189 0.10101848 0.09819445 0.10342691\n",
    " 0.10052092 0.09823987 0.09928544 0.09975714]\n",
    "Dummy prediction sample with ReLU activations:\n",
    "[0.09428151 0.09640544 0.10352176 0.09853189 0.10229858 0.09985063\n",
    " 0.09449275 0.10236338 0.10237048 0.10588358]\n",
    "Dummy prediction sample with Leaky ReLU activations:\n",
    "[0.09453645 0.09626262 0.1035647  0.09868714 0.10195887 0.10033994\n",
    " 0.09460139 0.10198885 0.10230915 0.10575089]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate Model\n",
    "\n",
    "### 3.1. Cross Entropy Loss\n",
    "The cross-entropy (CE) can be used to measure the difference between two probability distributions.\n",
    "Since the one-hot encoded label and the softmax activated prediction are both probability distributions, the error between them can be calculated using the following equation.  \n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\hat{\\mathbf{Y}}, \\mathbf{Y}) = -\\frac{1}{M} \\sum_{m=1}^M \\sum_{c=1}^C {^{(m)} y_c} \\log {^{(m)} \\hat{y}_c}\n",
    "$$\n",
    "\n",
    "### 3.2 Accuracy\n",
    "The accuracy of model predictions can be computed by the following equation.\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Correct predictions}}{\\text{All predictions}}\n",
    "$$\n",
    "\n",
    "A sample can be categorized into the class with highest probability, given the sample's softmax activated prediction.\n",
    "To find out indices of the highest probability for the samples,\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{Y}} = \\argmax(\\hat{\\mathbf{Y}}, \\text{axis}=-1)\n",
    "$$\n",
    "\n",
    "### <font color=#582c83> (10%) Exercise 4. Cross Entropy Loss and Classification Accuracy </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 1 line of code)\n",
    "def ce_loss(predictions, labels):\n",
    "    \"\"\" Cross Entropy Loss\n",
    "    Args:\n",
    "        predictions: predicted probabilities, 2d array with shape (# samples, # classes)\n",
    "        labels: true one-hot encoded labels, 2d array with shape (# samples, # classes)\n",
    "    Returns:\n",
    "        loss: average cross entropy loss, a scalar value\n",
    "    \"\"\"\n",
    "    sample_wise_losses = (-labels * np.log(predictions + 1e-15)).sum(axis=-1)\n",
    "    return sample_wise_losses.mean()\n",
    "\n",
    "def classify(predictions):\n",
    "    \"\"\" (OPTIONAL) Classification from (probablistic) predictions\n",
    "    Args:\n",
    "        predictions: predicted probabilities, 2d array with shape (# samples, # classes)\n",
    "    Returns:\n",
    "        pred_classes: one-hot encoded predicted classes, 2d array with shape (# samples, # classes)\n",
    "    \"\"\"\n",
    "    one_hot_ids = predictions.argmax(axis=-1)\n",
    "    pred_classes = np.zeros_like(predictions)\n",
    "    pred_classes[np.arange(predictions.shape[0]), one_hot_ids] = 1\n",
    "    return pred_classes\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    \"\"\" Average classification accuracy\n",
    "    Args:\n",
    "        predictions: predicted probabilities, 2d array with shape (# samples, # classes)\n",
    "        labels: true one-hot encoded labels, 2d array with shape (# samples, # classes)\n",
    "    Returns:\n",
    "        acc: average classification accuracy, a scalar value\n",
    "    \"\"\"\n",
    "    # predicted_classes = classify(predictions)\n",
    "    # predictions.argmax(axis=-1)\n",
    "    acc = np.mean(predictions.argmax(axis=-1) == labels.argmax(axis=-1))\n",
    "    # acc = np.mean((predicted_classes == labels).all(axis=-1))\n",
    "    return acc\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(f\"Dummy model CE loss: {ce_loss(preds_dummy, sample_batch_train[1].numpy())}\")\n",
    "print(f\"True label samples: \\n{sample_batch_train[1].numpy()[:10]}\")\n",
    "print(f\"Dummy classification samples: \\n{classify(preds_dummy)[:10]}\")\n",
    "print(f\"Dummy model accuracy: {accuracy(preds_dummy, sample_batch_train[1].numpy()) * 100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "Dummy model CE loss: 2.3025913914571188\n",
    "True label samples: \n",
    "[[0 0 0 0 0 0 0 0 0 1]\n",
    " [0 0 0 0 0 0 0 0 0 1]\n",
    " [1 0 0 0 0 0 0 0 0 0]\n",
    " [0 1 0 0 0 0 0 0 0 0]\n",
    " [0 0 0 0 1 0 0 0 0 0]\n",
    " [0 0 0 0 0 1 0 0 0 0]\n",
    " [0 0 0 0 1 0 0 0 0 0]\n",
    " [0 1 0 0 0 0 0 0 0 0]\n",
    " [0 0 0 0 0 0 0 1 0 0]\n",
    " [0 0 0 0 0 0 0 1 0 0]]\n",
    "Dummy classification samples: \n",
    "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
    "Dummy model accuracy: 9.375%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Back-Propagate Gradeients of Loss\n",
    "In order to update model parameters (weights and biases), we need compute gradients of the CE loss or partial derivatives of the loss w.r.t. these model parameters.\n",
    "\n",
    "### 4.1. Derivative of Activation Functions \n",
    "The derivatives of activation functions play important roles when computing the gradients of loss.\n",
    "- Derivative of Sigmoid\n",
    "$$\\sigma'(z) = \\sigma(z) (1 - \\sigma(z)$$\n",
    "\n",
    "- Derivative of Hyperbolic Tangent\n",
    "$$\\tanh' (z) = 1 - \\tanh^2 (z) = \\text{sech}^2 (z)$$\n",
    "\n",
    "- Derivative of Rectified Linear Unit\n",
    "$$\n",
    "\\text{ReLU}' (z) = \n",
    "    \\begin{cases}\n",
    "        0   & z \\leq 0 \\\\\n",
    "        1   & z > 0\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "- Derivative of Leaky ReLU\n",
    "$$\n",
    "\\text{LeakyReLU}' (z; a \\geq 0) = \n",
    "    \\begin{cases}\n",
    "        a   & z \\leq 0 \\\\\n",
    "        1   & z > 0\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "### <font color=#582c83> (4%) Exercise 5. Derivatives of Activation Functions </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 9 lines of code)\n",
    "def d_sigmoid(z):\n",
    "    \"\"\" Derivative of sigmoid function\n",
    "    Args:\n",
    "        z: independent variable, could be an arrary of any shape or a scalar. \n",
    "    Returns:\n",
    "        dependent variable, could be an arrary of any shape or a scalar. \n",
    "    \"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def d_tanh(z):\n",
    "    \"\"\" Derivative of hyperbolic tangent function\n",
    "    Args:\n",
    "        z: independent variable, could be an arrary of any shape or a scalar. \n",
    "    Returns:\n",
    "        dependent variable, could be an arrary of any shape or a scalar. \n",
    "    \"\"\"\n",
    "    return 1 - tanh(z) ** 2\n",
    "\n",
    "def d_relu(z):\n",
    "    \"\"\" Derivative of ReLU function\n",
    "    Args:\n",
    "        z: independent variable, could be an arrary of any shape or a scalar. \n",
    "    Returns:\n",
    "        dependent variable, could be an arrary of any shape or a scalar. \n",
    "    \"\"\"\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def d_leaky_relu(z, a=0.05):\n",
    "    \"\"\" Derivative of Leaky ReLU function\n",
    "    Args:\n",
    "        z: independent variable, could be an arrary of any shape or a scalar. \n",
    "        a: constant, a scalar. \n",
    "    Returns:\n",
    "        dependent variable, could be an arrary of any shape or a scalar. \n",
    "    \"\"\"\n",
    "    return np.where(z > 0, 1.0, a)\n",
    "\n",
    "print(f\"Sigmoid derivative = \\n{d_sigmoid(z_dummy)}\")\n",
    "print(f\"Hyper Tangent derivative = \\n{d_tanh(z_dummy)}\")\n",
    "print(f\"ReLU derivative = \\n{d_relu(z_dummy)}\")\n",
    "print(f\"Leaky ReLU derivative = \\n{d_leaky_relu(z_dummy, a=0.1)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Back Propagation\n",
    " \n",
    "The back-propagation process starts from the last layer and back propagate to the first layer. \n",
    "\n",
    "Due to the fact that the last layer is softmax activated, $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}^{[L]}}$ can be computed differently without explicitly solve for derivative of softmax function.\n",
    "\n",
    "$$\n",
    "d\\mathbf{Z}^{[L]} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}^{[L]}} = \\mathbf{\\hat{Y}} - \\mathbf{Y}\n",
    "$$\n",
    "\n",
    "Then, from last layer $L$ to first layer, we need to repeatedly computing the gradients of loss according to the chain rule. \n",
    "The computation of a general layer $[l]$ is as follows.\n",
    "$$d\\mathbf{W}^{[l]} = d\\mathbf{Z}^{[l]} \\frac{\\partial \\mathbf{Z}^{[l]}}{\\partial{\\mathbf{W}^{[l]}}} = d\\mathbf{Z}^{[l]T} \\cdot \\mathbf{X}^{[l-1]}$$\n",
    "$$d\\mathbf{b}^{[l]} = d\\mathbf{Z}^{[l]} \\frac{\\partial \\mathbf{Z}^{[l]}}{\\partial{\\mathbf{b}^{[l]}}} = mean(d\\mathbf{Z}^{[l]}, axis=0, keepdims=True)$$\n",
    "$$d\\mathbf{X}^{[l-1]} = d\\mathbf{Z}^{[l]} \\frac{\\partial \\mathbf{Z}^{[l]}}{\\partial{\\mathbf{X}^{[l-1]}}} = d\\mathbf{Z}^{[l]} \\cdot \\mathbf{W}^{[l]}$$\n",
    "$$d\\mathbf{Z}^{[l-1]} = d\\mathbf{X}^{[l-1]} * a'(\\mathbf{Z}^{[l-1]})$$\n",
    "\n",
    "Another special case is when $l=1$. \n",
    "Since the back-propagation will stop, and $d\\mathbf{Z}^{[0]}$ does not exist, there is no need to compute $d\\mathbf{X}^{[0]}$ and $d\\mathbf{Z}^{[0]}$.\n",
    "\n",
    "### <font color=#582c83> (18%) Exercise 6. Gradients of Loss Back-Propagation </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backward(predictions, labels, cache, params, activation=\"relu\"):\n",
    "    \"\"\" Backward function\n",
    "    Args:\n",
    "        predictions: predicted probabilities, 2d array with shape (# samples, # output features)\n",
    "        labels: true one-hot encoded labels, 2d array with shape (# samples, # output features)\n",
    "        cache: linear transformed and activated features from forward pass\n",
    "        params: a dictionary of weights and biases\n",
    "        activation: activation function used in hidden layers during forward pass, could be sigmoid, relu, etc.\n",
    "    Returns:\n",
    "        grads: a dictionary of gradients with respect to weights and biases\n",
    "        grads = {\n",
    "            'dW1': gradient of the weight matrix of the first layer, 2d array with shape (# hidden features in 1st layer, # input features)\n",
    "            'db1': gradient of the bias vector of the first layer, 2d array with shape (1, # hidden features in 1st layer)\n",
    "            ...\n",
    "            'dWi': gradient of the weight matrix of the i-th layer, 2d array with shape (# hidden features in i-th layer, # hidden features in (i-1)-th layer)\n",
    "            'dbi': gradient of the bias vector of the i-th layer, 2d array with shape (1, # hidden features in i-th layer)\n",
    "            ...\n",
    "            'dWL': gradient of the weight matrix of the last layer, 2d array with shape (# hidden features in output layer, # hidden features in (L-1)-th layer)\n",
    "            'dbL': gradient of the bias vector of the last layer, 2d array with shape (1, # output features)\n",
    "        }\n",
    "    \"\"\"\n",
    "### Start CODE HERE ### (≈ 18 lines of code)\n",
    "    num_layers = len(params) // 2\n",
    "    grads = {f'dZ{num_layers}': predictions - labels}\n",
    "    for i in reversed(range(num_layers)):\n",
    "        grads[f'dW{i+1}'] = grads[f'dZ{i+1}'].T @ cache['X' + str(i)]\n",
    "        grads[f'db{i+1}'] = np.mean(grads[f'dZ{i+1}'], axis=0, keepdims=True)\n",
    "        if i==0:\n",
    "            break  \n",
    "        grads[f'dX{i}'] = grads[f'dZ{i+1}'] @ params[f'W{i+1}']\n",
    "        if activation == 'sigmoid':\n",
    "            grads[f'dZ{i}'] = grads[f'dX{i}'] * d_sigmoid(cache[f'Z{i}'])\n",
    "        elif activation == 'tanh':\n",
    "            grads[f'dZ{i}'] = grads[f'dX{i}'] * d_tanh(cache[f'Z{i}'])\n",
    "        elif activation == 'relu':\n",
    "            grads[f'dZ{i}'] = grads[f'dX{i}'] * d_relu(cache[f'Z{i}'])\n",
    "        elif activation == 'leaky_relu':\n",
    "            grads[f'dZ{i}'] = grads[f'dX{i}'] * d_leaky_relu(cache[f'Z{i}'])\n",
    "        else:\n",
    "            grads[f'dZ{i}'] = grads[f'dX{i}']\n",
    "\n",
    "    return grads\n",
    "### END CODE HERE ###\n",
    "\n",
    "grads_dummy = backward(preds_dummy, sample_batch_train[1].numpy(), cache_dummy, params_dummy, \"leaky_relu\")\n",
    "for key in grads_dummy:\n",
    "    print(f\"{key} shape: {grads_dummy[key].shape}\")\n",
    "# print(grads_dummy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "dZ3 shape: (128, 10)\n",
    "dW3 shape: (10, 3)\n",
    "db3 shape: (1, 10)\n",
    "dX2 shape: (128, 3)\n",
    "dZ2 shape: (128, 3)\n",
    "dW2 shape: (3, 4)\n",
    "db2 shape: (1, 3)\n",
    "dX1 shape: (128, 4)\n",
    "dZ1 shape: (128, 4)\n",
    "dW1 shape: (4, 3072)\n",
    "db1 shape: (1, 4)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Update Model Parameters\n",
    "Now, it's time to perform gradient descent algorithm to improve the model.\n",
    "Due to the size limit of the computer memory, the entire dataset may not be able to be fit into the RAM at once.\n",
    "Instead, break the dataset down into smaller batches is a more practical solution.\n",
    "Therefore, the model parameters need to be updated multiple times until all the data samples are used in the training loop.    \n",
    "\n",
    "\n",
    "### <font color=#582c83> (33%) Exercise 7. Stochastic Gradient Descent (SGD) Optimization </font>\n",
    "\n",
    "Train your model. Bring both training loss and test loss down. **Note: you may need to spend more time on this task. So, get started as early as possible.**\n",
    "- Set hyper-parameters: number of iterations(epochs), learning rate, hidden layer sizes.\n",
    "- Repeat model parameters updating until CE loss converged:\n",
    "    1. Extract a batch from the dataset. \n",
    "    2. Predict batch classes. \n",
    "    3. Evaluate model.\n",
    "    4. Compute gradients.\n",
    "    5. Update parameters on the loss descending directions.\n",
    "\n",
    "- > Tip: Choose the activation function wisely. A poorly activated layer may cause longer training duration as well as degraded model performance.\n",
    "\n",
    "<font color=red> **Bring validation accuracy over 70%** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start CODE HERE ### (≈ 11 lines of code)\n",
    "# Hyper-parameters\n",
    "num_iters = 50\n",
    "learning_rate = 3e-4\n",
    "hidden_layer_sizes = (32, 32)\n",
    "# Model parameters\n",
    "params = init_params(in_dims=sample_batch_train[0].numpy().shape[-1], hidden_dims=hidden_layer_sizes, out_dims=sample_batch_train[1].numpy().shape[-1])\n",
    "# Reserve metrics storage\n",
    "losses_train, losses_val = [], []\n",
    "accuracies_train, accuracies_val = [], []\n",
    "\n",
    "# LOOP\n",
    "for ep in range(num_iters):\n",
    "    ep_loss, ep_acc = [], []\n",
    "    for bi, batch_train in enumerate(dataloader_train):\n",
    "        preds_train, cache_train = forward(batch_train[0].numpy(), params, \"relu\", \"softmax\")\n",
    "        batch_loss = ce_loss(preds_train, batch_train[1].numpy())\n",
    "        batch_acc = accuracy(preds_train, batch_train[1].numpy())\n",
    "        ep_loss.append(batch_loss)\n",
    "        ep_acc.append(batch_acc)\n",
    "        if not (bi+1) % 50:\n",
    "            print(f\"Training epoch {ep+1} batch {bi+1} loss: {batch_loss}, accuracy: {batch_acc}\")\n",
    "        grads = backward(preds_train, batch_train[1].numpy(), cache_train, params, activation=\"relu\")\n",
    "        for l in range(len(params_dummy) // 2):\n",
    "            params[f'W{l+1}'] = params[f'W{l+1}'] - learning_rate * grads[f'dW{l+1}']\n",
    "            params[f'b{l+1}'] = params[f'b{l+1}'] - learning_rate * grads[f'db{l+1}']\n",
    "    # Validate updated model on entire val dataset\n",
    "    data_val = next(iter(dataloader_val))\n",
    "    preds_val, _ = forward(data_val[0].numpy(), params, \"relu\", \"softmax\")\n",
    "    loss_val = ce_loss(preds_val, data_val[1].numpy())\n",
    "    acc_val = accuracy(preds_val, data_val[1].numpy())\n",
    "    print(f\"Epoch {ep+1} validation accuracy: {acc_val}\")    \n",
    "    # Summarize epoch\n",
    "    losses_train.append(np.mean(ep_loss))\n",
    "    accuracies_train.append(np.mean(ep_acc))\n",
    "    losses_val.append(loss_val)\n",
    "    accuracies_val.append(acc_val)\n",
    "### END CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Exam Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss and accuracy curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(num_iters), losses_train, range(num_iters), losses_val)\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.title(\"Loss Decrease\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"BCE Loss\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(num_iters), accuracies_train, range(num_iters), accuracies_val)\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.title(\"Accuracy Growth\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix \n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "trimmed_classes = [c[:4] for c in dataset_train.classes] \n",
    "cm = confusion_matrix(data_val[1].numpy().argmax(axis=-1), preds_val.argmax(axis=-1))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=trimmed_classes)\n",
    "disp.plot(cmap=plt.cm.PuRd)\n",
    "plt.title(\"Confusion Matrix on Validation Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val[1].sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#582c83> (15%) Exercise 8. Model Analyses </font>\n",
    "Based on the validation confusion matrix.\n",
    "1. Please rank your model's classification accuracies based on the classes. \n",
    "2. Please report the top 3 classes your model struggled most. \n",
    "3. Take your model's most struggling class as an example, please try to briefly explain why the model was not performing well making correct predictions on this class. \n",
    "\n",
    "> Please write your answers below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Congrats on finishing this assignment! "
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "cifar10mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
