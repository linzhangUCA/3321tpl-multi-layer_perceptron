{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron Neural Network\n",
    "\n",
    "**Due: Wednesday, 11/22/2023, 2:15 PM**\n",
    "\n",
    "Welcome to your fourth assignment. You will build a multi-layer perceptron neural network in this assignment. The goal of building such a  is to classify hand-written digits.\n",
    "\n",
    "## Exercises:\n",
    "1. $\\color{violet}{\\textbf{(20\\%) Data Preprocessing}}$\n",
    "2. $\\color{violet}{\\textbf{(5\\%) Logistic Regression Model}}$\n",
    "3. $\\color{violet}{\\textbf{(5\\%) Cross Entropy Loss}}$\n",
    "4. $\\color{violet}{\\textbf{(40\\%) Gradient Descent Optimization}}$\n",
    "5. $\\color{violet}{\\textbf{(15\\%) Evaluation on Test Dataset}}$\n",
    "6. $\\color{violet}{\\textbf{(15\\%) Test Model with New Image}}$\n",
    "\n",
    "## Instructions:\n",
    "- Write your code only between the $\\color{green}{\\textbf{\\small \\#\\#\\# START CODE HERE \\#\\#\\#}}$ and $\\color{green}{\\textbf{\\small \\#\\#\\# END CODE HERE \\#\\#\\#}}$ commented lines. $\\color{red}{\\textbf{Do not modify code out of the designated area.}}$\n",
    "- Reference answers are provided after a certain coding blocks. Be aware if your answer is different from the reference..\n",
    "- **Need to install [Torchvision](https://pytorch.org/vision/stable/index.html)**\n",
    "    ```console\n",
    "    pip install torchvision\n",
    "    ```\n",
    "**You will learn:**\n",
    "- Usage of Rectified Linear Unit (ReLU) activation function.\n",
    "- Generalize number and dimension of the hidden layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "**NOTE: math representations of forward and backward propogation has been updated. Please use the equations in the [updated slides](https://linzhanguca.github.io/_docs/applied_deep_learning-2022/0921/nn_p2.pdf) or the follows if you prefer no transopose in the forward propagation.**\n",
    "\n",
    "To build your neural network, you will complete several \"helper functions\". These helper functions will be used to realize the forward and backward propagation when training a K-layer neural network. Each small helper function you will implement will have detailed instructions that will walk you through the necessary steps. Here is an outline of this assignment, you will:\n",
    "\n",
    "- Initialize the parameters for K-layer neural network.\n",
    "- Implement the forward propagation. \n",
    "     - Compute linear transformation $\\mathbf{Z}^{[k]} = \\mathbf{X}^{[k-1]} \\cdot \\mathbf{W}^{[k]} + \\mathbf{b}^{[k]}$.\n",
    "     - Compute activation: $X^{[k]} = g(\\mathbf{Z}^{[k]})$.\n",
    "     - Stack the \"linear transfortmation\" and \"activation\" to compute predictions in the final layer.\n",
    "- Compute the cross entropy loss: \n",
    "    $$\\mathcal{L(\\hat{\\mathbf{y}}, \\mathbf{y}) = \\frac{1}{M}\\sum_{i=1}^M (-\\mathbf{y}log(\\hat{\\mathbf{y}}) - (1 - \\mathbf{y})log(1 - \\hat{\\mathbf{y}}))}$$\n",
    "- Compute gradients of the parameters for backward propagation.\n",
    "    $$d\\mathbf{Z}^{[k]} = \\frac{\\partial{J}}{\\partial{\\mathbf{Z}^{[k]}}} = d\\mathbf{X}^{[k]} * g'^{[k]}(\\mathbf{Z}^{[k]})$$\n",
    "    $$d\\mathbf{W}^{[k]} = \\frac{\\partial{J}}{\\partial{\\mathbf{W}^{[k]}}} = \\frac{1}{M}\\mathbf{X}^{[k-1]\\mathbf{T}} \\cdot d\\mathbf{Z}^{[k]}$$\n",
    "    $$d\\mathbf{b}^{[k]} = \\frac{\\partial{J}}{\\partial{\\mathbf{b}^{[k]}}} = \\frac{1}{M} \\Sigma d\\mathbf{Z}^{[k]}$$\n",
    "    $$d\\mathbf{X}^{[k-1]} = \\frac{\\partial{J}}{\\partial{\\mathbf{X}^{[k-1]}}} = d\\mathbf{Z}^{[k]} \\cdot \\mathbf{W}^{[k]\\mathbf{T}}$$\n",
    "- Update the parameters using gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset\n",
    "Torchvision provides a descent pool of datasets. We'll load one of the built-in dataset, [FashionMNIST](https://pytorch.org/vision/stable/datasets.html) to investigate multi-class classification using a generic Multi-Layer Perceptron (MLP) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training feature array shape: (60000, 28, 28), test feature array shape: (10000, 28, 28)\n",
      "training target array shape: (60000,), test target array shape: (10000,)\n",
      "{'T-shirt/top': 0, 'Trouser': 1, 'Pullover': 2, 'Dress': 3, 'Coat': 4, 'Sandal': 5, 'Shirt': 6, 'Sneaker': 7, 'Bag': 8, 'Ankle boot': 9}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "train_set = datasets.FashionMNIST('./data', train=True, download=True)\n",
    "test_set = datasets.FashionMNIST('./data', train=False, download=True)\n",
    "\n",
    "# Extract important arrays\n",
    "train_feature_array = train_set.data.numpy()\n",
    "train_target_array = train_set.targets.numpy()\n",
    "test_feature_array = test_set.data.numpy()\n",
    "test_target_array = test_set.targets.numpy()\n",
    "category_keys = train_set.classes\n",
    "category_vals = range(len(category_keys))\n",
    "category_dict = dict((map(lambda i,j : (i,j), category_keys, category_vals)))\n",
    "\n",
    "print(f\"training feature array shape: {train_feature_array.shape}, test feature array shape: {test_feature_array.shape}\")\n",
    "print(f\"training target array shape: {train_target_array.shape}, test target array shape: {test_target_array.shape}\")\n",
    "print(category_dict)\n",
    "\n",
    "# Visualize\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 4, 4\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = np.random.randint(0, len(train_set))\n",
    "    img, label = train_set[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(category_keys[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img, cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess the Data\n",
    "A $\\mathcal{C}$-calss dataset with   is made up with a feature matrix: $\\mathbf{X} = [^{(1)}\\mathbf{x}, ^{(2)}\\mathbf{x}, ..., ^{(M)}\\mathbf{x}]^T$. and a one-hot encoded target matrix $\\mathbf{Y} = [^{(1)}\\mathbf{y}, ^{(2)}\\mathbf{y}, ..., ^{(M)}\\mathbf{y}]^T$. Where $M$ is the total number of instances in the dataset, $^{(m)}\\mathbf{x} = [^{(m)}x_1, ..., ^{(m)}x_n]$ is a normalized and flattened row vector bears $n$ feature values, and $^{(m)}\\mathbf{y} = [0, ..., 0, 1, 0, ..., 0]$ is a one-hot encoded row vector.\n",
    "\n",
    "- A grey-scale image can be represented by a **2-dimensional array with shape $(width, height)$**. Where, $width$ indicates number of pixels on horizontal direction, $height$ indicates number of pixels on vertical direction.\n",
    "- We can use an **integer ranged 0~255** to describe a pixel's color intensity. However, it is easier for your computer to handlle float values.\n",
    "- We would like to convert an image array into a row vector, or a **2d array with shape $(1, width*height)$**. So that, we can stack these row vectors vertically to form a feature matrix.\n",
    "- We also would like to encode target array into one-hot format.\n",
    "\n",
    "\n",
    "We will access the raw data by extracting it from the dataloaders. Then, process and prepare the raw data so that it can be used in later steps.\n",
    "\n",
    "### $\\color{violet}{\\textbf{(20\\%) Exercise 1: Data Preprocessing}}$\n",
    "1. Reshape feature array.\n",
    "2. One-hot encode target array\n",
    "3. Rescale feature arrary, represent each pixel with a float numbers in range 0~1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training feature shape: (60000, 784), test feature shape: (10000, 784), training target shape: (60000, 10), test target shape: (10000, 10)\n",
      "sample feature portion: \n",
      "[0.99607843 0.79215686 0.82745098 0.79215686 0.79607843 0.79215686\n",
      " 0.83921569 0.84313725 0.83921569 0.84705882 0.82352941 0.92941176\n",
      " 0.57254902 0.         0.00392157 0.         0.         0.\n",
      " 0.         0.         0.00784314 0.01568627 0.01176471 0.\n",
      " 0.         0.         0.38039216 0.98823529 0.80784314 0.83529412\n",
      " 0.80392157 0.81176471 0.8        0.80784314 0.83529412 0.83921569\n",
      " 0.84313725 0.84705882 0.85490196 0.90980392 0.77254902 0.\n",
      " 0.         0.00784314 0.01960784 0.02745098 0.00784314 0.00392157\n",
      " 0.         0.        ]\n",
      "sample target: \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### ( 10 lines of code)\n",
    "M_train = train_target_array.shape[0]  # number of sampels in training data\n",
    "M_test = test_target_array.shape[0]  # number of samples in test data\n",
    "# Reshape feature and target arrays\n",
    "flatten_feature_train = train_feature_array.reshape(M_train, -1)  # (60000, 28, 28) -> (60000, 1)\n",
    "flatten_feature_test = test_feature_array.reshape(M_test, -1)  # (10000, 28, 28) -> (10000, 1)\n",
    "# One hot encode targets\n",
    "onehot_target_train = np.zeros((M_train, len(category_dict)))\n",
    "onehot_target_train[np.arange(M_train), train_target_array] = 1\n",
    "onehot_target_test = np.zeros((M_test, len(category_dict)))\n",
    "onehot_target_test[np.arange(M_test), test_target_array] = 1\n",
    "# print(np.sum(onehot_target_test, axis=0))\n",
    "# Rescale features\n",
    "rescale_feature_train = flatten_feature_train / 255\n",
    "rescale_feature_test = flatten_feature_test / 255\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Rename\n",
    "feature_train = rescale_feature_train\n",
    "feature_test = rescale_feature_test\n",
    "target_train = onehot_target_train\n",
    "target_test = onehot_target_test\n",
    "print(f\"training feature shape: {feature_train.shape}, test feature shape: {feature_test.shape}, training target shape: {target_train.shape}, test target shape: {target_test.shape}\")\n",
    "print(f\"sample feature portion: \\n{feature_train[3321][350:400]}\")\n",
    "print(f\"sample target: \\n{target_train[3321]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "training feature shape: (60000, 784), test feature shape: (10000, 784), training target shape: (60000, 10), test target shape: (10000, 10)\n",
    "sample feature portion: \n",
    "[0.99607843 0.79215686 0.82745098 0.79215686 0.79607843 0.79215686\n",
    " 0.83921569 0.84313725 0.83921569 0.84705882 0.82352941 0.92941176\n",
    " 0.57254902 0.         0.00392157 0.         0.         0.\n",
    " 0.         0.         0.00784314 0.01568627 0.01176471 0.\n",
    " 0.         0.         0.38039216 0.98823529 0.80784314 0.83529412\n",
    " 0.80392157 0.81176471 0.8        0.80784314 0.83529412 0.83921569\n",
    " 0.84313725 0.84705882 0.85490196 0.90980392 0.77254902 0.\n",
    " 0.         0.00784314 0.01960784 0.02745098 0.00784314 0.00392157\n",
    " 0.         0.        ]\n",
    "sample target: \n",
    "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forward Pass\n",
    "A Multi-Layer Perceptron (MLP) model is featured with multiple layers of transformed features. Any two adjacent layer are connected by a linear model and an activation function. The linear model is governed by a set of weight parameters and a set of bias parameters. The general structure of an MLP model is shown below.\n",
    "![](./nnNNN.png)\n",
    "\n",
    "### 2.1. Initialize Parameters\n",
    "A linear model governed by weights $\\mathbf{W}^{[l]}$ and $\\mathbf{b}^{[l]}$ transforms features, $\\mathbf{X}^{[l-1]}$ in the $l-1$-th layer to $\\mathbf{Z}^{[l]}$ in the $l$-th layer.\n",
    "\n",
    "$\\mathbf{Z}^{[l]} = \\mathbf{X}^{[l-1]} \\cdot \\mathbf{W}^{[l]T} + \\mathbf{b}^{[l]}$\n",
    "\n",
    "Assume $\\mathbf{X}^{[l-1]}$ has $N_{l-1}$ features and $\\mathbf{X}^{[l]}$ has $N_{l}$ features, then $\\mathbf{W}^{[l]}$ is with shape $(N_l, N_{l-1})$, $\\mathbf{b}^{[l]}$ is with shape $(1, N_l)$\n",
    "\n",
    "### $\\color{violet}{\\textbf{(10\\%) Exercise 2: Parameter Initialization}}$\n",
    "Define a function to initialize weights and biases parameters and save these parameters in a **dictionary**. \n",
    "- Input sizes of all the layers (**include the input layer**) using a **list**.\n",
    "- Use a **`for` loop** to randomly initialize $\\mathbf{W}^{[l]}$ and $\\mathbf{b}^{[l]}$ for the $l$-th layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['W1', 'b1', 'W2', 'b2', 'W3', 'b3'])\n",
      "{'W1': array([[ 3.68455981e-05, -1.38782645e-04, -7.23142013e-05,\n",
      "         9.20641119e-05, -1.95355264e-05,  3.87255258e-05],\n",
      "       [-6.00629078e-05,  3.25435259e-06,  6.46665063e-05,\n",
      "        -7.84748031e-05,  1.41884652e-04, -6.15479349e-05],\n",
      "       [-1.10127440e-04, -6.82385574e-05,  7.96622349e-05,\n",
      "         5.25934678e-05, -1.78411966e-04,  1.28629057e-04],\n",
      "       [ 5.66879246e-05,  8.95999019e-05, -1.62874713e-05,\n",
      "         3.51109174e-05, -1.03338181e-04, -8.88623865e-05],\n",
      "       [ 5.10860443e-05,  1.19661547e-04,  8.64757553e-05,\n",
      "        -1.41101724e-04,  1.38003186e-04, -1.06812125e-04]]), 'b1': array([[ 6.38004843e-05, -6.19666755e-05,  1.12931166e-04,\n",
      "        -3.12290798e-05,  5.59708416e-05]]), 'W2': array([[ 1.02685210e-04, -1.28307992e-04, -4.03018811e-05,\n",
      "        -4.09270011e-05, -1.84834135e-05],\n",
      "       [ 1.71466862e-04, -9.69611346e-05,  4.40873689e-05,\n",
      "        -1.10516782e-04, -5.17959570e-05],\n",
      "       [-1.87055902e-04, -6.31341925e-05, -5.47992268e-05,\n",
      "         3.67804566e-05, -1.05795654e-04],\n",
      "       [ 1.24964681e-04,  3.60854003e-05,  1.30113815e-04,\n",
      "         1.01177937e-04,  2.22286230e-05]]), 'b2': array([[ 1.16709537e-04, -1.12496462e-04, -9.47925077e-05,\n",
      "        -4.49789001e-05]]), 'W3': array([[ 1.01049508e-04, -3.52965408e-05,  2.06672986e-04,\n",
      "        -1.67359842e-05],\n",
      "       [-7.34788781e-05,  1.28189925e-05,  3.67334763e-05,\n",
      "        -3.00482492e-05],\n",
      "       [ 9.36343566e-05, -3.80351405e-05,  6.59823461e-06,\n",
      "         5.61206851e-05]]), 'b3': array([[ 2.57409859e-04,  1.69968799e-05, -7.22321090e-05]])}\n"
     ]
    }
   ],
   "source": [
    "def init_params(layer_sizes):\n",
    "    \"\"\" Parameter initialization function\n",
    "    Args:\n",
    "        layer_sizes -- list/tuple, (input size, ..., hidden layer size, ..., output size)\n",
    "    Returns:\n",
    "        parameters -- dictionary, contains parameters: Wi and bi, i is the i-th layer\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    ### START CODE HERE ### ( 2 lines of code)\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        parameters['W'+str(i+1)] = np.random.normal(0, 0.0001, size=(layer_sizes[i+1], layer_sizes[i]))\n",
    "        parameters['b'+str(i+1)] = np.random.normal(0, 0.0001, size=(1, layer_sizes[i+1]))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return parameters\n",
    "\n",
    "# Sanity check\n",
    "np.random.seed(3321)\n",
    "dummy_layer_sizes = (6, 5, 4, 3)  # (input size, layer1 size, layer2 size, output size)\n",
    "dummy_params = init_params(dummy_layer_sizes)\n",
    "print(dummy_params.keys())\n",
    "print(dummy_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "dict_keys(['W1', 'b1', 'W2', 'b2', 'W3', 'b3'])\n",
    "{'W1': array([[ 3.68455981e-05, -1.38782645e-04, -7.23142013e-05,\n",
    "         9.20641119e-05, -1.95355264e-05,  3.87255258e-05],\n",
    "       [-6.00629078e-05,  3.25435259e-06,  6.46665063e-05,\n",
    "        -7.84748031e-05,  1.41884652e-04, -6.15479349e-05],\n",
    "       [-1.10127440e-04, -6.82385574e-05,  7.96622349e-05,\n",
    "         5.25934678e-05, -1.78411966e-04,  1.28629057e-04],\n",
    "       [ 5.66879246e-05,  8.95999019e-05, -1.62874713e-05,\n",
    "         3.51109174e-05, -1.03338181e-04, -8.88623865e-05],\n",
    "       [ 5.10860443e-05,  1.19661547e-04,  8.64757553e-05,\n",
    "        -1.41101724e-04,  1.38003186e-04, -1.06812125e-04]]), 'b1': array([[ 6.38004843e-05, -6.19666755e-05,  1.12931166e-04,\n",
    "        -3.12290798e-05,  5.59708416e-05]]), 'W2': array([[ 1.02685210e-04, -1.28307992e-04, -4.03018811e-05,\n",
    "        -4.09270011e-05, -1.84834135e-05],\n",
    "       [ 1.71466862e-04, -9.69611346e-05,  4.40873689e-05,\n",
    "        -1.10516782e-04, -5.17959570e-05],\n",
    "       [-1.87055902e-04, -6.31341925e-05, -5.47992268e-05,\n",
    "         3.67804566e-05, -1.05795654e-04],\n",
    "       [ 1.24964681e-04,  3.60854003e-05,  1.30113815e-04,\n",
    "         1.01177937e-04,  2.22286230e-05]]), 'b2': array([[ 1.16709537e-04, -1.12496462e-04, -9.47925077e-05,\n",
    "        -4.49789001e-05]]), 'W3': array([[ 1.01049508e-04, -3.52965408e-05,  2.06672986e-04,\n",
    "        -1.67359842e-05],\n",
    "       [-7.34788781e-05,  1.28189925e-05,  3.67334763e-05,\n",
    "        -3.00482492e-05],\n",
    "       [ 9.36343566e-05, -3.80351405e-05,  6.59823461e-06,\n",
    "         5.61206851e-05]]), 'b3': array([[ 2.57409859e-04,  1.69968799e-05, -7.22321090e-05]])}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Forward Propagation\n",
    "A linear model transforms features, $\\mathbf{X}^{[l-1]}$ in the $l-1$-th layer to $\\mathbf{Z}^{[l]}$ in the $l$-th layer. Then we apply a Linear Rectified Unit (ReLU) function on $\\mathbf{Z}^{[l]}$ to form new features $\\mathbf{Z}^{[l]}$ in the $l$-th layer.\n",
    "\n",
    "$\\mathbf{Z}^{[l]} = \\mathbf{X}^{[l-1]} \\cdot \\mathbf{W}^{[l]T} + \\mathbf{b}^{[l]}$\n",
    "\n",
    "$\\mathbf{X}^{[l]} = ReLU(\\mathbf{Z}^{[l]})$\n",
    "\n",
    "The last layer needs to be activcated by a softmax function.\n",
    "\n",
    "#### $\\hat{y}_i = \\frac{e^{z^{[L]}_i}}{\\sum^C_{i=1} e^{z^{[L]}_i}}$, where $C$ is the total number of the classes.\n",
    "The maxtrix $\\mathbf{Z}^{[L]}$ has shape: $(M, C)$, where $M$ is the number of samples. When applying softmax activation, we only want to apply it on the 2nd dimension (1st axis in numpy). So that each row in $\\mathbf{Z}^{[L]}$ will be converted to probabilities.\n",
    "### $\\color{violet}{\\textbf{(40\\%) Exercise 3: Linear Model and Activation}}$\n",
    "- Define ReLU activation function and softmax activation function.\n",
    "- Define linear model.\n",
    "- Define forward propagation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]]\n",
      "cache dictionary keys: dict_keys(['X0', 'Z1', 'X1', 'Z2', 'X2', 'Z3'])\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### ( 2 lines of code)\n",
    "def relu(x):\n",
    "    \"\"\" Rectified linear unit function\n",
    "    Args:\n",
    "        x -- scalar/array\n",
    "    Returns:\n",
    "        y -- scalar/array, 0 if x <= 0, x if x >0\n",
    "    \"\"\"\n",
    "    y = np.maximum(0, x)\n",
    "\n",
    "    return y\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\" Softmax function\n",
    "    Args:\n",
    "        x -- array\n",
    "    Returns:\n",
    "        prob -- array\n",
    "    \"\"\"\n",
    "    prob = np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
    "    \n",
    "    return prob\n",
    "\n",
    "def linear(feature, weight, bias):\n",
    "    \"\"\" Linear model\n",
    "    Args:\n",
    "        feature (matrix): 2d array with shape (M, N^[l-1])\n",
    "        weight (matrix): 2d array with shape (N^[l], N^[l-1])\n",
    "        bias (row vector): 2d array with shape (1, N^[l])\n",
    "    Returns:\n",
    "        Z (matrix): 2d array with shape (M, , N^[l])\n",
    "    \"\"\"\n",
    "    Z = np.dot(feature, weight.T) + bias\n",
    "        \n",
    "    return Z\n",
    "\n",
    "def forward(input_feature, params):\n",
    "    \"\"\" Forward propagation process\n",
    "    Args:\n",
    "        input_feature (matrix): 2d array with shape (M, N^[0])\n",
    "        params: dictionary\n",
    "    Returns:\n",
    "        prediction: 2d array with shape (M, C)\n",
    "        cache: dictionary, stores intemediate Xs and Zs.\n",
    "    \"\"\"\n",
    "    cache = {'X0': input_feature}\n",
    "    for i in range(int(len(params) / 2) - 1):\n",
    "        cache['Z' + str(i+1)] = linear(cache['X' + str(i)], params['W' + str(i+1)], params['b' + str(i+1)])\n",
    "        cache['X' + str(i+1)] = relu(cache['Z' + str(i+1)])\n",
    "    cache['Z' + str(i+2)] = linear(cache['X' + str(i+1)], params['W' + str(i+2)], params['b' + str(i+2)])\n",
    "    prediction = softmax(cache['Z' + str(i+2)])\n",
    "\n",
    "    return prediction, cache\n",
    "\n",
    "# Sanity check\n",
    "np.random.seed(3321)\n",
    "dummy_input = np.random.randn(8,6)\n",
    "dummy_pred, dummy_cache = forward(dummy_input, dummy_params)\n",
    "print(dummy_pred)\n",
    "print(f\"cache dictionary keys: {dummy_cache.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "[[0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]]\n",
    "cache dictionary keys: dict_keys(['X0', 'Z1', 'X1', 'Z2', 'X2', 'Z3'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Linear Transformation\n",
    "Use the following equation to perform linear transformation:\n",
    "$$\\mathbf{Z}^{[k]} = \\mathbf{X}^{[k-1]} \\cdot \\mathbf{W}^{[k]} + \\mathbf{b}^{[k]}$$\n",
    "\n",
    "#### **(5%) Exercise 2.2**: Linear transformation.\n",
    "Complete `linear()` function to transoform the features in the previous layer (`X_j`) into the non-activated features in the next layer (`Z_k`) using parameters: `W_k` and `b_k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(X_j, W_k, b_k):\n",
    "    \"\"\"\n",
    "    linear transformation converts neurons in previous layer to neuron in current layer.\n",
    "\n",
    "    Arguments:\n",
    "        X_j -- numpy array, feature matrix of the (k-1)th layer. shape: (number_of_examples, previous_layer_dimension)\n",
    "        W_k -- numpy array, weights matrix connects (k-1)th layer to kth layer. shape: (previous_layer_dimension, current_layer_dimension)\n",
    "        b_k -- numpy array, bias vector connects (k-1)th layer to kth layer. shape: (1, current_layer_dimension)\n",
    "\n",
    "    Returns:\n",
    "        Z_k -- numpy array, linear transformed (pre-activation) featues matrix. shape: (number_of_examples, current_layer_dimension ) \n",
    "        cache -- dictionary {'X_j', 'W_k', 'b_k', 'Z_k'}, useful intermediate results for later usage.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    Z_k = np.dot(X_j, W_k) + b_k\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert Z_k.shape == (X_j.shape[0], W_k.shape[1])\n",
    "    cache = {\n",
    "        'X_j': X_j,\n",
    "        'W_k': W_k,\n",
    "        'b_k': b_k,\n",
    "        'Z_k': Z_k,\n",
    "    }\n",
    "\n",
    "    return Z_k, cache\n",
    "\n",
    "# test\n",
    "np.random.seed(4350)\n",
    "X = np.random.randn(3, 4)\n",
    "W = np.random.randn(4, 2)\n",
    "b = np.random.randn(1, 2)\n",
    "Z, cache = linear(X, W, b)\n",
    "print(f\"pre-activation features: {Z}\")\n",
    "print(f\"linear cache: {cache}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "\n",
    "```console\n",
    "pre-activation features: [[-0.5691033   2.86655348]\n",
    " [ 1.50009864  1.85049448]\n",
    " [ 0.69190761 -2.80338728]]\n",
    "linear cache: {'X_j': array([[-0.97866006,  0.05901167, -1.42715059, -0.27632879],\n",
    "       [-0.45200202, -0.64213329,  0.22489383,  0.10720757],\n",
    "       [-0.66929759, -0.05123023,  2.44058435, -0.88938706]]), 'W_k': array([[ 0.03347135, -0.49223651],\n",
    "       [-1.51707154, -0.9012758 ],\n",
    "       [ 0.40908305, -1.20410713],\n",
    "       [ 0.81364786,  1.56567891]]), 'b_k': array([[0.36183616, 1.15220702]]), 'Z_k': array([[-0.5691033 ,  2.86655348],\n",
    "       [ 1.50009864,  1.85049448],\n",
    "       [ 0.69190761, -2.80338728]])}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Activation Functions\n",
    "Activation functions add non-linear transformations toward the features. \n",
    "- Sigmoid\n",
    "$$y = \\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "- Hyperbolic Tangent\n",
    "$$y = tanh(x) = \\frac{e^x - e^{-x}}{e^x+e^{-x}}$$\n",
    "- Rectified Linear Unit\n",
    "$$y = ReLU(x) = \n",
    "    \\begin{cases}\n",
    "        0   & x \\leq 0 \\\\\n",
    "        x   & x > 0\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "Linearly transformed features will be non-linearly activated by:\n",
    "$$X^{[k]} = g(\\mathbf{Z}^{[k]})$$\n",
    "All the activation functions are either pre-built in `utils.py` or included in NumPy library, you don't have to do anything in this step. However, you can run the following code block to observe how different activation functions affect the features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(4350)\n",
    "Z = np.random.randn(1,4)\n",
    "print(f\"\\noriginal feature: {Z}, \\nsigmoid activated feature: {sigmoid(Z)} \\ntanh activated feature: {np.tanh(Z)} \\nrelu activated feature: {relu(Z)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Stack Linear and Activation \n",
    "Now, you can stack linear transformations and activations to implement any layer neural network's forward propagation.\n",
    "\n",
    "#### **(20%) Exercise 2.4: Forward propagation**\n",
    "Complete `forward()` function to implement the $K$-layer Neural Network. You will need to perform `linear()` then use any activation functions $K-1$ times, then follows the last `linear()` transformation followed by the `sigmoid()` activation.\n",
    "You'll get a column vector, `yhat` with shape: `(number of example, 1)` in the end. \n",
    "\n",
    "> Tips:\n",
    "- > Use the functions you had previously written \n",
    "- > Use a for loop to repeat `z=linear(x) -> x=relu(z)` or `z=linear(x) -> x=np.tanh(z)` or `z=linear(x) -> x=sigmoid(z)` (K-1) times\n",
    "- > Don't forget to keep track of the intermediate results in each layer. You can use a nested dictionary: `caches` to store `cache` from each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(inputs, params, activation='relu'):\n",
    "    \"\"\"\n",
    "    forward function loops linear and activation to tranform the original features into the probability of the predictions.\n",
    "\n",
    "    Arguments:\n",
    "        inputs -- numpy array, original feature matrix. shape: (number_of_examples, original_feature_dimension)\n",
    "        params -- dictionary, stores weights and biases connects (k-1)th layer to kth layer.\n",
    "        activation -- string, activation type for the hidden layers (exclude the final layer).\n",
    "\n",
    "    Returns:\n",
    "        yhat -- numpy array, column vector of the predicted probability of the classes. shape: (number_of_examples, 1) \n",
    "        caches -- nested dictionary {'layer1: {cache}', ... , 'layerK': {cache}}, stores all the intermediate results in each layer.\n",
    "    \"\"\"\n",
    "\n",
    "    caches = {}\n",
    "    num_layers = len(params) // 2\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    for k in range(1, num_layers):\n",
    "        Z_k, cache = None\n",
    "        if activation=='relu':\n",
    "            X_k = None\n",
    "        elif activation=='tanh':\n",
    "            X_k = None\n",
    "        elif activation=='sigmoid':\n",
    "            X_k = None\n",
    "        else:\n",
    "            X_k = Z_k\n",
    "        caches['layer' + str(k)] = cache\n",
    "\n",
    "    Z_K, cache = None # final layer linear transform\n",
    "    yhat = None  # final layer activation \n",
    "    caches['layer' + str(num_layers)] = cache\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert yhat.shape == (inputs.shape[0], 1)\n",
    "\n",
    "    return yhat, caches\n",
    "\n",
    "\n",
    "\n",
    "# Test forward()\n",
    "X = np.array([\n",
    "        [-0.31178367, 0.72900392, 0.21782079, -0.8990918],\n",
    "        [-2.48678065, 0.91325152, 1.12706373, -1.51409323],\n",
    "        [1.63929108, -0.4298936, 2.63128056, 0.60182225],\n",
    "        [-0.33588161, 1.23773784, 0.11112817, 0.12915125],\n",
    "        [0.07612761, -0.15512816, 0.63422534, 0.810655],\n",
    "    ]).T\n",
    "parameters = {\n",
    "    'W1': np.array([\n",
    "        [0.35480861, 1.81259031, -1.3564758, -0.46363197, 0.82465384],\n",
    "        [-1.17643148, 1.56448966, 0.71270509, -0.1810066, 0.53419953],\n",
    "        [-0.58661296, -1.48185327, 0.85724762, 0.94309899, 0.11444143],\n",
    "        [-0.02195668, -2.12714455, -0.83440747, -0.46550831, 0.23371059]\n",
    "    ]).T,\n",
    "    'b1': np.array([[1.38503523, -0.51962709, -0.78015214, 0.95560959]]),\n",
    "    'W2': np.array([\n",
    "        [-0.12673638, -1.36861282, 1.21848065, -0.85750144],\n",
    "        [-0.56147088, -1.0335199, 0.35877096, 1.07368134],\n",
    "        [-0.37550472, 0.39636757, -0.47144628, 2.33660781]\n",
    "    ]).T,\n",
    "    'b2': np.array([[1.50278553, -0.59545972, 0.52834106]]),\n",
    "    'W3': np.array([[0.9398248, 0.42628539, -0.75815703]]).T,\n",
    "    'b3': np.array([[-0.16236698]])}\n",
    "y, caches = forward(X, parameters)\n",
    "print(f\"preds = {y}\")\n",
    "print(f\"layer2 cache = {caches['layer2']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "```console\n",
    "preds = [[0.03921668]\n",
    " [0.70498921]\n",
    " [0.19734387]\n",
    " [0.04728177]]\n",
    "layer2 cache = {'X_j': array([[0.        , 0.        , 4.18500916, 5.05850802],\n",
    "       [3.18040135, 0.        , 0.        , 0.        ],\n",
    "       [0.4074501 , 3.18141622, 0.        , 0.        ],\n",
    "       [0.        , 0.        , 2.72141639, 3.82321852]]), 'W_k': array([[-0.12673638, -0.56147088, -0.37550472],\n",
    "       [-1.36861282, -1.0335199 ,  0.39636757],\n",
    "       [ 1.21848065,  0.35877096, -0.47144628],\n",
    "       [-0.85750144,  1.07368134,  2.33660781]]), 'b_k': array([[ 1.50278553, -0.59545972,  0.52834106]]), 'Z_k': array([[ 2.2644603 ,  6.3372257 , 10.3750834 ],\n",
    "       [ 1.09971298, -2.38116247, -0.66591466],\n",
    "       [-2.90298025, -4.11228806,  1.63635184],\n",
    "       [ 1.54036335,  4.48582383,  8.17870168]])}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 - Loss Function\n",
    "\n",
    "In modern deep learning community, cost and loss is used interchangebly. People no longer distinguish the example-wise loss and the total cost. It is convention that using loss function to represent the cost function we were using. You can use `loss_fn()` which is given below to compute the cross entropy loss and monitor the training procedure later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_fn(preds, labels):\n",
    "    loss = np.mean(-labels * np.log(preds) - (1-labels) * np.log(1-preds))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Backward Propagation\n",
    "\n",
    "Back propagation is used to calculate the gradient of the loss function with respect to the parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Derivatives of Activation Functions\n",
    "The derivative of the activation function is essential to compute the gradients.  \n",
    "#### **(10%) Exercise 3.1: Derivatives of activation functions**\n",
    "- Derivative of sigmoid function: \n",
    "    $$\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$$\n",
    "- Derivative of hyperbolic tangent function: \n",
    "    $$\\tanh'(x) = 1 - \\tanh^2(x)$$\n",
    "- Derivative of relu function: \n",
    "    $$ReLU'(x) = \n",
    "        \\begin{cases}\n",
    "            0   & x \\leq 0 \\\\\n",
    "            1   & x > 0\n",
    "        \\end{cases}\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# sigmoid derivative\n",
    "def d_sigmoid(x):\n",
    "    y = 1 / (1 + np.exp(-x))\n",
    "    dydx = None\n",
    "\n",
    "    return dydx\n",
    "\n",
    "# tanh derivative\n",
    "def d_tanh(x):\n",
    "    y = np.tanh(x)\n",
    "    dydx = None\n",
    "\n",
    "    return dydx\n",
    "\n",
    "# relu derivative\n",
    "def d_relu(x):\n",
    "    dydx = None\n",
    "\n",
    "    return dydx\n",
    "### END CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Backward Loop\n",
    "Now, you can compute the gradients of the weights and biases from the last layer to the first layer using the intermediate results and the derivatives of the activation functions.\n",
    "\n",
    "#### **(30%) Exercise 3.2: Gradients computation**\n",
    "Complete `backward()` function. Loop the following equations to compute the gradients of the parameters.\n",
    "$$d\\mathbf{Z}^{[k]} = \\frac{\\partial{J}}{\\partial{\\mathbf{Z}^{[k]}}} = d\\mathbf{X}^{[k]} * g'^{[k]}(\\mathbf{Z}^{[k]})$$\n",
    "$$d\\mathbf{W}^{[k]} = \\frac{\\partial{J}}{\\partial{\\mathbf{W}^{[k]}}} = \\frac{1}{M}\\mathbf{X}^{[k-1]\\mathbf{T}} \\cdot d\\mathbf{Z}^{[k]}$$\n",
    "$$d\\mathbf{b}^{[k]} = \\frac{\\partial{J}}{\\partial{\\mathbf{b}^{[k]}}} = \\frac{1}{M} \\Sigma d\\mathbf{Z}^{[k]}$$\n",
    "$$d\\mathbf{X}^{[k-1]} = \\frac{\\partial{J}}{\\partial{\\mathbf{X}^{[k-1]}}} = d\\mathbf{Z}^{[k]} \\cdot \\mathbf{W}^{[k]\\mathbf{T}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward(preds, labels, caches, activation='relu'):\n",
    "    \"\"\"\n",
    "    backward() function computes gradients of the parameters from the last layer all the way back to the first layer.\n",
    "\n",
    "    Arguments:\n",
    "        preds -- numpy array, probabilities of predictions. shape: (number_of_examples, 1)\n",
    "        labels -- numpy array, ground truth. shape: (number_of_examples, 1)\n",
    "        caches -- nested dictionary. Stores all the intermediate results in each layer.\n",
    "        activation -- string, activation type for the hidden layers (exclude the final layer).\n",
    "\n",
    "    Returns:\n",
    "        grads -- dictionary, gradients of the parameters in each layer \n",
    "    \"\"\"\n",
    "\n",
    "    assert preds.shape == labels.shape\n",
    "    grads = {}\n",
    "    K = len(caches)\n",
    "    M = labels.shape[0]\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    dX_k = None\n",
    "    for k in reversed(range(1, K + 1)):\n",
    "        if k == K:\n",
    "            dZ_k = None  # final layer activation derivative\n",
    "        else:\n",
    "            if activation=='relu':\n",
    "                dZ_k = None\n",
    "            elif activation=='tanh':\n",
    "                dZ_k = None\n",
    "            elif activation=='sigmoid':\n",
    "                dZ_k = None\n",
    "            else:\n",
    "                dZ_k = dX_k\n",
    "        dW_k = None\n",
    "        db_k = None\n",
    "        dX_j = None\n",
    "        dX_k = None\n",
    "        grads['dW' + str(k)] = None\n",
    "        grads['db' + str(k)] = None\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "# Test backward()\n",
    "yhat = np.array([[1.78862847, 0.43650985]]).T\n",
    "y = np.array([[1, 0]]).T\n",
    "caches = {\n",
    "    'layer1': {\n",
    "        'X_j': np.array([\n",
    "            [0.09649747, -1.8634927],\n",
    "            [-0.2773882, -0.35475898],\n",
    "            [-0.08274148, -0.62700068],\n",
    "            [-0.04381817, -0.47721803]\n",
    "        ]).T,\n",
    "        'W_k': np.array([\n",
    "            [-1.31386475, 0.88462238, 0.88131804, 1.70957306],\n",
    "            [0.05003364, -0.40467741, -0.54535995, -1.54647732],\n",
    "            [0.98236743, -1.10106763, -1.18504653, -0.2056499]\n",
    "        ]).T,\n",
    "        'b_k': np.array([\n",
    "            [1.48614836],\n",
    "            [0.23671627],\n",
    "            [1.02378514]\n",
    "        ]).T,\n",
    "        'Z_k': np.array([\n",
    "            [-0.7129932, 0.62524497],\n",
    "            [-0.16051336, -0.76883635],\n",
    "            [-0.23003072, 0.74505627]\n",
    "        ]).T\n",
    "    },\n",
    "    'layer2': {\n",
    "        'X_j': np.array([\n",
    "            [1.97611078, -1.24412333],\n",
    "            [-0.62641691, -0.80376609],\n",
    "            [-2.41908317, -0.92379202]\n",
    "        ]).T,\n",
    "        'W_k': np.array([[-1.02387576, 1.12397796, -0.13191423]]).T,\n",
    "        'b_k': np.array([[-1.62328545]]),\n",
    "        'Z_k': np.array([[0.64667545, -0.35627076]]).T,\n",
    "    }\n",
    "}\n",
    "grads = backward(preds=yhat, labels=y, caches=caches)\n",
    "print(grads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "```console\n",
    "{'dW2': array([[-0.39202432],\n",
    "       [-0.13325855],\n",
    "       [-0.04601089]]), 'db2': array([[0.15187861]]), 'dW1': array([[0.41010002, 0.        , 0.05283652],\n",
    "       [0.07807203, 0.        , 0.01005865],\n",
    "       [0.13798444, 0.        , 0.01777766],\n",
    "       [0.10502167, 0.        , 0.0135308 ]]), 'db1': array([[-0.22007063,  0.        , -0.02835349]])}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Update Parameters\n",
    "\n",
    "In this section you will update the parameters of the model, using gradient descent: \n",
    "$$ W^{[k]} = W^{[k]} - \\alpha \\text{ } dW^{[k]} $$\n",
    "$$ b^{[k]} = b^{[k]} - \\alpha \\text{ } db^{[k]} $$\n",
    "where $\\alpha$ is the learning rate. After computing the updated parameters, store them back to the parameters dictionary. \n",
    "\n",
    "#### **(5%) Exercise 3.3:** Gradient Descent\n",
    "Complete `update()` to update your parameters using gradient descent.\n",
    "> Hint: use `str()` to convert integers into strings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    backward() function computes gradients of the parameters from the last layer all the way back to the first layer.\n",
    "\n",
    "    Arguments:\n",
    "        params -- dictionary, old parameters.\n",
    "        grads -- dictionary, gradients of the parameters in each layer \n",
    "        learning_rate -- scalar, controls the speed of training.\n",
    "\n",
    "    Returns:\n",
    "        params -- dictionary, updated parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    K = len(params) // 2\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    for k in range(K):\n",
    "        params['W' + str(k + 1)] = None\n",
    "        params['b' + str(k + 1)] = None\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "# test\n",
    "np.random.seed(4350)\n",
    "W1 = np.random.randn(4, 3)\n",
    "b1 = np.random.randn(1, 3)\n",
    "W2 = np.random.randn(3, 1)\n",
    "b2 = np.random.randn(1, 1)\n",
    "parameters = {\"W1\": W1,\n",
    "                \"b1\": b1,\n",
    "                \"W2\": W2,\n",
    "                \"b2\": b2}\n",
    "np.random.seed(3)\n",
    "dW1 = np.random.randn(4, 3)\n",
    "db1 = np.random.randn(1, 3)\n",
    "dW2 = np.random.randn(3, 1)\n",
    "db2 = np.random.randn(1, 1)\n",
    "grads = {\"dW1\": dW1,\n",
    "            \"db1\": db1,\n",
    "            \"dW2\": dW2,\n",
    "            \"db2\": db2}\n",
    "\n",
    "parameters = update(parameters, grads, 0.1)\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "```console\n",
    "W1 = [[-1.1575229   0.01536069 -1.43680034]\n",
    " [-0.08997952 -0.4242632  -0.60665739]\n",
    " [ 0.23316797  0.16990764 -0.66491577]\n",
    " [-0.00350843  2.57197082 -0.9778493 ]]\n",
    "b1 = [[-0.05466045 -0.66319382 -1.52207491]]\n",
    "W2 = [[-0.86080806]\n",
    " [ 0.46361905]\n",
    " [-1.0494594 ]]\n",
    "b2 = [[0.71541112]]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Train a K-layer Neural Network\n",
    "Put up together forward and backward propagation and train a neural network with any number of layers\n",
    "\n",
    "#### **(20%) Exercise 4.1:** Training\n",
    "Complete `train()` function. You can tune your model by using different `layers_dims`, `activation`, `learning_rate`, `num_iterations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    inputs, labels, params, activation='relu', learning_rate=0.0075, num_iterations=3000, print_cost=False\n",
    "):\n",
    "    \"\"\"\n",
    "    train() function performs forward and backward propagation to obtain updated parameters.\n",
    "\n",
    "    Arguments:\n",
    "        inputs -- numpy array, original feature matrix. shape: (number_of_examples, original_feature_dimension)\n",
    "        labels -- numpy array, ground truth. shape: (number_of_examples, 1)\n",
    "        params -- dictionary, stores weights and biases connects (k-1)th layer to kth layer.\n",
    "        activation -- string, activation type for the hidden layers (exclude the final layer).\n",
    "        learning_rate -- scalar, controls the speed of training.\n",
    "        num_iterations -- scalar, numbers of training loops.\n",
    "        print_cost -- bool, to print averaged loss in every 100 iterations or not.\n",
    "\n",
    "    Returns:\n",
    "        params -- dictionary, updated parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    grads = {}\n",
    "    losses = []  # to keep track of the cost\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    for i in range(1, num_iterations+1):\n",
    "        preds, caches = None\n",
    "        loss = None\n",
    "        grads = None\n",
    "        params = None\n",
    "        if print_cost and not (i % 100):\n",
    "            print(f\"Cost after iteration {i}: {np.squeeze(loss)}\")\n",
    "        losses.append(loss)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    plt.plot(np.squeeze(losses))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title('Learning rate =' + str(learning_rate))\n",
    "    # plt.show()\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "# Training\n",
    "np.random.seed(4350)\n",
    "\n",
    "### START CODE HERE ###\n",
    "layers_dims = None\n",
    "params = None  # init params\n",
    "### END CODE HERE ###\n",
    "\n",
    "params = train(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    params,\n",
    "    activation='relu',\n",
    "    num_iterations=5000,\n",
    "    learning_rate=0.03,\n",
    "    # num_iterations=13500,\n",
    "    # learning_rate=0.01,\n",
    "    print_cost=True\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "train_preds, _ = forward(train_X, params)\n",
    "train_preds = train_preds > 0.5\n",
    "train_accuracy = np.sum(train_preds == train_y) / train_y.shape[0]\n",
    "print(f\"train_accuracy: {train_accuracy}\")\n",
    "test_preds, _ = forward(test_X, params)\n",
    "test_preds = test_preds > 0.5\n",
    "test_accuracy = np.sum(test_preds == test_y) / test_y.shape[0]\n",
    "print(f\"test_accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "```console\n",
    "Cost after iteration 100: 0.6555767625124284\n",
    "Cost after iteration 200: 0.6467861650031772\n",
    "Cost after iteration 300: 0.644673251343426\n",
    "Cost after iteration 400: 0.6441493622996151\n",
    "Cost after iteration 500: 0.6440171212577572\n",
    "Cost after iteration 600: 0.6439833108092134\n",
    "Cost after iteration 700: 0.6439744892856368\n",
    "Cost after iteration 800: 0.6439720291615333\n",
    "Cost after iteration 900: 0.6439711781357069\n",
    "Cost after iteration 1000: 0.6439707173668276\n",
    "Cost after iteration 1100: 0.6439703266137381\n",
    "Cost after iteration 1200: 0.6439699125428286\n",
    "Cost after iteration 1300: 0.6439694437892591\n",
    "Cost after iteration 1400: 0.643968904782073\n",
    "Cost after iteration 1500: 0.6439682828411811\n",
    "Cost after iteration 1600: 0.643967564970407\n",
    "Cost after iteration 1700: 0.643966720541062\n",
    "Cost after iteration 1800: 0.6439657237669492\n",
    "Cost after iteration 1900: 0.6439645425002293\n",
    "Cost after iteration 2000: 0.6439631343005117\n",
    "Cost after iteration 2100: 0.6439614362611531\n",
    "Cost after iteration 2200: 0.6439593737815364\n",
    "Cost after iteration 2300: 0.6439567865212631\n",
    "Cost after iteration 2400: 0.6439534598397181\n",
    "Cost after iteration 2500: 0.6439492745389768\n",
    "...\n",
    "Cost after iteration 4700: 0.25480665125699675\n",
    "Cost after iteration 4800: 0.24463425857274818\n",
    "Cost after iteration 4900: 0.1242258171168041\n",
    "Cost after iteration 5000: 0.29679326419487606\n",
    "\n",
    "train_accuracy: 0.7607655502392344\n",
    "test_accuracy: 0.78\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice the training accuracy is relatively low and the later stage of the training is very fluctuated. The loss has the trend of getting smaller. The test accuracy is relatively high (compare to 68% in assignment 2). These are the good signs. Can you adjust the hyperparameters to train a better model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Your Own Images\n",
    "You can upload new images to `/images` folder and test them using the model you've trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prediction function\n",
    "def predict(X, params):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a K-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    pred -- predicted classes for the given data X\n",
    "    \"\"\"\n",
    "    \n",
    "    M = X.shape[0]\n",
    "    yhat, caches = forward(X, params, activation='relu')\n",
    "    pred = (yhat > 0.5).astype(np.float32)\n",
    "        \n",
    "    return pred\n",
    "\n",
    "\n",
    "# Make \n",
    "file = \"snow_leopard.jpg\"   # change this to the name of your image file \n",
    "# preprocess the image to fit your algorithm.\n",
    "fname = \"images/\" + file\n",
    "im = cv2.imread(fname)\n",
    "im_rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "imresize = cv2.resize(im_rgb, image_size[:-1])\n",
    "imfloat = imresize/255.\n",
    "imflatten = imfloat.reshape(1,-1)\n",
    "pred_cls = predict(imflatten, params)\n",
    "print(pred_cls)\n",
    "plt.imshow(imresize)\n",
    "print(f\"Your model predicts a {classes[int(np.squeeze(pred_cls))].decode('utf-8')} picture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Congrats on finishing this assignment! "
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
