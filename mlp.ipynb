{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron Neural Network\n",
    "\n",
    "Welcome to your third assignment. You will build a Multi-Layer Perceptron (MLP) neural network in this assignment. The goal of building such a model is to classify images of wearings.\n",
    "\n",
    "## Exercises:\n",
    "1. $\\color{violet}{\\textbf{(10\\%) Data Preprocessing}}$\n",
    "2. $\\color{violet}{\\textbf{(5\\%) Parameter Initialization}}$\n",
    "3. $\\color{violet}{\\textbf{(15\\%) Linear Model and Activations}}$\n",
    "4. $\\color{violet}{\\textbf{(5\\%) Cross Entropy Loss}}$\n",
    "5. $\\color{violet}{\\textbf{(25\\%) Gradient Computation}}$\n",
    "6. $\\color{violet}{\\textbf{(30\\%) Gradient Descent Optimization}}$\n",
    "7. $\\color{violet}{\\textbf{(10\\%) Accuracy Evaluation}}$\n",
    "\n",
    "## Instructions:\n",
    "- Write your code only between the $\\color{green}{\\textbf{\\small \\#\\#\\# START CODE HERE \\#\\#\\#}}$ and $\\color{green}{\\textbf{\\small \\#\\#\\# END CODE HERE \\#\\#\\#}}$ commented lines. \n",
    "- $\\color{red}{\\textbf{Change code out of the designated area at your own risk.}}$\n",
    "- Reference answers are provided after a certain coding blocks. Be aware if your answer is different from the reference..\n",
    "- **Need to install [Torchvision](https://pytorch.org/vision/stable/index.html)**\n",
    "    ```console\n",
    "    pip install torchvision\n",
    "    ```\n",
    "**You will learn:**\n",
    "- One-hot encoding for multi-class targets.\n",
    "- Rectified Linear Unit (ReLU) activation function.\n",
    "- Softmax activation.\n",
    "- Forward and backward propagation of a generic MLP model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset\n",
    "Torchvision provides a descent pool of datasets. We'll load one of the built-in dataset, [FashionMNIST](https://pytorch.org/vision/stable/datasets.html) to investigate multi-class classification using a generic Multi-Layer Perceptron (MLP) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training features array shape: (60000, 28, 28), test features array shape: (10000, 28, 28)\n",
      "training labels array shape: (60000,), test labels array shape: (10000,)\n",
      "{'T-shirt/top': 0, 'Trouser': 1, 'Pullover': 2, 'Dress': 3, 'Coat': 4, 'Sandal': 5, 'Shirt': 6, 'Sneaker': 7, 'Bag': 8, 'Ankle boot': 9}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "train_set = datasets.FashionMNIST('./data', train=True, download=True)\n",
    "test_set = datasets.FashionMNIST('./data', train=False, download=True)\n",
    "\n",
    "# Extract important arrays and information\n",
    "raw_features_train = train_set.data.numpy()\n",
    "raw_labels_train = train_set.targets.numpy()\n",
    "raw_features_test = test_set.data.numpy()\n",
    "raw_labels_test = test_set.targets.numpy()\n",
    "category_keys = train_set.classes\n",
    "category_vals = range(len(category_keys))\n",
    "category_dict = dict((map(lambda i,j : (i,j), category_keys, category_vals)))\n",
    "\n",
    "print(f\"training features array shape: {raw_features_train.shape}, test features array shape: {raw_features_test.shape}\")\n",
    "print(f\"training labels array shape: {raw_labels_train.shape}, test labels array shape: {raw_labels_test.shape}\")\n",
    "print(category_dict)\n",
    "\n",
    "# Visualize random image and target pair from training dataset\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 4, 4\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = np.random.randint(0, len(train_set))\n",
    "    img, label = train_set[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(category_keys[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img, cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess the Data\n",
    "A $\\mathcal{C}$-calss dataset is made up with a feature matrix: $\\mathbf{X} = [^{(1)}\\mathbf{x}, ^{(2)}\\mathbf{x}, ..., ^{(M)}\\mathbf{x}]^T$. and a one-hot encoded target matrix $\\mathbf{Y} = [^{(1)}\\mathbf{y}, ^{(2)}\\mathbf{y}, ..., ^{(M)}\\mathbf{y}]^T$. Where $M$ is the total number of instances in the dataset, $^{(m)}\\mathbf{x} = [^{(m)}x_1, ..., ^{(m)}x_n]$ is a normalized and flattened row vector bears $n$ feature values, and $^{(m)}\\mathbf{y} = [0, ..., 0, 1, 0, ..., 0]$ is a one-hot encoded row vector.\n",
    "\n",
    "- A grey-scale image can be represented by a **2-dimensional array with shape $(width, height)$**. Where, $width$ indicates number of pixels on horizontal direction, $height$ indicates number of pixels on vertical direction.\n",
    "- We can use an **integer ranged 0~255** to describe a pixel's color intensity. However, it is easier for your computer to handlle float values.\n",
    "- We would like to convert an image array into a row vector, or a **2d array with shape $(1, width*height)$**. So that, we can stack these row vectors on the first dimensions to form a feature matrix.\n",
    "- We also would like to encode target array into one-hot format.\n",
    "\n",
    "\n",
    "We will access the raw data by extracting it from the dataloaders. Then, process and prepare the raw data so that it can be used in later steps.\n",
    "\n",
    "### $\\color{violet}{\\textbf{(10\\%) Exercise 1: Data Preprocessing}}$\n",
    "1. Reshape feature array.\n",
    "2. One-hot encode target array\n",
    "3. Rescale feature arrary, represent each pixel with a float numbers in range 0~1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training features shape: (60000, 784), test feature shape: (10000, 784), training target shape: (60000, 10), test target shape: (10000, 10)\n",
      "sample features slice: \n",
      "[[0.80392157 0.81176471 0.8        0.80784314 0.83529412 0.83921569\n",
      "  0.84313725 0.84705882 0.85490196 0.90980392 0.77254902 0.\n",
      "  0.         0.00784314 0.01960784 0.02745098 0.00784314 0.00392157\n",
      "  0.         0.        ]\n",
      " [0.68235294 0.67843137 0.70196078 0.68627451 0.70980392 0.79607843\n",
      "  0.65490196 0.68235294 0.76470588 0.20784314 0.         0.\n",
      "  0.         0.         0.21568627 0.7372549  0.6745098  0.68627451\n",
      "  0.78039216 0.70196078]\n",
      " [0.         0.45882353 0.79607843 0.71372549 0.10980392 0.\n",
      "  0.38039216 0.45882353 0.38823529 0.38039216 0.45098039 0.08627451\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.37254902 0.4627451  0.25098039 0.17647059 0.2745098  0.21176471\n",
      "  0.24705882 0.76862745 0.6627451  0.41568627 0.43921569 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n",
      "sample labels slice: \n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (≈ 8 lines of code)\n",
    "# Reshape feature and target arrays\n",
    "reshaped_features_train = raw_features_train.reshape(raw_labels_train.shape[0], -1)  # (60000, 28, 28) -> (60000, 784)\n",
    "reshaped_features_test = raw_features_test.reshape(raw_features_test.shape[0], -1)  # (10000, 28, 28) -> (10000, 784)\n",
    "# One hot encode targets\n",
    "onehot_labels_train = np.zeros((raw_labels_train.shape[0], len(category_dict)))  # (60000,) -> (60000, 10)\n",
    "onehot_labels_train[np.arange(raw_labels_train.shape[0]), raw_labels_train] = 1\n",
    "onehot_labels_test = np.zeros((raw_labels_test.shape[0], len(category_dict)))  # (10000,) -> (10000, 10)\n",
    "onehot_labels_test[np.arange(raw_labels_test.shape[0]), raw_labels_test] = 1\n",
    "# Rescale features\n",
    "rescale_feature_train = reshaped_features_train / 255\n",
    "rescale_feature_test = reshaped_features_test / 255\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Rename for later usage\n",
    "features_train = rescale_feature_train\n",
    "features_test = rescale_feature_test\n",
    "labels_train = onehot_labels_train\n",
    "labels_test = onehot_labels_test\n",
    "print(f\"training features shape: {features_train.shape}, test feature shape: {features_test.shape}, training target shape: {labels_train.shape}, test target shape: {labels_test.shape}\")\n",
    "print(f\"sample features slice: \\n{features_train[3321:3325, 380:400]}\")\n",
    "print(f\"sample labels slice: \\n{labels_train[3321:3325]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "training features shape: (60000, 784), test feature shape: (10000, 784), training target shape: (60000, 10), test target shape: (10000, 10)\n",
    "sample features slice: \n",
    "[[0.80392157 0.81176471 0.8        0.80784314 0.83529412 0.83921569\n",
    "  0.84313725 0.84705882 0.85490196 0.90980392 0.77254902 0.\n",
    "  0.         0.00784314 0.01960784 0.02745098 0.00784314 0.00392157\n",
    "  0.         0.        ]\n",
    " [0.68235294 0.67843137 0.70196078 0.68627451 0.70980392 0.79607843\n",
    "  0.65490196 0.68235294 0.76470588 0.20784314 0.         0.\n",
    "  0.         0.         0.21568627 0.7372549  0.6745098  0.68627451\n",
    "  0.78039216 0.70196078]\n",
    " [0.         0.45882353 0.79607843 0.71372549 0.10980392 0.\n",
    "  0.38039216 0.45882353 0.38823529 0.38039216 0.45098039 0.08627451\n",
    "  0.         0.         0.         0.         0.         0.\n",
    "  0.         0.        ]\n",
    " [0.37254902 0.4627451  0.25098039 0.17647059 0.2745098  0.21176471\n",
    "  0.24705882 0.76862745 0.6627451  0.41568627 0.43921569 0.\n",
    "  0.         0.         0.         0.         0.         0.\n",
    "  0.         0.        ]]\n",
    "sample labels slice: \n",
    "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
    " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forward Pass\n",
    "A Multi-Layer Perceptron (MLP) model is featured with multiple layers of transformed features. Any two adjacent layer are connected by a linear model and an activation function. The linear model is governed by a set of weight parameters and a set of bias parameters. The general structure of an MLP model is shown below.\n",
    "\n",
    "![](./model.png)\n",
    "\n",
    "### 2.1. Initialize Parameters\n",
    "A linear model governed by weights $\\mathbf{W}^{[l]}$ and $\\mathbf{b}^{[l]}$ transforms features, $\\mathbf{X}^{[l-1]}$ in the $l-1$-th layer to $\\mathbf{Z}^{[l]}$ in the $l$-th layer.\n",
    "\n",
    "$\\mathbf{Z}^{[l]} = \\mathbf{X}^{[l-1]} \\cdot \\mathbf{W}^{[l]T} + \\mathbf{b}^{[l]}$\n",
    "\n",
    "Assume $\\mathbf{X}^{[l-1]}$ has $N_{l-1}$ features and $\\mathbf{X}^{[l]}$ has $N_{l}$ features, then $\\mathbf{W}^{[l]}$ is with shape $(N_l, N_{l-1})$, $\\mathbf{b}^{[l]}$ is with shape $(1, N_l)$\n",
    "\n",
    "\n",
    "In order to get the MLP model prepared, we need to initialize weight matrix and bias vector in every layer.\n",
    "\n",
    "### $\\color{violet}{\\textbf{(5\\%) Exercise 2: Parameter Initialization}}$\n",
    "Define a function to initialize weights and biases parameters and save these parameters in a **dictionary**. \n",
    "- Input sizes of all the layers (**include the input layer**) using a **list**.\n",
    "- Use a **`for` loop** to randomly initialize $\\mathbf{W}^{[l]}$ and $\\mathbf{b}^{[l]}$ for the $l$-th layer.\n",
    "- You may find `np.random.normal()` is a useful function when you were trying to create random values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['W1', 'b1', 'W2', 'b2', 'W3', 'b3'])\n",
      "{'W1': array([[ 3.68455981e-05, -1.38782645e-04, -7.23142013e-05,\n",
      "         9.20641119e-05, -1.95355264e-05,  3.87255258e-05],\n",
      "       [-6.00629078e-05,  3.25435259e-06,  6.46665063e-05,\n",
      "        -7.84748031e-05,  1.41884652e-04, -6.15479349e-05],\n",
      "       [-1.10127440e-04, -6.82385574e-05,  7.96622349e-05,\n",
      "         5.25934678e-05, -1.78411966e-04,  1.28629057e-04],\n",
      "       [ 5.66879246e-05,  8.95999019e-05, -1.62874713e-05,\n",
      "         3.51109174e-05, -1.03338181e-04, -8.88623865e-05],\n",
      "       [ 5.10860443e-05,  1.19661547e-04,  8.64757553e-05,\n",
      "        -1.41101724e-04,  1.38003186e-04, -1.06812125e-04]]), 'b1': array([[ 6.38004843e-05, -6.19666755e-05,  1.12931166e-04,\n",
      "        -3.12290798e-05,  5.59708416e-05]]), 'W2': array([[ 1.02685210e-04, -1.28307992e-04, -4.03018811e-05,\n",
      "        -4.09270011e-05, -1.84834135e-05],\n",
      "       [ 1.71466862e-04, -9.69611346e-05,  4.40873689e-05,\n",
      "        -1.10516782e-04, -5.17959570e-05],\n",
      "       [-1.87055902e-04, -6.31341925e-05, -5.47992268e-05,\n",
      "         3.67804566e-05, -1.05795654e-04],\n",
      "       [ 1.24964681e-04,  3.60854003e-05,  1.30113815e-04,\n",
      "         1.01177937e-04,  2.22286230e-05]]), 'b2': array([[ 1.16709537e-04, -1.12496462e-04, -9.47925077e-05,\n",
      "        -4.49789001e-05]]), 'W3': array([[ 1.01049508e-04, -3.52965408e-05,  2.06672986e-04,\n",
      "        -1.67359842e-05],\n",
      "       [-7.34788781e-05,  1.28189925e-05,  3.67334763e-05,\n",
      "        -3.00482492e-05],\n",
      "       [ 9.36343566e-05, -3.80351405e-05,  6.59823461e-06,\n",
      "         5.61206851e-05]]), 'b3': array([[ 2.57409859e-04,  1.69968799e-05, -7.22321090e-05]])}\n"
     ]
    }
   ],
   "source": [
    "def init_params(layer_sizes):\n",
    "    \"\"\" Parameter initialization function\n",
    "    Args:\n",
    "        layer_sizes -- list/tuple, (input size, ..., hidden layer size, ..., output size)\n",
    "    Returns:\n",
    "        parameters -- dictionary, contains parameters: Wi and bi, i is the i-th layer\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        parameters['W'+str(i+1)] = np.random.normal(0, 0.0001, size=(layer_sizes[i+1], layer_sizes[i]))\n",
    "        parameters['b'+str(i+1)] = np.random.normal(0, 0.0001, size=(1, layer_sizes[i+1]))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return parameters\n",
    "\n",
    "# Sanity check\n",
    "np.random.seed(3321)\n",
    "dummy_layer_sizes = (6, 5, 4, 3)  # (input size, layer1 size, layer2 size, output size)\n",
    "dummy_params = init_params(dummy_layer_sizes)\n",
    "print(dummy_params.keys())\n",
    "print(dummy_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "dict_keys(['W1', 'b1', 'W2', 'b2', 'W3', 'b3'])\n",
    "{'W1': array([[ 3.68455981e-05, -1.38782645e-04, -7.23142013e-05,\n",
    "         9.20641119e-05, -1.95355264e-05,  3.87255258e-05],\n",
    "       [-6.00629078e-05,  3.25435259e-06,  6.46665063e-05,\n",
    "        -7.84748031e-05,  1.41884652e-04, -6.15479349e-05],\n",
    "       [-1.10127440e-04, -6.82385574e-05,  7.96622349e-05,\n",
    "         5.25934678e-05, -1.78411966e-04,  1.28629057e-04],\n",
    "       [ 5.66879246e-05,  8.95999019e-05, -1.62874713e-05,\n",
    "         3.51109174e-05, -1.03338181e-04, -8.88623865e-05],\n",
    "       [ 5.10860443e-05,  1.19661547e-04,  8.64757553e-05,\n",
    "        -1.41101724e-04,  1.38003186e-04, -1.06812125e-04]]), 'b1': array([[ 6.38004843e-05, -6.19666755e-05,  1.12931166e-04,\n",
    "        -3.12290798e-05,  5.59708416e-05]]), 'W2': array([[ 1.02685210e-04, -1.28307992e-04, -4.03018811e-05,\n",
    "        -4.09270011e-05, -1.84834135e-05],\n",
    "       [ 1.71466862e-04, -9.69611346e-05,  4.40873689e-05,\n",
    "        -1.10516782e-04, -5.17959570e-05],\n",
    "       [-1.87055902e-04, -6.31341925e-05, -5.47992268e-05,\n",
    "         3.67804566e-05, -1.05795654e-04],\n",
    "       [ 1.24964681e-04,  3.60854003e-05,  1.30113815e-04,\n",
    "         1.01177937e-04,  2.22286230e-05]]), 'b2': array([[ 1.16709537e-04, -1.12496462e-04, -9.47925077e-05,\n",
    "        -4.49789001e-05]]), 'W3': array([[ 1.01049508e-04, -3.52965408e-05,  2.06672986e-04,\n",
    "        -1.67359842e-05],\n",
    "       [-7.34788781e-05,  1.28189925e-05,  3.67334763e-05,\n",
    "        -3.00482492e-05],\n",
    "       [ 9.36343566e-05, -3.80351405e-05,  6.59823461e-06,\n",
    "         5.61206851e-05]]), 'b3': array([[ 2.57409859e-04,  1.69968799e-05, -7.22321090e-05]])}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Forward Propagation\n",
    "A linear model transforms features, $\\mathbf{X}^{[l-1]}$ in the $l-1$-th layer to $\\mathbf{Z}^{[l]}$ in the $l$-th layer. Then we apply a Linear Rectified Unit (ReLU) function on $\\mathbf{Z}^{[l]}$ to form new features $\\mathbf{Z}^{[l]}$ in the $l$-th layer.\n",
    "\n",
    "$$\\mathbf{Z}^{[l]} = \\mathbf{X}^{[l-1]} \\cdot \\mathbf{W}^{[l]T} + \\mathbf{b}^{[l]}$$\n",
    "\n",
    "$$\\mathbf{X}^{[l]} = ReLU(\\mathbf{Z}^{[l]}) = \n",
    "    \\begin{cases}\n",
    "        0   & z \\leq 0 \\\\\n",
    "        z   & z > 0\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "or,\n",
    "\n",
    "$$\\mathbf{X}^{[l]} = \\sigma (\\mathbf{Z}^{[l]}) = \\frac{1}{1 + e^{-\\mathbf{Z}^{[l]}}}$$\n",
    "\n",
    "The last layer needs to be activcated by a softmax function.\n",
    "\n",
    "#### $$\\hat{y}_i = \\frac{e^{z^{[L]}_i}}{\\sum^C_{i=1} e^{z^{[L]}_i}}$$\n",
    "\n",
    "The maxtrix $\\mathbf{Z}^{[L]}$ has shape: $(M, C)$, where $M$ is the number of samples, $C$ is the number of classes. When applying softmax activation, we only want to apply it on the 2nd dimension (1st axis in numpy). So that each row in $\\mathbf{Z}^{[L]}$ will be converted to probabilities.\n",
    "\n",
    "### $\\color{violet}{\\textbf{(15\\%) Exercise 3: Linear Model and Activations}}$\n",
    "- Define sigmoid function\n",
    "- Define ReLU function and softmax function.\n",
    "- Define linear model.\n",
    "- Define forward propagation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]]\n",
      "cache dictionary keys: dict_keys(['X0', 'Z1', 'X1', 'Z2', 'X2', 'Z3'])\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (≈ 9 lines of code)\n",
    "def sigmoid(x):\n",
    "    \"\"\" Sigmoid function\n",
    "    Args:\n",
    "        x -- scalar/array\n",
    "    Returns:\n",
    "        y -- scalar/array, y = 1 / (1 + e^{-x})\n",
    "    \"\"\"\n",
    "    y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "    return y\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\" Rectified linear unit function\n",
    "    Args:\n",
    "        x -- scalar/array\n",
    "    Returns:\n",
    "        y -- scalar/array, 0 if x <= 0, x if x >0\n",
    "    \"\"\"\n",
    "    y = np.maximum(0, x)\n",
    "\n",
    "    return y\n",
    "\n",
    "def softmax(out_features):\n",
    "    \"\"\" Softmax function\n",
    "    Args:\n",
    "        out_features: array with shape (M, C)\n",
    "    Returns:\n",
    "        probs: array with same shape as arr\n",
    "    \"\"\"\n",
    "    probs = np.exp(out_features) / np.sum(np.exp(out_features), axis=-1, keepdims=True)\n",
    "    \n",
    "    return probs\n",
    "\n",
    "def linear(in_features, weight, bias):\n",
    "    \"\"\" Linear model\n",
    "    Args:\n",
    "        in_features (matrix): 2d array with shape (M, N^[l-1])\n",
    "        weight (matrix): 2d array with shape (N^[l], N^[l-1])\n",
    "        bias (row vector): 2d array with shape (1, N^[l])\n",
    "    Returns:\n",
    "        out_features (matrix): 2d array with shape (M, , N^[l])\n",
    "    \"\"\"\n",
    "    out_features = in_features @ weight.T + bias\n",
    "        \n",
    "    return out_features\n",
    "\n",
    "def forward(in_features, params, activation='relu'):\n",
    "    \"\"\" Forward propagation process\n",
    "    Args:\n",
    "        in_features (matrix): 2d array with shape (M, N^[0])\n",
    "        params: dictionary, stores W's and b's\n",
    "    Returns:\n",
    "        prediction: 2d array with shape (M, C)\n",
    "        cache: dictionary, stores intemediate X's and Z's.\n",
    "    \"\"\"\n",
    "    num_layers = int(len(params) / 2)\n",
    "    cache = {'X0': in_features}\n",
    "    for i in range(num_layers - 1):\n",
    "        cache['Z' + str(i+1)] = linear(cache['X' + str(i)], params['W' + str(i+1)], params['b' + str(i+1)])\n",
    "        if activation == 'relu':\n",
    "            cache['X' + str(i+1)] = relu(cache['Z' + str(i+1)])\n",
    "        elif activation == 'sigmoid':\n",
    "            cache['X' + str(i+1)] = sigmoid(cache['Z' + str(i+1)])\n",
    "    cache['Z' + str(i+2)] = linear(cache['X' + str(i+1)], params['W' + str(i+2)], params['b' + str(i+2)])\n",
    "    prediction = softmax(cache['Z' + str(i+2)])\n",
    "\n",
    "    return prediction, cache\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Sanity check\n",
    "np.random.seed(3321)\n",
    "dummy_features = np.random.randn(8,6)\n",
    "dummy_preds, dummy_cache = forward(dummy_features, dummy_params, activation='relu')\n",
    "print(dummy_preds)\n",
    "print(f\"cache dictionary keys: {dummy_cache.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "[[0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]]\n",
    "cache dictionary keys: dict_keys(['X0', 'Z1', 'X1', 'Z2', 'X2', 'Z3'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Class Cross Entropy Loss\n",
    "For multi-class classification problem, it is quite standard to use a general form of cross entropy function to evaluate model prediction vs. target. \n",
    "#### $\\mathcal{L}(\\mathbf{\\hat{Y}}, \\mathbf{Y}) = -\\frac{1}{M} \\sum_{m=1}^M \\sum_{c=1}^C {^{(m)} y_c} \\log {^{(m)} \\hat{y}_c}$\n",
    "\n",
    "### $\\color{violet}{\\textbf{(5\\%) Exercise 4: Cross Entropy Loss}}$\n",
    "Define a cross entropy function to compute the average loss between prediction matrix and target matrix. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE loss of dummy prediction: 1.0986472024186367\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (≈ 1 line of code)\n",
    "def ce_loss(predictions, labels):\n",
    "    \"\"\" Cross entropy loss function\n",
    "    Args:\n",
    "        predictions (matrix): 2d array with shape (M, C)\n",
    "        labels (matrix): 2d array with shape (M, C)\n",
    "    Returns:\n",
    "        loss: scalar, averaged ce loss\n",
    "    \"\"\"\n",
    "    errors = -np.sum(labels * np.log(predictions + 1e-10), axis=1)\n",
    "\n",
    "    return errors.mean()\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Sanity check\n",
    "dummy_labels = np.zeros((8, 3))\n",
    "dummy_labels[np.arange(8), np.random.randint(0, 3, (8,))] = 1\n",
    "dummy_loss = ce_loss(dummy_preds, dummy_labels)\n",
    "print(f\"CE loss of dummy prediction: {dummy_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "CE loss of dummy prediction: 1.098647202419345\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Back-Propagation\n",
    "In order to know how to update weights and biases, we need to compute the gradient of the loss. This requires compute gradient of loss w.r.t. the variables in the last layer first. Then compute gradient of loss w.r.t. the variables in the previous layer next. And so on, until the gradient of loss w.r.t. the first layer is computed. \n",
    "\n",
    "Due to the fact that the last layer is softmax activated, $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}^{[L]}}$ can be computed differently without explicitly solve for derivative of softmax function.\n",
    "$$d\\mathbf{Z}^{[L]} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}^{[L]}} = \\mathbf{\\hat{Y}} - \\mathbf{Y} $$\n",
    "\n",
    "Then, from last layer $L$ to first layer, we need to repeatedly computing the gradient of loss according to the chain rule. The computation of a general layer $[l]$ is as follows.\n",
    "$$d\\mathbf{W}^{[l]} = d\\mathbf{Z}^{[l]} \\frac{\\partial \\mathbf{Z}^{[l]}}{\\partial{\\mathbf{W}^{[l]}}} = d\\mathbf{Z}^{[l]T} \\cdot \\mathbf{X}^{[l-1]}$$\n",
    "$$d\\mathbf{b}^{[l]} = d\\mathbf{Z}^{[l]} \\frac{\\partial \\mathbf{Z}^{[l]}}{\\partial{\\mathbf{b}^{[l]}}} = mean(d\\mathbf{Z}^{[l]}, axis=0, keepdims=True)$$\n",
    "$$d\\mathbf{X}^{[l-1]} = d\\mathbf{Z}^{[l]} \\frac{\\partial \\mathbf{Z}^{[l]}}{\\partial{\\mathbf{X}^{[l-1]}}} = d\\mathbf{Z}^{[l]} \\cdot \\mathbf{W}^{[l]}$$\n",
    "$$d\\mathbf{Z}^{[l-1]} = d\\mathbf{X}^{[l-1]} * relu'(\\mathbf{Z}^{[l-1]})$$\n",
    "\n",
    "\n",
    "### $\\color{violet}{\\textbf{(25\\%) Exercise 5: Gradient Computation}}$\n",
    "- Define derivative of ReLU function.\n",
    "- Define a function to perform backward propagation to compute gradient of the (cross entropy) loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['dZ3', 'dW3', 'db3', 'dX2', 'dZ2', 'dW2', 'db2', 'dX1', 'dZ1', 'dW1', 'db1'])\n",
      "{'dZ3': array([[-0.66660332,  0.33331653,  0.33328679],\n",
      "       [ 0.33339668,  0.33331653, -0.66671321],\n",
      "       [-0.66660332,  0.33331653,  0.33328679],\n",
      "       [ 0.33339668,  0.33331653, -0.66671321],\n",
      "       [ 0.33339668, -0.66668347,  0.33328679],\n",
      "       [ 0.33339668,  0.33331653, -0.66671321],\n",
      "       [ 0.33339668,  0.33331653, -0.66671321],\n",
      "       [ 0.33339668, -0.66668347,  0.33328679]]), 'dW3': array([[ 7.77983020e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [ 7.77962299e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [-1.55594532e-04,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00]]), 'db3': array([[ 0.08339668,  0.08331653, -0.16671321]]), 'dX2': array([[-6.06445677e-05,  1.51249634e-05, -1.23325920e-04,\n",
      "         1.98449678e-05],\n",
      "       [-5.32294163e-05,  1.78635631e-05,  7.67488316e-05,\n",
      "        -5.30117015e-05],\n",
      "       [-6.06445677e-05,  1.51249634e-05, -1.23325920e-04,\n",
      "         1.98449678e-05],\n",
      "       [-5.32294163e-05,  1.78635631e-05,  7.67488316e-05,\n",
      "        -5.30117015e-05],\n",
      "       [ 1.13883818e-04, -3.29905699e-05,  4.66135899e-05,\n",
      "         3.31572328e-05],\n",
      "       [-5.32294163e-05,  1.78635631e-05,  7.67488316e-05,\n",
      "        -5.30117015e-05],\n",
      "       [-5.32294163e-05,  1.78635631e-05,  7.67488316e-05,\n",
      "        -5.30117015e-05],\n",
      "       [ 1.13883818e-04, -3.29905699e-05,  4.66135899e-05,\n",
      "         3.31572328e-05]]), 'dZ2': array([[-6.06445677e-05,  0.00000000e+00, -0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [-5.32294163e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "        -0.00000000e+00],\n",
      "       [-6.06445677e-05,  0.00000000e+00, -0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [-5.32294163e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "        -0.00000000e+00],\n",
      "       [ 1.13883818e-04, -0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [-5.32294163e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "        -0.00000000e+00],\n",
      "       [-5.32294163e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "        -0.00000000e+00],\n",
      "       [ 1.13883818e-04, -0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00]]), 'dW2': array([[-3.29568730e-08,  4.96018508e-09, -7.53891708e-08,\n",
      "        -1.50064992e-08,  5.73359920e-08],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00]]), 'db2': array([[-1.33048955e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00]]), 'dX1': array([[-6.22730015e-09,  7.78118273e-09,  2.44409016e-09,\n",
      "         2.48200029e-09,  1.12091862e-09],\n",
      "       [-5.46587377e-09,  6.82975954e-09,  2.14524561e-09,\n",
      "         2.17852038e-09,  9.83861313e-10],\n",
      "       [-6.22730015e-09,  7.78118273e-09,  2.44409016e-09,\n",
      "         2.48200029e-09,  1.12091862e-09],\n",
      "       [-5.46587377e-09,  6.82975954e-09,  2.14524561e-09,\n",
      "         2.17852038e-09,  9.83861313e-10],\n",
      "       [ 1.16941838e-08, -1.46122041e-08, -4.58973210e-09,\n",
      "        -4.66092316e-09, -2.10496171e-09],\n",
      "       [-5.46587377e-09,  6.82975954e-09,  2.14524561e-09,\n",
      "         2.17852038e-09,  9.83861313e-10],\n",
      "       [-5.46587377e-09,  6.82975954e-09,  2.14524561e-09,\n",
      "         2.17852038e-09,  9.83861313e-10],\n",
      "       [ 1.16941838e-08, -1.46122041e-08, -4.58973210e-09,\n",
      "        -4.66092316e-09, -2.10496171e-09]]), 'dZ1': array([[-6.22730015e-09,  0.00000000e+00,  2.44409016e-09,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [-0.00000000e+00,  6.82975954e-09,  0.00000000e+00,\n",
      "         0.00000000e+00,  9.83861313e-10],\n",
      "       [-6.22730015e-09,  0.00000000e+00,  2.44409016e-09,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [-0.00000000e+00,  0.00000000e+00,  2.14524561e-09,\n",
      "         2.17852038e-09,  9.83861313e-10],\n",
      "       [ 0.00000000e+00, -1.46122041e-08, -0.00000000e+00,\n",
      "        -0.00000000e+00, -2.10496171e-09],\n",
      "       [-5.46587377e-09,  6.82975954e-09,  2.14524561e-09,\n",
      "         0.00000000e+00,  9.83861313e-10],\n",
      "       [-5.46587377e-09,  6.82975954e-09,  0.00000000e+00,\n",
      "         0.00000000e+00,  9.83861313e-10],\n",
      "       [ 1.16941838e-08, -0.00000000e+00, -4.58973210e-09,\n",
      "        -0.00000000e+00, -2.10496171e-09]]), 'dW1': array([[ 1.32450372e-08,  5.55766626e-09, -1.04503552e-08,\n",
      "        -2.81657005e-08, -7.48770180e-09, -1.71428652e-08],\n",
      "       [-1.59726569e-08, -2.42476215e-08, -3.30173568e-09,\n",
      "         1.18631879e-08,  5.05852160e-09,  1.17949703e-08],\n",
      "       [-1.22979413e-09,  6.05439978e-10,  4.63013284e-09,\n",
      "         1.22042014e-08, -2.95647063e-09,  6.90196809e-09],\n",
      "       [ 1.23495799e-09,  1.95195212e-09, -3.54825882e-10,\n",
      "         7.64898491e-10, -2.25124333e-09, -1.93588520e-09],\n",
      "       [-2.67123348e-09, -2.85117481e-10,  4.54407252e-10,\n",
      "         5.99184990e-09,  1.04095175e-09,  1.97834506e-09]]), 'db1': array([[-1.46152051e-09,  7.34634315e-10,  5.73617427e-10,\n",
      "         2.72315047e-10, -3.43097706e-11]])}\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (≈ 8 lines of code)\n",
    "def d_sigmoid(x):\n",
    "    \"\"\" Derivative of sigmoid function\n",
    "    Args:\n",
    "        x: scalar/array\n",
    "    Returns:\n",
    "        dydx: scalar/array, 0 if x < 0, 1 if x >= 0\n",
    "    \"\"\"\n",
    "    dydx = sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "    return dydx\n",
    "\n",
    "def d_relu(x):\n",
    "    \"\"\" Derivative of ReLU function\n",
    "    Args:\n",
    "        x: scalar/array\n",
    "    Returns:\n",
    "        dydx: scalar/array, 0 if x < 0, 1 if x >= 0\n",
    "    \"\"\"\n",
    "    dydx = np.ones_like(x)\n",
    "    dydx[x < 0] = 0\n",
    "\n",
    "    return dydx\n",
    "\n",
    "def grad(predictions, labels, params, cache, activation='relu'):\n",
    "    \"\"\" Backward propogating gradient computation\n",
    "    Args:\n",
    "        predictions (matrix): 2d array with shape (M, C)\n",
    "        labels (matrix): 2d array with shape (M, C)\n",
    "        params: dictionary, stores W's and b's.\n",
    "        cache: dictionary, stores intemediate X's and Z's.\n",
    "    Returns:\n",
    "        grads -- dictionary, stores dW's and db's\n",
    "    \"\"\"\n",
    "    num_layers = int(len(params) / 2)\n",
    "    grads = {'dZ' + str(num_layers): predictions - labels}\n",
    "    for i in reversed(range(num_layers)):\n",
    "        grads['dW'+ str(i+1)] = grads['dZ' + str(i+1)].T @ cache['X' + str(i)]\n",
    "        grads['db' + str(i+1)] = np.mean(grads['dZ' + str(i+1)], axis=0, keepdims=True)\n",
    "        if i==0:\n",
    "            break  \n",
    "        grads['dX' + str(i)] = grads['dZ' + str(i+1)] @ params['W' + str(i+1)]\n",
    "        if activation == 'relu':\n",
    "            grads['dZ' + str(i)] = grads['dX' + str(i)] * d_relu(cache['Z' + str(i)])\n",
    "        elif activation == 'sigmoid':\n",
    "            grads['dZ' + str(i)] = grads['dX' + str(i)] * d_sigmoid(cache['Z' + str(i)])\n",
    "        else:\n",
    "            grads['dZ' + str(i)] = grads['dX' + str(i)]\n",
    "\n",
    "    return grads\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Sanity check\n",
    "dummy_grads = grad(dummy_preds, dummy_labels, dummy_params, dummy_cache)\n",
    "print(dummy_grads.keys())\n",
    "print(dummy_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "dict_keys(['dZ3', 'dW3', 'db3', 'dX2', 'dZ2', 'dW2', 'db2', 'dX1', 'dZ1', 'dW1', 'db1'])\n",
    "{'dZ3': array([[ 0.33339668, -0.66668347,  0.33328679],\n",
    "       [-0.66660332,  0.33331653,  0.33328679],\n",
    "       [ 0.33339668,  0.33331653, -0.66671321],\n",
    "       [ 0.33339668,  0.33331653, -0.66671321],\n",
    "       [ 0.33339668, -0.66668347,  0.33328679],\n",
    "       [ 0.33339668,  0.33331653, -0.66671321],\n",
    "       [-0.66660332,  0.33331653,  0.33328679],\n",
    "       [ 0.33339668,  0.33331653, -0.66671321]]), 'dW3': array([[ 7.79094514e-05,  0.00000000e+00,  0.00000000e+00,\n",
    "         0.00000000e+00],\n",
    "       [ 7.77672619e-05,  0.00000000e+00,  0.00000000e+00,\n",
    "         0.00000000e+00],\n",
    "       [-1.55676713e-04,  0.00000000e+00,  0.00000000e+00,\n",
    "         0.00000000e+00]]), 'db3': array([[ 0.08339668,  0.08331653, -0.16671321]]), 'dX2': array([[ 1.13883818e-04, -3.29905699e-05,  4.66135899e-05,\n",
    "         3.31572328e-05],\n",
    "       [-6.06445677e-05,  1.51249634e-05, -1.23325920e-04,\n",
    "         1.98449678e-05],\n",
    "       [-5.32294163e-05,  1.78635631e-05,  7.67488316e-05,\n",
    "        -5.30117015e-05],\n",
    "       [-5.32294163e-05,  1.78635631e-05,  7.67488316e-05,\n",
    "        -5.30117015e-05],\n",
    "       [ 1.13883818e-04, -3.29905699e-05,  4.66135899e-05,\n",
    "         3.31572328e-05],\n",
    "       [-5.32294163e-05,  1.78635631e-05,  7.67488316e-05,\n",
    "        -5.30117015e-05],\n",
    "...\n",
    "         7.64898491e-10, -2.25124333e-09, -1.93588520e-09],\n",
    "       [-1.56762876e-09, -3.74956163e-09, -1.11294146e-09,\n",
    "         8.11358070e-11, -4.79680573e-10,  6.84456641e-11]]), 'db1': array([[-1.36634221e-09,  9.72490113e-10,  4.98906290e-10,\n",
    "         2.72315047e-10,  3.86057435e-10]])}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimization\n",
    "We have been able to compute the gradient of the cross entropy loss. Now, it's time to perform gradient descent optimization to bring the loss down.\n",
    "\n",
    "![](./gradient_dscent.png)\n",
    "\n",
    "\n",
    "### $\\color{violet}{\\textbf{(30\\%) Exercise 6: Gradient Descent Optimization}}$\n",
    "Train your model. Bring both training loss and test loss down. **Note: you may need to spend more time on this task. So, get started as early as possible.**\n",
    "1. Customize layer sizes.\n",
    "2. Use previously used function to complete the training process.\n",
    "3. Set appropriate iterations and learning rate\n",
    "4. You can use a `for` loop to update weights and biases embedded in the `params` dictionary.\n",
    "\n",
    "> Try to at least bring loss down below 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 training loss: 2.3025849817925037, test loss: 2.302584981106125\n",
      "Iteration 2 training loss: 2.3025845529373368, test loss: 2.302584553011715\n",
      "Iteration 3 training loss: 2.3025841098700464, test loss: 2.3025841108566154\n",
      "Iteration 4 training loss: 2.302583629768213, test loss: 2.3025836321317072\n",
      "Iteration 5 training loss: 2.302583088524615, test loss: 2.302583092835672\n",
      "Iteration 6 training loss: 2.3025824593850603, test loss: 2.3025824655839475\n",
      "Iteration 7 training loss: 2.3025817113685942, test loss: 2.3025817193084235\n",
      "Iteration 8 training loss: 2.3025808084597577, test loss: 2.3025808177886864\n",
      "Iteration 9 training loss: 2.3025797086206867, test loss: 2.30257971903495\n",
      "Iteration 10 training loss: 2.302578360677879, test loss: 2.302578372010739\n",
      "Iteration 11 training loss: 2.302576700084314, test loss: 2.302576712128316\n",
      "Iteration 12 training loss: 2.302574643875764, test loss: 2.302574656584371\n",
      "Iteration 13 training loss: 2.302572086146935, test loss: 2.3025720992321252\n",
      "Iteration 14 training loss: 2.302568892596603, test loss: 2.3025689056641125\n",
      "Iteration 15 training loss: 2.3025648930583364, test loss: 2.3025649054302404\n",
      "Iteration 16 training loss: 2.302559871643954, test loss: 2.302559882513341\n",
      "Iteration 17 training loss: 2.3025535545576195, test loss: 2.302553562614755\n",
      "Iteration 18 training loss: 2.302545594549323, test loss: 2.3025455980627956\n",
      "Iteration 19 training loss: 2.3025355500834817, test loss: 2.3025355469092785\n",
      "Iteration 20 training loss: 2.3025228570730256, test loss: 2.3025228444083945\n",
      "Iteration 21 training loss: 2.302506794929459, test loss: 2.302506769175383\n",
      "Iteration 22 training loss: 2.30248644385619, test loss: 2.30248639995417\n",
      "Iteration 23 training loss: 2.3024606290552123, test loss: 2.302460560634038\n",
      "Iteration 24 training loss: 2.30242785010705, test loss: 2.3024277482473994\n",
      "Iteration 25 training loss: 2.302386189344854, test loss: 2.302386042136121\n",
      "Iteration 26 training loss: 2.302333195410009, test loss: 2.302332987187548\n",
      "Iteration 27 training loss: 2.302265734903755, test loss: 2.302265445122274\n",
      "Iteration 28 training loss: 2.3021798026446456, test loss: 2.30217940394396\n",
      "Iteration 29 training loss: 2.3020702796659256, test loss: 2.302069737557049\n",
      "Iteration 30 training loss: 2.3019306263566217, test loss: 2.3019298952780454\n",
      "Iteration 31 training loss: 2.3017524955907787, test loss: 2.301751516617117\n",
      "Iteration 32 training loss: 2.301525246125287, test loss: 2.30152394321539\n",
      "Iteration 33 training loss: 2.3012353332741973, test loss: 2.301233607619681\n",
      "Iteration 34 training loss: 2.3008655535412803, test loss: 2.3008632782728817\n",
      "Iteration 35 training loss: 2.3003941233518925, test loss: 2.3003911366950325\n",
      "Iteration 36 training loss: 2.299793566097313, test loss: 2.2997896611746875\n",
      "Iteration 37 training loss: 2.2990294080344817, test loss: 2.2990243180263565\n",
      "Iteration 38 training loss: 2.298058688916594, test loss: 2.2980520732122445\n",
      "Iteration 39 training loss: 2.296828352772275, test loss: 2.296819788459704\n",
      "Iteration 40 training loss: 2.295273651771973, test loss: 2.295262609674012\n",
      "Iteration 41 training loss: 2.2933167978069133, test loss: 2.293302642831264\n",
      "Iteration 42 training loss: 2.2908662428073985, test loss: 2.2908482109758275\n",
      "Iteration 43 training loss: 2.287817178963621, test loss: 2.287794324035251\n",
      "Iteration 44 training loss: 2.284054083993572, test loss: 2.284025292341733\n",
      "Iteration 45 training loss: 2.2794561593760756, test loss: 2.2794201327505688\n",
      "Iteration 46 training loss: 2.273906380071796, test loss: 2.273861673164778\n",
      "Iteration 47 training loss: 2.2673047222906466, test loss: 2.2672496862456084\n",
      "Iteration 48 training loss: 2.259584493791973, test loss: 2.259517446557384\n",
      "Iteration 49 training loss: 2.2507287227284953, test loss: 2.2506482061180915\n",
      "Iteration 50 training loss: 2.240782328228183, test loss: 2.24068737818522\n",
      "Iteration 51 training loss: 2.229852786673429, test loss: 2.229743472254193\n",
      "Iteration 52 training loss: 2.218096863438166, test loss: 2.217974529564262\n",
      "Iteration 53 training loss: 2.205691589295381, test loss: 2.2055581346811852\n",
      "Iteration 54 training loss: 2.1927990985220682, test loss: 2.192658213998736\n",
      "Iteration 55 training loss: 2.179531052176733, test loss: 2.1793883177384714\n",
      "Iteration 56 training loss: 2.165913395061219, test loss: 2.1657765320618507\n",
      "Iteration 57 training loss: 2.151871562623537, test loss: 2.151747348880832\n",
      "Iteration 58 training loss: 2.1372324742936395, test loss: 2.137127712280846\n",
      "Iteration 59 training loss: 2.1217447190819683, test loss: 2.121670715498656\n",
      "Iteration 60 training loss: 2.1050949428919283, test loss: 2.1050657604437206\n",
      "Iteration 61 training loss: 2.086944384519313, test loss: 2.08697547777141\n",
      "Iteration 62 training loss: 2.066968643807651, test loss: 2.06707612217207\n",
      "Iteration 63 training loss: 2.044910821980738, test loss: 2.045111682184964\n",
      "Iteration 64 training loss: 2.0206445647068603, test loss: 2.020954426318101\n",
      "Iteration 65 training loss: 1.9942337661191782, test loss: 1.9946650683196465\n",
      "Iteration 66 training loss: 1.9659686010088835, test loss: 1.9665309691821538\n",
      "Iteration 67 training loss: 1.9363545389406018, test loss: 1.9370544517057464\n",
      "Iteration 68 training loss: 1.9060356111286376, test loss: 1.906876272122569\n",
      "Iteration 69 training loss: 1.8756705555059903, test loss: 1.8766511533067705\n",
      "Iteration 70 training loss: 1.8458098501263573, test loss: 1.8469274688041852\n",
      "Iteration 71 training loss: 1.8168260483047542, test loss: 1.8180760524398087\n",
      "Iteration 72 training loss: 1.7889109779507355, test loss: 1.7902876897478661\n",
      "Iteration 73 training loss: 1.7621246729045568, test loss: 1.7636241939932742\n",
      "Iteration 74 training loss: 1.7364543110789337, test loss: 1.7380732865590085\n",
      "Iteration 75 training loss: 1.7118607098633913, test loss: 1.7135995727141846\n",
      "Iteration 76 training loss: 1.6883040173023303, test loss: 1.6901650515039928\n",
      "Iteration 77 training loss: 1.6657524966261406, test loss: 1.6677385188027907\n",
      "Iteration 78 training loss: 1.6441812151271433, test loss: 1.6462965940615777\n",
      "Iteration 79 training loss: 1.6235625708203045, test loss: 1.625811990073296\n",
      "Iteration 80 training loss: 1.6038656375167526, test loss: 1.6062532075636209\n",
      "Iteration 81 training loss: 1.5850589226203957, test loss: 1.5875887881655903\n",
      "Iteration 82 training loss: 1.567109290585563, test loss: 1.569785115305674\n",
      "Iteration 83 training loss: 1.5499813545607997, test loss: 1.5528066076271991\n",
      "Iteration 84 training loss: 1.5336375826728876, test loss: 1.536615321300059\n",
      "Iteration 85 training loss: 1.518036557284153, test loss: 1.5211688008550868\n",
      "Iteration 86 training loss: 1.5031324853572712, test loss: 1.5064203551293298\n",
      "Iteration 87 training loss: 1.4888798692684266, test loss: 1.4923245452561567\n",
      "Iteration 88 training loss: 1.4752331753724055, test loss: 1.4788351853475892\n",
      "Iteration 89 training loss: 1.4621486041517717, test loss: 1.4659090124842995\n",
      "Iteration 90 training loss: 1.4495848391078063, test loss: 1.4535040914007062\n",
      "Iteration 91 training loss: 1.437502535080786, test loss: 1.441580547328602\n",
      "Iteration 92 training loss: 1.425863778946342, test loss: 1.4301002241278449\n",
      "Iteration 93 training loss: 1.4146332981746708, test loss: 1.4190279768172642\n",
      "Iteration 94 training loss: 1.4037780303575529, test loss: 1.4083300296979793\n",
      "Iteration 95 training loss: 1.393267084032514, test loss: 1.3979761740456582\n",
      "Iteration 96 training loss: 1.3830716148498732, test loss: 1.3879368917738166\n",
      "Iteration 97 training loss: 1.3731647283877968, test loss: 1.3781855047365064\n",
      "Iteration 98 training loss: 1.3635213549020821, test loss: 1.3686967500117848\n",
      "Iteration 99 training loss: 1.3541186002407661, test loss: 1.3594478005804513\n",
      "Iteration 100 training loss: 1.3449354184011872, test loss: 1.3504174272289278\n",
      "Iteration 101 training loss: 1.3359523641631763, test loss: 1.3415868973299876\n",
      "Iteration 102 training loss: 1.3271510041308985, test loss: 1.3329365732796052\n",
      "Iteration 103 training loss: 1.3185141158977747, test loss: 1.3244499191310086\n",
      "Iteration 104 training loss: 1.3100249785647364, test loss: 1.316109555454481\n",
      "Iteration 105 training loss: 1.3016683231222808, test loss: 1.3079003202312158\n",
      "Iteration 106 training loss: 1.293429609368254, test loss: 1.2998077341289296\n",
      "Iteration 107 training loss: 1.2852950836818808, test loss: 1.291818303947561\n",
      "Iteration 108 training loss: 1.2772526455574462, test loss: 1.2839197254405998\n",
      "Iteration 109 training loss: 1.269290798707902, test loss: 1.2761009199136804\n",
      "Iteration 110 training loss: 1.2613991734006276, test loss: 1.2683508263158085\n",
      "Iteration 111 training loss: 1.2535683729070572, test loss: 1.2606602137718523\n",
      "Iteration 112 training loss: 1.245790115404157, test loss: 1.253020412001019\n",
      "Iteration 113 training loss: 1.2380571505054385, test loss: 1.2454245643938175\n",
      "Iteration 114 training loss: 1.2303634585430312, test loss: 1.2378663925400475\n",
      "Iteration 115 training loss: 1.2227044075127147, test loss: 1.2303412318026268\n",
      "Iteration 116 training loss: 1.2150764128363318, test loss: 1.2228455696354028\n",
      "Iteration 117 training loss: 1.2074772489387913, test loss: 1.215377670968321\n",
      "Iteration 118 training loss: 1.1999061117351906, test loss: 1.207935774896876\n",
      "Iteration 119 training loss: 1.1923635160586898, test loss: 1.2005210559348753\n",
      "Iteration 120 training loss: 1.1848513300384014, test loss: 1.1931341573268512\n",
      "Iteration 121 training loss: 1.1773726436543908, test loss: 1.18578037485164\n",
      "Iteration 122 training loss: 1.1699313774487134, test loss: 1.1784603173330936\n",
      "Iteration 123 training loss: 1.162532736991592, test loss: 1.1711857272812218\n",
      "Iteration 124 training loss: 1.155182845607425, test loss: 1.1639510314353572\n",
      "Iteration 125 training loss: 1.147888806625496, test loss: 1.1567838478910246\n",
      "Iteration 126 training loss: 1.1406590031517423, test loss: 1.1496548325397797\n",
      "Iteration 127 training loss: 1.1335036823746831, test loss: 1.142645306870991\n",
      "Iteration 128 training loss: 1.1264403456730445, test loss: 1.1356393714954856\n",
      "Iteration 129 training loss: 1.1195117149031069, test loss: 1.1289338668335958\n",
      "Iteration 130 training loss: 1.112856045000955, test loss: 1.1221744485354723\n",
      "Iteration 131 training loss: 1.107030439270176, test loss: 1.1169049846822559\n",
      "Iteration 132 training loss: 1.1042637817592362, test loss: 1.1133954321065862\n",
      "Iteration 133 training loss: 1.1138243926238995, test loss: 1.1250068762405456\n",
      "Iteration 134 training loss: 1.1566588135578546, test loss: 1.1647730174457211\n",
      "Iteration 135 training loss: 1.248516943849176, test loss: 1.2632626431090175\n",
      "Iteration 136 training loss: 1.2069494003036279, test loss: 1.213086585849435\n",
      "Iteration 137 training loss: 1.1555132867939617, test loss: 1.1681445658561458\n",
      "Iteration 138 training loss: 1.1710948188588783, test loss: 1.1792154044193413\n",
      "Iteration 139 training loss: 1.2096692762041907, test loss: 1.2239624658261945\n",
      "Iteration 140 training loss: 1.1622053260156489, test loss: 1.169656903081667\n",
      "Iteration 141 training loss: 1.136933183610098, test loss: 1.1500574688925769\n",
      "Iteration 142 training loss: 1.1607347046963639, test loss: 1.1691651793651088\n",
      "Iteration 143 training loss: 1.1522390891187606, test loss: 1.1658756454789185\n",
      "Iteration 144 training loss: 1.1342178235742275, test loss: 1.1425871296758368\n",
      "Iteration 145 training loss: 1.1207236118741644, test loss: 1.134261598025457\n",
      "Iteration 146 training loss: 1.124733266988749, test loss: 1.1334427057596728\n",
      "Iteration 147 training loss: 1.1145369835354222, test loss: 1.1282716876593466\n",
      "Iteration 148 training loss: 1.1057421630142448, test loss: 1.1146106008023648\n",
      "Iteration 149 training loss: 1.0994090656833722, test loss: 1.1132936122687378\n",
      "Iteration 150 training loss: 1.093934619601352, test loss: 1.1030067244733948\n",
      "Iteration 151 training loss: 1.0896596221103252, test loss: 1.1037186576986542\n",
      "Iteration 152 training loss: 1.0810554001225487, test loss: 1.0903150863309758\n",
      "Iteration 153 training loss: 1.0792044177101416, test loss: 1.0934383484821886\n",
      "Iteration 154 training loss: 1.0703067439130485, test loss: 1.0797584197314598\n",
      "Iteration 155 training loss: 1.0700910232407967, test loss: 1.0845013470026803\n",
      "Iteration 156 training loss: 1.060147171217456, test loss: 1.0697875934182122\n",
      "Iteration 157 training loss: 1.0612947965144301, test loss: 1.0758769470282907\n",
      "Iteration 158 training loss: 1.0508391504105312, test loss: 1.0606673810419642\n",
      "Iteration 159 training loss: 1.0530640416119061, test loss: 1.0678142455451034\n",
      "Iteration 160 training loss: 1.0420906396074214, test loss: 1.0521037460096818\n",
      "Iteration 161 training loss: 1.0451948616340012, test loss: 1.0601101634613512\n",
      "Iteration 162 training loss: 1.0338469003962325, test loss: 1.04404318479749\n",
      "Iteration 163 training loss: 1.037671164960388, test loss: 1.052748376415821\n",
      "Iteration 164 training loss: 1.0260074343072143, test loss: 1.0363835425632084\n",
      "Iteration 165 training loss: 1.0304498851877866, test loss: 1.045684840632204\n",
      "Iteration 166 training loss: 1.0185073496598798, test loss: 1.0290616131991523\n",
      "Iteration 167 training loss: 1.0235005835977558, test loss: 1.038890890560827\n",
      "Iteration 168 training loss: 1.011309702768795, test loss: 1.02203702482368\n",
      "Iteration 169 training loss: 1.0167941423840166, test loss: 1.0323367632461509\n",
      "Iteration 170 training loss: 1.0043806229776429, test loss: 1.0152778467247212\n",
      "Iteration 171 training loss: 1.0103215375326613, test loss: 1.0260139227697973\n",
      "Iteration 172 training loss: 0.9977004164875127, test loss: 1.0087637708916557\n",
      "Iteration 173 training loss: 1.0040608491571983, test loss: 1.019901037696663\n",
      "Iteration 174 training loss: 0.9912358846832745, test loss: 1.00246116707364\n",
      "Iteration 175 training loss: 0.9979876882047131, test loss: 1.0139744422775494\n",
      "Iteration 176 training loss: 0.9849690139609315, test loss: 0.9963536584946147\n",
      "Iteration 177 training loss: 0.9920944206951907, test loss: 1.0082250287481584\n",
      "Iteration 178 training loss: 0.9788865775806521, test loss: 0.9904265505588724\n",
      "Iteration 179 training loss: 0.9863657579726207, test loss: 1.0026377959193555\n",
      "Iteration 180 training loss: 0.9729772313204536, test loss: 0.9846689439943274\n",
      "Iteration 181 training loss: 0.9807891715296816, test loss: 0.9972013104871141\n",
      "Iteration 182 training loss: 0.9672304575377987, test loss: 0.9790716137312037\n",
      "Iteration 183 training loss: 0.9753645741361657, test loss: 0.9919131293490343\n",
      "Iteration 184 training loss: 0.9616281996685426, test loss: 0.9736166036611061\n",
      "Iteration 185 training loss: 0.9700639207173716, test loss: 0.9867460514894565\n",
      "Iteration 186 training loss: 0.9561603725736777, test loss: 0.9682935535973808\n",
      "Iteration 187 training loss: 0.9648869605709658, test loss: 0.9817000397053883\n",
      "Iteration 188 training loss: 0.9508183988744398, test loss: 0.9630926682889365\n",
      "Iteration 189 training loss: 0.9598214788689698, test loss: 0.9767634571393142\n",
      "Iteration 190 training loss: 0.9455987416981578, test loss: 0.9580112080661461\n",
      "Iteration 191 training loss: 0.9548606511773123, test loss: 0.9719292959501087\n",
      "Iteration 192 training loss: 0.9404890382232123, test loss: 0.9530366794048034\n",
      "Iteration 193 training loss: 0.9499984147490913, test loss: 0.9671903113759006\n",
      "Iteration 194 training loss: 0.9354874246245799, test loss: 0.9481667084398463\n",
      "Iteration 195 training loss: 0.9452300428456212, test loss: 0.96254289420476\n",
      "Iteration 196 training loss: 0.9305781937866462, test loss: 0.9433856955052384\n",
      "Iteration 197 training loss: 0.9405370280787673, test loss: 0.9579672050069228\n",
      "Iteration 198 training loss: 0.9257707758568422, test loss: 0.9387039798859063\n",
      "Iteration 199 training loss: 0.9359388823670953, test loss: 0.9534843278553159\n",
      "Iteration 200 training loss: 0.9210600110037812, test loss: 0.9341155701940028\n",
      "Iteration 201 training loss: 0.9314149745714356, test loss: 0.9490730521503224\n",
      "Iteration 202 training loss: 0.9164301036672448, test loss: 0.9296051829305927\n",
      "Iteration 203 training loss: 0.9269550792450043, test loss: 0.9447227073352497\n",
      "Iteration 204 training loss: 0.9118930592824444, test loss: 0.9251854954626342\n",
      "Iteration 205 training loss: 0.922582606933605, test loss: 0.9404579494012621\n",
      "Iteration 206 training loss: 0.9074489044052633, test loss: 0.920856282545762\n",
      "Iteration 207 training loss: 0.9182882823257827, test loss: 0.9362690741230445\n",
      "Iteration 208 training loss: 0.9030849593527921, test loss: 0.9166035067593413\n",
      "Iteration 209 training loss: 0.9140427344442579, test loss: 0.9321266175299959\n",
      "Iteration 210 training loss: 0.8988057058051525, test loss: 0.9124338328134723\n",
      "Iteration 211 training loss: 0.9098736764882189, test loss: 0.9280581685883171\n",
      "Iteration 212 training loss: 0.8946061410236912, test loss: 0.9083403481622019\n",
      "Iteration 213 training loss: 0.9057628882110785, test loss: 0.9240468445816731\n",
      "Iteration 214 training loss: 0.8904885142437947, test loss: 0.9043272589982063\n",
      "Iteration 215 training loss: 0.9017193129382512, test loss: 0.9201017755664445\n",
      "Iteration 216 training loss: 0.8864550771730226, test loss: 0.900395810543231\n",
      "Iteration 217 training loss: 0.8977383764133589, test loss: 0.9162175360241803\n",
      "Iteration 218 training loss: 0.8824966854140898, test loss: 0.8965366650114073\n",
      "Iteration 219 training loss: 0.8938203147804721, test loss: 0.9123942439951439\n",
      "Iteration 220 training loss: 0.8786193819518799, test loss: 0.8927554541455945\n",
      "Iteration 221 training loss: 0.8899631543234541, test loss: 0.908630355842608\n",
      "Iteration 222 training loss: 0.8748122814129898, test loss: 0.889042655562929\n",
      "Iteration 223 training loss: 0.8861559686374392, test loss: 0.9049150109575694\n",
      "Iteration 224 training loss: 0.8710844900609107, test loss: 0.8854067042576771\n",
      "Iteration 225 training loss: 0.8824170960059895, test loss: 0.9012661363500014\n",
      "Iteration 226 training loss: 0.8674278977868743, test loss: 0.8818404021100436\n",
      "Iteration 227 training loss: 0.8787353329884185, test loss: 0.8976724511214448\n",
      "Iteration 228 training loss: 0.8638491106261863, test loss: 0.8783505913274632\n",
      "Iteration 229 training loss: 0.8751173543339692, test loss: 0.8941402037973654\n",
      "Iteration 230 training loss: 0.8603492173625857, test loss: 0.8749393390840418\n",
      "Iteration 231 training loss: 0.8715708400642636, test loss: 0.8906781762905756\n",
      "Iteration 232 training loss: 0.8569105827241121, test loss: 0.8715872369242031\n",
      "Iteration 233 training loss: 0.8680747008373405, test loss: 0.8872633481843928\n",
      "Iteration 234 training loss: 0.8535390030984087, test loss: 0.8683010463039398\n",
      "Iteration 235 training loss: 0.8646316188530506, test loss: 0.8839000052219675\n",
      "Iteration 236 training loss: 0.8502311087368061, test loss: 0.8650771770390356\n",
      "Iteration 237 training loss: 0.8612494003965309, test loss: 0.8805947546257812\n",
      "Iteration 238 training loss: 0.8469952936812979, test loss: 0.8619238535941695\n",
      "Iteration 239 training loss: 0.8579200068873876, test loss: 0.8773402837053507\n",
      "Iteration 240 training loss: 0.8438166827412468, test loss: 0.8588272532885015\n",
      "Iteration 241 training loss: 0.8546419855594621, test loss: 0.8741353352553309\n",
      "Iteration 242 training loss: 0.8406989897047953, test loss: 0.8557886230683284\n",
      "Iteration 243 training loss: 0.851413811087631, test loss: 0.8709791930958695\n",
      "Iteration 244 training loss: 0.8376369866602777, test loss: 0.8528050123557045\n",
      "Iteration 245 training loss: 0.8482434028830901, test loss: 0.8678801341040077\n",
      "Iteration 246 training loss: 0.8346293508672864, test loss: 0.8498745669964973\n",
      "Iteration 247 training loss: 0.8451143299157172, test loss: 0.864821294429167\n",
      "Iteration 248 training loss: 0.8316739991303193, test loss: 0.8469951248945427\n",
      "Iteration 249 training loss: 0.8420352705788983, test loss: 0.8618110768316019\n",
      "Iteration 250 training loss: 0.828774337692209, test loss: 0.8441700600034477\n",
      "Iteration 251 training loss: 0.8390044812509527, test loss: 0.8588486643337796\n",
      "Iteration 252 training loss: 0.8259276348286639, test loss: 0.8413973790290787\n",
      "Iteration 253 training loss: 0.8360305465840874, test loss: 0.8559414892581563\n",
      "Iteration 254 training loss: 0.8231318654418474, test loss: 0.838673947691904\n",
      "Iteration 255 training loss: 0.8331085245532346, test loss: 0.8530849916246492\n",
      "Iteration 256 training loss: 0.8203812762900411, test loss: 0.8359947178405446\n",
      "Iteration 257 training loss: 0.8302324077137143, test loss: 0.8502739228120241\n",
      "Iteration 258 training loss: 0.8176758373557785, test loss: 0.8333593769307674\n",
      "Iteration 259 training loss: 0.8273991753009228, test loss: 0.8475029363414693\n",
      "Iteration 260 training loss: 0.8150176536258811, test loss: 0.8307715405559631\n",
      "Iteration 261 training loss: 0.8246229333588984, test loss: 0.8447891319372631\n",
      "Iteration 262 training loss: 0.812398754970284, test loss: 0.8282214804374664\n",
      "Iteration 263 training loss: 0.8218723484364989, test loss: 0.8420987722781355\n",
      "Iteration 264 training loss: 0.8098035127408374, test loss: 0.8256945451056534\n",
      "Iteration 265 training loss: 0.8191469096382553, test loss: 0.8394320696208013\n",
      "Iteration 266 training loss: 0.8072418790175294, test loss: 0.823201447792326\n",
      "Iteration 267 training loss: 0.8164508004370139, test loss: 0.8367933686767236\n",
      "Iteration 268 training loss: 0.8047277487911155, test loss: 0.8207550386975552\n",
      "Iteration 269 training loss: 0.8138159382648187, test loss: 0.834215686585088\n",
      "Iteration 270 training loss: 0.8022604607214809, test loss: 0.8183554748664269\n",
      "Iteration 271 training loss: 0.8112317812292115, test loss: 0.831687840231496\n",
      "Iteration 272 training loss: 0.7998228654764591, test loss: 0.8159853263005175\n",
      "Iteration 273 training loss: 0.8086783477694109, test loss: 0.8291901306331071\n",
      "Iteration 274 training loss: 0.7974176327135614, test loss: 0.8136464221226276\n",
      "Iteration 275 training loss: 0.8061539984922678, test loss: 0.8267207050425402\n",
      "Iteration 276 training loss: 0.7950390116799968, test loss: 0.8113334267852513\n",
      "Iteration 277 training loss: 0.8036656035913724, test loss: 0.8242860308350606\n",
      "Iteration 278 training loss: 0.7926987397869603, test loss: 0.8090583290945907\n",
      "Iteration 279 training loss: 0.8012218637944317, test loss: 0.8218959110916505\n",
      "Iteration 280 training loss: 0.7903895978305278, test loss: 0.8068124981690967\n",
      "Iteration 281 training loss: 0.7988065495505436, test loss: 0.8195331061233674\n",
      "Iteration 282 training loss: 0.7881115471312664, test loss: 0.8045978482230486\n",
      "Iteration 283 training loss: 0.7964243873165221, test loss: 0.8172018912093114\n",
      "Iteration 284 training loss: 0.7858615816374127, test loss: 0.8024105548154269\n",
      "Iteration 285 training loss: 0.7940754805155898, test loss: 0.8149025746964098\n",
      "Iteration 286 training loss: 0.7836336539014322, test loss: 0.8002451391758619\n",
      "Iteration 287 training loss: 0.7917422935347636, test loss: 0.812617270672579\n",
      "Iteration 288 training loss: 0.7814322409501915, test loss: 0.7981053904044916\n",
      "Iteration 289 training loss: 0.7894481376670138, test loss: 0.8103702039512357\n",
      "Iteration 290 training loss: 0.7792547345178233, test loss: 0.7959888449936584\n",
      "Iteration 291 training loss: 0.787175605415131, test loss: 0.8081430449721123\n",
      "Iteration 292 training loss: 0.7770935001116085, test loss: 0.7938878910703365\n",
      "Iteration 293 training loss: 0.7849248189027964, test loss: 0.8059366997161099\n",
      "Iteration 294 training loss: 0.7749649195357223, test loss: 0.7918192741163521\n",
      "Iteration 295 training loss: 0.7827099920188078, test loss: 0.8037659153386452\n",
      "Iteration 296 training loss: 0.7728590515841085, test loss: 0.7897726233041096\n",
      "Iteration 297 training loss: 0.7805170831112088, test loss: 0.8016161325453577\n",
      "Iteration 298 training loss: 0.7707741635268873, test loss: 0.7877469438443524\n",
      "Iteration 299 training loss: 0.7783517871568076, test loss: 0.7994935629756894\n",
      "Iteration 300 training loss: 0.768705069684804, test loss: 0.7857371344427354\n",
      "Iteration 301 training loss: 0.7762049234525535, test loss: 0.7973878471108745\n",
      "Iteration 302 training loss: 0.7666589319992402, test loss: 0.7837491425062277\n",
      "Iteration 303 training loss: 0.774077648489809, test loss: 0.7953010935487803\n",
      "Iteration 304 training loss: 0.7646245958880886, test loss: 0.7817725732394074\n",
      "Iteration 305 training loss: 0.7719670010083979, test loss: 0.7932308815991903\n",
      "Iteration 306 training loss: 0.762610604925228, test loss: 0.7798162714775897\n",
      "Iteration 307 training loss: 0.7698800417942703, test loss: 0.7911834904821637\n",
      "Iteration 308 training loss: 0.7606179278953982, test loss: 0.7778815207054265\n",
      "Iteration 309 training loss: 0.7678175508534522, test loss: 0.7891606882206691\n",
      "Iteration 310 training loss: 0.7586502012142367, test loss: 0.7759712295140683\n",
      "Iteration 311 training loss: 0.7657783912315628, test loss: 0.7871599088405219\n",
      "Iteration 312 training loss: 0.7566984495333641, test loss: 0.7740764056386653\n",
      "Iteration 313 training loss: 0.7637597594815183, test loss: 0.7851797155873617\n",
      "Iteration 314 training loss: 0.754769042267542, test loss: 0.7722036149156627\n",
      "Iteration 315 training loss: 0.7617653507860749, test loss: 0.7832227024690841\n",
      "Iteration 316 training loss: 0.7528634722419274, test loss: 0.7703541316370501\n",
      "Iteration 317 training loss: 0.7597989456534961, test loss: 0.78129301884985\n",
      "Iteration 318 training loss: 0.7509716217792967, test loss: 0.7685181934097952\n",
      "Iteration 319 training loss: 0.7578495528638363, test loss: 0.7793803372417182\n",
      "Iteration 320 training loss: 0.7490936283872293, test loss: 0.7666958267872205\n",
      "Iteration 321 training loss: 0.7559092714246185, test loss: 0.7774755328146411\n",
      "Iteration 322 training loss: 0.7472288927514159, test loss: 0.764886914903988\n",
      "Iteration 323 training loss: 0.7539866314418473, test loss: 0.7755882540307466\n",
      "Iteration 324 training loss: 0.7453835878052083, test loss: 0.7630974434203024\n",
      "Iteration 325 training loss: 0.7520875989131005, test loss: 0.773723951130276\n",
      "Iteration 326 training loss: 0.7435569623726948, test loss: 0.7613258598461382\n",
      "Iteration 327 training loss: 0.7502109297268065, test loss: 0.7718822946064163\n",
      "Iteration 328 training loss: 0.7417481880927885, test loss: 0.7595735976453779\n",
      "Iteration 329 training loss: 0.7483509871383068, test loss: 0.7700563555419185\n",
      "Iteration 330 training loss: 0.7399531908477299, test loss: 0.757833871158177\n",
      "Iteration 331 training loss: 0.7465023254510712, test loss: 0.7682407286678803\n",
      "Iteration 332 training loss: 0.7381733413877302, test loss: 0.7561090539446341\n",
      "Iteration 333 training loss: 0.7446666941772841, test loss: 0.76643745148621\n",
      "Iteration 334 training loss: 0.7364025692521814, test loss: 0.7543923091225118\n",
      "Iteration 335 training loss: 0.7428503779174387, test loss: 0.7646535525987254\n",
      "Iteration 336 training loss: 0.7346504273925063, test loss: 0.7526933789575846\n",
      "Iteration 337 training loss: 0.7410502977910797, test loss: 0.762886201592241\n",
      "Iteration 338 training loss: 0.732914390586753, test loss: 0.751010638454983\n",
      "Iteration 339 training loss: 0.7392572248399482, test loss: 0.7611250861055048\n",
      "Iteration 340 training loss: 0.731190283308617, test loss: 0.7493396055292989\n",
      "Iteration 341 training loss: 0.7374862739649687, test loss: 0.7593859840010472\n",
      "Iteration 342 training loss: 0.7294807046274608, test loss: 0.7476828630012362\n",
      "Iteration 343 training loss: 0.7357255291528109, test loss: 0.7576571450906207\n",
      "Iteration 344 training loss: 0.7277881770344959, test loss: 0.7460441863600993\n",
      "Iteration 345 training loss: 0.7339817487259422, test loss: 0.7559451485269139\n",
      "Iteration 346 training loss: 0.7261049300078425, test loss: 0.7444144582741208\n",
      "Iteration 347 training loss: 0.732245972026839, test loss: 0.7542406923391163\n",
      "Iteration 348 training loss: 0.7244371057080294, test loss: 0.7428000510395404\n",
      "Iteration 349 training loss: 0.7305283280448027, test loss: 0.7525540490118192\n",
      "Iteration 350 training loss: 0.7227852110778875, test loss: 0.7412010090546542\n",
      "Iteration 351 training loss: 0.7288319236072441, test loss: 0.7508886775927768\n",
      "Iteration 352 training loss: 0.7211533437129554, test loss: 0.7396221577283695\n",
      "Iteration 353 training loss: 0.7271618789857518, test loss: 0.7492500767681423\n",
      "Iteration 354 training loss: 0.7195418805023017, test loss: 0.7380630341491318\n",
      "Iteration 355 training loss: 0.7255093177767267, test loss: 0.7476290450170366\n",
      "Iteration 356 training loss: 0.7179432950492418, test loss: 0.7365173190816535\n",
      "Iteration 357 training loss: 0.7238714682457751, test loss: 0.7460224675679962\n",
      "Iteration 358 training loss: 0.7163572040264821, test loss: 0.7349834735192439\n",
      "Iteration 359 training loss: 0.722241228651155, test loss: 0.74442329989216\n",
      "Iteration 360 training loss: 0.7147857135506096, test loss: 0.7334648332899129\n",
      "Iteration 361 training loss: 0.7206330909034387, test loss: 0.7428472822522639\n",
      "Iteration 362 training loss: 0.7132300058431845, test loss: 0.7319617964964809\n",
      "Iteration 363 training loss: 0.7190332695542263, test loss: 0.7412794486355683\n",
      "Iteration 364 training loss: 0.7116799885164926, test loss: 0.7304643647510786\n",
      "Iteration 365 training loss: 0.7174414149059251, test loss: 0.7397197952087903\n",
      "Iteration 366 training loss: 0.7101406977158646, test loss: 0.7289777097349487\n",
      "Iteration 367 training loss: 0.7158625199161758, test loss: 0.7381728183525499\n",
      "Iteration 368 training loss: 0.708618831477066, test loss: 0.7275085040154285\n",
      "Iteration 369 training loss: 0.7142998618367219, test loss: 0.7366420765283616\n",
      "Iteration 370 training loss: 0.7071139890007446, test loss: 0.726056874942561\n",
      "Iteration 371 training loss: 0.7127549672560385, test loss: 0.7351292253117284\n",
      "Iteration 372 training loss: 0.7056252875594207, test loss: 0.7246211985698817\n",
      "Iteration 373 training loss: 0.7112249221633306, test loss: 0.7336313424035967\n",
      "Iteration 374 training loss: 0.7041421961934446, test loss: 0.7231904146116815\n",
      "Iteration 375 training loss: 0.7096949546844324, test loss: 0.7321332925730868\n",
      "Iteration 376 training loss: 0.7026723483342835, test loss: 0.7217736709908471\n",
      "Iteration 377 training loss: 0.7081817320592578, test loss: 0.7306520655362078\n",
      "Iteration 378 training loss: 0.7012128416150861, test loss: 0.7203672111574851\n",
      "Iteration 379 training loss: 0.7066810588130553, test loss: 0.7291830263751538\n",
      "Iteration 380 training loss: 0.6997677022574492, test loss: 0.7189754080287355\n",
      "Iteration 381 training loss: 0.7051947616088925, test loss: 0.7277286304346858\n",
      "Iteration 382 training loss: 0.6983347166644766, test loss: 0.7175960688223929\n",
      "Iteration 383 training loss: 0.7037198526139617, test loss: 0.726285962144472\n",
      "Iteration 384 training loss: 0.6969095231099608, test loss: 0.7162245241545051\n",
      "Iteration 385 training loss: 0.7022612210121121, test loss: 0.7248596077764516\n",
      "Iteration 386 training loss: 0.6955051710279956, test loss: 0.7148743575902181\n",
      "Iteration 387 training loss: 0.7008186757025587, test loss: 0.7234501136671733\n",
      "Iteration 388 training loss: 0.6941138722803994, test loss: 0.7135380436714984\n",
      "Iteration 389 training loss: 0.6993868488020053, test loss: 0.722051550411751\n",
      "Iteration 390 training loss: 0.6927278452582362, test loss: 0.7122068259298673\n",
      "Iteration 391 training loss: 0.6979570581524239, test loss: 0.7206550439573205\n",
      "Iteration 392 training loss: 0.691353572831036, test loss: 0.7108879610574134\n",
      "Iteration 393 training loss: 0.6965432746162792, test loss: 0.7192740500696078\n",
      "Iteration 394 training loss: 0.6899892898772503, test loss: 0.709578409775313\n",
      "Iteration 395 training loss: 0.6951376820118824, test loss: 0.7179007172856456\n",
      "Iteration 396 training loss: 0.6886366330694048, test loss: 0.708281217892737\n",
      "Iteration 397 training loss: 0.6937485677390512, test loss: 0.7165437782280195\n",
      "Iteration 398 training loss: 0.6872973771058426, test loss: 0.7069975881600664\n",
      "Iteration 399 training loss: 0.6923721893440746, test loss: 0.7152004487277258\n",
      "Iteration 400 training loss: 0.685965006392007, test loss: 0.7057199301873119\n",
      "Iteration 401 training loss: 0.690992947319476, test loss: 0.7138539829401279\n",
      "Iteration 402 training loss: 0.6846459168577121, test loss: 0.7044564589662359\n",
      "Iteration 403 training loss: 0.6896328813814796, test loss: 0.7125270162884327\n",
      "Iteration 404 training loss: 0.6833383007514945, test loss: 0.7032044871017958\n",
      "Iteration 405 training loss: 0.6882854385596021, test loss: 0.7112128248022702\n",
      "Iteration 406 training loss: 0.6820404160735621, test loss: 0.7019625548579357\n",
      "Iteration 407 training loss: 0.6869490783680623, test loss: 0.7099095440645953\n",
      "Iteration 408 training loss: 0.6807469917258429, test loss: 0.700724826722802\n",
      "Iteration 409 training loss: 0.685611634102372, test loss: 0.708605738416051\n",
      "Iteration 410 training loss: 0.6794637390486884, test loss: 0.6994978465785147\n",
      "Iteration 411 training loss: 0.6842851302042254, test loss: 0.7073132061439861\n",
      "Iteration 412 training loss: 0.6781937618787234, test loss: 0.6982846703390669\n",
      "Iteration 413 training loss: 0.6829757377226523, test loss: 0.706038357581072\n",
      "Iteration 414 training loss: 0.6769403317888801, test loss: 0.6970879250176516\n",
      "Iteration 415 training loss: 0.681684142488617, test loss: 0.7047815196541831\n",
      "Iteration 416 training loss: 0.6756964476733625, test loss: 0.6959004432793062\n",
      "Iteration 417 training loss: 0.680400904541863, test loss: 0.7035330627506351\n",
      "Iteration 418 training loss: 0.6744579836246692, test loss: 0.6947185609285341\n",
      "Iteration 419 training loss: 0.6791203651591614, test loss: 0.7022873039113745\n",
      "Iteration 420 training loss: 0.6732307294490298, test loss: 0.6935474805408521\n",
      "Iteration 421 training loss: 0.6778461224228992, test loss: 0.7010481961237927\n",
      "Iteration 422 training loss: 0.6720056221287684, test loss: 0.6923791004723143\n",
      "Iteration 423 training loss: 0.6765787044887286, test loss: 0.699816119173246\n",
      "Iteration 424 training loss: 0.670793549710394, test loss: 0.6912245769840593\n",
      "Iteration 425 training loss: 0.6753265769124082, test loss: 0.6985995131736642\n",
      "Iteration 426 training loss: 0.6695915935728214, test loss: 0.6900802761592189\n",
      "Iteration 427 training loss: 0.674088508877878, test loss: 0.6973977309189957\n",
      "Iteration 428 training loss: 0.6683973612789872, test loss: 0.6889432583464106\n",
      "Iteration 429 training loss: 0.6728524569470978, test loss: 0.6961974982300094\n",
      "Iteration 430 training loss: 0.6672115595155406, test loss: 0.6878149516912881\n",
      "Iteration 431 training loss: 0.6716312160672951, test loss: 0.6950120845147473\n",
      "Iteration 432 training loss: 0.6660309563139079, test loss: 0.6866915552987443\n",
      "Iteration 433 training loss: 0.6704096205282615, test loss: 0.693827089497307\n",
      "Iteration 434 training loss: 0.6648614064773044, test loss: 0.6855793479174397\n",
      "Iteration 435 training loss: 0.6692047748910983, test loss: 0.6926583433531275\n",
      "Iteration 436 training loss: 0.6636999982816948, test loss: 0.6844748032588179\n",
      "Iteration 437 training loss: 0.6680018920852325, test loss: 0.6914923060848017\n",
      "Iteration 438 training loss: 0.6625438111403115, test loss: 0.6833752137660917\n",
      "Iteration 439 training loss: 0.6668109045865735, test loss: 0.6903382105047822\n",
      "Iteration 440 training loss: 0.6613978878628058, test loss: 0.6822854338194195\n",
      "Iteration 441 training loss: 0.6656286525870639, test loss: 0.6891934084291436\n",
      "Iteration 442 training loss: 0.6602582295581482, test loss: 0.6812016540859592\n",
      "Iteration 443 training loss: 0.6644531642597948, test loss: 0.6880557901401458\n",
      "Iteration 444 training loss: 0.6591302487664668, test loss: 0.6801300701088925\n",
      "Iteration 445 training loss: 0.663286735861074, test loss: 0.686926988362859\n",
      "Iteration 446 training loss: 0.6580062782510816, test loss: 0.6790616141045346\n",
      "Iteration 447 training loss: 0.6621166017225707, test loss: 0.6857942841280087\n",
      "Iteration 448 training loss: 0.656881735559704, test loss: 0.6779923576654681\n",
      "Iteration 449 training loss: 0.6609479839960196, test loss: 0.684662919621984\n",
      "Iteration 450 training loss: 0.6557650764650143, test loss: 0.6769309239184546\n",
      "Iteration 451 training loss: 0.6597892342514164, test loss: 0.6835412074419895\n",
      "Iteration 452 training loss: 0.654656480572278, test loss: 0.6758769630616225\n",
      "Iteration 453 training loss: 0.658643311935501, test loss: 0.6824323333967832\n",
      "Iteration 454 training loss: 0.6535556014393155, test loss: 0.6748306331992983\n",
      "Iteration 455 training loss: 0.6575045252091539, test loss: 0.6813300893890173\n",
      "Iteration 456 training loss: 0.6524676573179797, test loss: 0.6737976604506724\n",
      "Iteration 457 training loss: 0.6563786494137454, test loss: 0.6802410165793226\n",
      "Iteration 458 training loss: 0.6513823629944495, test loss: 0.6727671021460747\n",
      "Iteration 459 training loss: 0.6552586936993662, test loss: 0.6791581928536232\n",
      "Iteration 460 training loss: 0.6503047639736563, test loss: 0.671744397525366\n",
      "Iteration 461 training loss: 0.6541392644538324, test loss: 0.6780756833206135\n",
      "Iteration 462 training loss: 0.6492280762935726, test loss: 0.6707227511262684\n",
      "Iteration 463 training loss: 0.6530241080461823, test loss: 0.676998277358052\n",
      "Iteration 464 training loss: 0.6481560212053432, test loss: 0.6697054863074956\n",
      "Iteration 465 training loss: 0.6519079763695464, test loss: 0.6759198670275909\n",
      "Iteration 466 training loss: 0.6470887551662581, test loss: 0.6686929321485935\n",
      "Iteration 467 training loss: 0.6508020617085791, test loss: 0.6748517427336004\n",
      "Iteration 468 training loss: 0.6460294811936641, test loss: 0.6676883285693016\n",
      "Iteration 469 training loss: 0.6497077616507697, test loss: 0.6737948181221634\n",
      "Iteration 470 training loss: 0.644975263627484, test loss: 0.6666881297009215\n",
      "Iteration 471 training loss: 0.6486178989354726, test loss: 0.6727427295175109\n",
      "Iteration 472 training loss: 0.6439304139284824, test loss: 0.6656974320151541\n",
      "Iteration 473 training loss: 0.647532902352851, test loss: 0.6716955763420323\n",
      "Iteration 474 training loss: 0.6428975759048474, test loss: 0.6647190334131111\n",
      "Iteration 475 training loss: 0.646461331305168, test loss: 0.6706621246543255\n",
      "Iteration 476 training loss: 0.6418679307914996, test loss: 0.6637436360284722\n",
      "Iteration 477 training loss: 0.6453956153222955, test loss: 0.6696347462648949\n",
      "Iteration 478 training loss: 0.640837806574473, test loss: 0.6627673691024991\n",
      "Iteration 479 training loss: 0.6443325315851972, test loss: 0.6686103121652824\n",
      "Iteration 480 training loss: 0.6398183573271651, test loss: 0.6618018818354954\n",
      "Iteration 481 training loss: 0.6432756516165271, test loss: 0.6675918412164812\n",
      "Iteration 482 training loss: 0.638798401453729, test loss: 0.6608354440904497\n",
      "Iteration 483 training loss: 0.642219877081679, test loss: 0.6665748524839757\n",
      "Iteration 484 training loss: 0.6377860747619607, test loss: 0.6598767322202548\n",
      "Iteration 485 training loss: 0.6411673366190545, test loss: 0.6655612029314768\n",
      "Iteration 486 training loss: 0.636778444516222, test loss: 0.6589230412555679\n",
      "Iteration 487 training loss: 0.640126014613235, test loss: 0.6645589802270968\n",
      "Iteration 488 training loss: 0.6357729524525114, test loss: 0.6579708136233865\n",
      "Iteration 489 training loss: 0.6390803263030366, test loss: 0.6635523702582299\n",
      "Iteration 490 training loss: 0.6347693561884437, test loss: 0.6570207017868778\n",
      "Iteration 491 training loss: 0.6380341221099732, test loss: 0.6625455127492389\n",
      "Iteration 492 training loss: 0.6337705581502584, test loss: 0.6560753415865602\n",
      "Iteration 493 training loss: 0.6369923760759473, test loss: 0.6615431854697296\n",
      "Iteration 494 training loss: 0.6327694247783414, test loss: 0.6551276128381293\n",
      "Iteration 495 training loss: 0.6359571436959947, test loss: 0.6605481289516564\n",
      "Iteration 496 training loss: 0.6317786235634156, test loss: 0.6541899494284836\n",
      "Iteration 497 training loss: 0.6349304501328923, test loss: 0.6595621511257258\n",
      "Iteration 498 training loss: 0.6307882588744838, test loss: 0.6532529786430681\n",
      "Iteration 499 training loss: 0.633905906949403, test loss: 0.6585785669540349\n",
      "Iteration 500 training loss: 0.6298113605769099, test loss: 0.6523297093365531\n",
      "Iteration 501 training loss: 0.6328961149652822, test loss: 0.6576099893669519\n",
      "Iteration 502 training loss: 0.6288368634588009, test loss: 0.6514083509757115\n",
      "Iteration 503 training loss: 0.6318876051405847, test loss: 0.6566428365502438\n",
      "Iteration 504 training loss: 0.627866520865847, test loss: 0.6504908527689287\n",
      "Iteration 505 training loss: 0.6308894545349315, test loss: 0.6556861061807767\n",
      "Iteration 506 training loss: 0.6269030041547161, test loss: 0.6495811008118942\n",
      "Iteration 507 training loss: 0.6298930965890928, test loss: 0.654731495596956\n",
      "Iteration 508 training loss: 0.6259446186345027, test loss: 0.6486762090753638\n",
      "Iteration 509 training loss: 0.6289043103671363, test loss: 0.6537848035931515\n",
      "Iteration 510 training loss: 0.6249893319770624, test loss: 0.6477739371510602\n",
      "Iteration 511 training loss: 0.6279173918167261, test loss: 0.6528394819310679\n",
      "Iteration 512 training loss: 0.6240409535004037, test loss: 0.6468786483316936\n",
      "Iteration 513 training loss: 0.6269369714859481, test loss: 0.6519012408381281\n",
      "Iteration 514 training loss: 0.6230951123943556, test loss: 0.6459864921145058\n",
      "Iteration 515 training loss: 0.625963817633915, test loss: 0.6509708466694876\n",
      "Iteration 516 training loss: 0.6221575305468149, test loss: 0.6451021233351052\n",
      "Iteration 517 training loss: 0.6249923024781565, test loss: 0.6500415075348145\n",
      "Iteration 518 training loss: 0.6212178710161198, test loss: 0.6442158822051113\n",
      "Iteration 519 training loss: 0.6240243854286615, test loss: 0.6491157905402418\n",
      "Iteration 520 training loss: 0.6202805139220054, test loss: 0.6433319285568081\n",
      "Iteration 521 training loss: 0.6230528922775416, test loss: 0.6481865960519008\n",
      "Iteration 522 training loss: 0.6193434212482792, test loss: 0.6424486680974264\n",
      "Iteration 523 training loss: 0.6220818782283833, test loss: 0.6472574985612058\n",
      "Iteration 524 training loss: 0.6184053008523773, test loss: 0.641565285739792\n",
      "Iteration 525 training loss: 0.6211061971962676, test loss: 0.6463237871714825\n",
      "Iteration 526 training loss: 0.6174620058848669, test loss: 0.6406777021419255\n",
      "Iteration 527 training loss: 0.620128533807205, test loss: 0.6453881471026396\n",
      "Iteration 528 training loss: 0.6165284383521348, test loss: 0.6397997445721034\n",
      "Iteration 529 training loss: 0.6191700982278409, test loss: 0.6444722234445283\n",
      "Iteration 530 training loss: 0.6155984509087747, test loss: 0.6389254494608684\n",
      "Iteration 531 training loss: 0.6182101760947762, test loss: 0.643555463717875\n",
      "Iteration 532 training loss: 0.614670972756681, test loss: 0.6380541091851323\n",
      "Iteration 533 training loss: 0.6172541328956851, test loss: 0.64264327969277\n",
      "Iteration 534 training loss: 0.6137510613958149, test loss: 0.6371899382189207\n",
      "Iteration 535 training loss: 0.6163022543044006, test loss: 0.6417349491870936\n",
      "Iteration 536 training loss: 0.6128295047292824, test loss: 0.636324496023525\n",
      "Iteration 537 training loss: 0.6153481770755055, test loss: 0.6408246445202829\n",
      "Iteration 538 training loss: 0.6119099635652973, test loss: 0.6354610760283794\n",
      "Iteration 539 training loss: 0.6143943750252204, test loss: 0.639913640994204\n",
      "Iteration 540 training loss: 0.6109899461234688, test loss: 0.6345963170453622\n",
      "Iteration 541 training loss: 0.6134373348415623, test loss: 0.6389995312378248\n",
      "Iteration 542 training loss: 0.6100752719473848, test loss: 0.6337366012621422\n",
      "Iteration 543 training loss: 0.6124970232180246, test loss: 0.6381021820445661\n",
      "Iteration 544 training loss: 0.6091703660017758, test loss: 0.6328871870776771\n",
      "Iteration 545 training loss: 0.6115682881075097, test loss: 0.6372173679988253\n",
      "Iteration 546 training loss: 0.6082806210271308, test loss: 0.6320532545140098\n",
      "Iteration 547 training loss: 0.6106563437930039, test loss: 0.6363500978200269\n",
      "Iteration 548 training loss: 0.6074028364356223, test loss: 0.6312310091510946\n",
      "Iteration 549 training loss: 0.6097553679901616, test loss: 0.6354939921029125\n",
      "Iteration 550 training loss: 0.6065302872648972, test loss: 0.6304130772217951\n",
      "Iteration 551 training loss: 0.6088514166635376, test loss: 0.634635811313189\n",
      "Iteration 552 training loss: 0.605651893739122, test loss: 0.6295893143657124\n",
      "Iteration 553 training loss: 0.607943878805974, test loss: 0.6337738942539978\n",
      "Iteration 554 training loss: 0.6047696511170304, test loss: 0.6287616333318495\n",
      "Iteration 555 training loss: 0.6070286084090798, test loss: 0.6329031804278951\n",
      "Iteration 556 training loss: 0.6038895811208469, test loss: 0.6279366456571468\n",
      "Iteration 557 training loss: 0.606124154011653, test loss: 0.6320437568655174\n",
      "Iteration 558 training loss: 0.6030073185927235, test loss: 0.6271089499490176\n",
      "Iteration 559 training loss: 0.6052108747950987, test loss: 0.631175307208232\n",
      "Iteration 560 training loss: 0.6021221227593249, test loss: 0.626278558502697\n",
      "Iteration 561 training loss: 0.6043100484917202, test loss: 0.6303197548364591\n",
      "Iteration 562 training loss: 0.6012520447178017, test loss: 0.6254635915029341\n",
      "Iteration 563 training loss: 0.603411150796593, test loss: 0.6294663908889182\n",
      "Iteration 564 training loss: 0.6003833955515184, test loss: 0.6246494735399909\n",
      "Iteration 565 training loss: 0.60252397299689, test loss: 0.6286256109883456\n",
      "Iteration 566 training loss: 0.5995297813576679, test loss: 0.6238504811444671\n",
      "Iteration 567 training loss: 0.6016510131464765, test loss: 0.627799781919944\n",
      "Iteration 568 training loss: 0.5986836146129716, test loss: 0.6230581446004385\n",
      "Iteration 569 training loss: 0.6007863915757767, test loss: 0.6269823651844021\n",
      "Iteration 570 training loss: 0.5978392793659092, test loss: 0.6222675973936929\n",
      "Iteration 571 training loss: 0.5999213896868065, test loss: 0.6261649507238236\n",
      "Iteration 572 training loss: 0.5970024974167925, test loss: 0.6214841731880782\n",
      "Iteration 573 training loss: 0.5990610251041637, test loss: 0.6253525043932987\n",
      "Iteration 574 training loss: 0.5961610350249975, test loss: 0.6206957614275531\n",
      "Iteration 575 training loss: 0.5981995487335263, test loss: 0.6245395430040195\n",
      "Iteration 576 training loss: 0.5953248664485711, test loss: 0.6199123755560283\n",
      "Iteration 577 training loss: 0.5973372640247362, test loss: 0.623725212742392\n",
      "Iteration 578 training loss: 0.5944872916012784, test loss: 0.6191272292985376\n",
      "Iteration 579 training loss: 0.5964737486356693, test loss: 0.6229093157451816\n",
      "Iteration 580 training loss: 0.5936430189520007, test loss: 0.6183348919179732\n",
      "Iteration 581 training loss: 0.5955963701529498, test loss: 0.6220785769394035\n",
      "Iteration 582 training loss: 0.5927954857458392, test loss: 0.6175391965693658\n",
      "Iteration 583 training loss: 0.5947224149817376, test loss: 0.6212516713507165\n",
      "Iteration 584 training loss: 0.5919525632566963, test loss: 0.6167482182323257\n",
      "Iteration 585 training loss: 0.5938551163851636, test loss: 0.6204316575249236\n",
      "Iteration 586 training loss: 0.591112592799949, test loss: 0.615959159964806\n",
      "Iteration 587 training loss: 0.5929867794618482, test loss: 0.619611076259447\n",
      "Iteration 588 training loss: 0.5902732618007087, test loss: 0.6151701129086679\n",
      "Iteration 589 training loss: 0.5921253458148786, test loss: 0.6187977916111785\n",
      "Iteration 590 training loss: 0.5894493668754038, test loss: 0.6143957745005509\n",
      "Iteration 591 training loss: 0.5912791663564434, test loss: 0.618000530794702\n",
      "Iteration 592 training loss: 0.5886307070679393, test loss: 0.6136263389047791\n",
      "Iteration 593 training loss: 0.5904366992492437, test loss: 0.6172064936334177\n",
      "Iteration 594 training loss: 0.5878198758542745, test loss: 0.6128641801932647\n",
      "Iteration 595 training loss: 0.5895992970109065, test loss: 0.6164179832966838\n",
      "Iteration 596 training loss: 0.5870117250001436, test loss: 0.6121044881463664\n",
      "Iteration 597 training loss: 0.5887677491687255, test loss: 0.6156348710656061\n",
      "Iteration 598 training loss: 0.5862131507979349, test loss: 0.6113539078488796\n",
      "Iteration 599 training loss: 0.5879428015402529, test loss: 0.614858933559794\n",
      "Iteration 600 training loss: 0.5854181693388282, test loss: 0.6106068615076261\n",
      "Iteration 601 training loss: 0.5871272415465713, test loss: 0.6140929220353697\n",
      "Iteration 602 training loss: 0.5846279800652918, test loss: 0.6098640950941618\n",
      "Iteration 603 training loss: 0.5863098483961604, test loss: 0.6133258206039754\n",
      "Iteration 604 training loss: 0.5838419056087537, test loss: 0.6091247970474362\n",
      "Iteration 605 training loss: 0.5854966125898722, test loss: 0.6125624002998916\n",
      "Iteration 606 training loss: 0.5830620210428302, test loss: 0.6083913622160991\n",
      "Iteration 607 training loss: 0.5846898921414005, test loss: 0.611804975120855\n",
      "Iteration 608 training loss: 0.582288197410048, test loss: 0.6076630249725005\n",
      "Iteration 609 training loss: 0.5838744789200383, test loss: 0.6110389789722837\n",
      "Iteration 610 training loss: 0.5815051304347776, test loss: 0.6069238621382775\n",
      "Iteration 611 training loss: 0.5830579139383079, test loss: 0.610271697446689\n",
      "Iteration 612 training loss: 0.5807146239851398, test loss: 0.6061767545298883\n",
      "Iteration 613 training loss: 0.582232020852555, test loss: 0.6094957701078759\n",
      "Iteration 614 training loss: 0.5799282919296761, test loss: 0.6054334003350201\n",
      "Iteration 615 training loss: 0.5814064984211472, test loss: 0.6087198865505566\n",
      "Iteration 616 training loss: 0.5791422705828515, test loss: 0.6046901876508005\n",
      "Iteration 617 training loss: 0.5805827450280597, test loss: 0.6079460416395916\n",
      "Iteration 618 training loss: 0.5783679899064746, test loss: 0.6039584020156828\n",
      "Iteration 619 training loss: 0.5797715960569834, test loss: 0.607185121567338\n",
      "Iteration 620 training loss: 0.5775985522743523, test loss: 0.6032298052092934\n",
      "Iteration 621 training loss: 0.5789583452077823, test loss: 0.6064229079708048\n",
      "Iteration 622 training loss: 0.5768324360340727, test loss: 0.6025037407580438\n",
      "Iteration 623 training loss: 0.5781473549520048, test loss: 0.6056638462810133\n",
      "Iteration 624 training loss: 0.576073673678025, test loss: 0.6017843008378253\n",
      "Iteration 625 training loss: 0.5773503727712948, test loss: 0.6049186626095975\n",
      "Iteration 626 training loss: 0.5753328126899587, test loss: 0.6010813896464874\n",
      "Iteration 627 training loss: 0.5765642528552455, test loss: 0.6041861652810073\n",
      "Iteration 628 training loss: 0.5746077457586042, test loss: 0.600392150250117\n",
      "Iteration 629 training loss: 0.5757796705044932, test loss: 0.603455801033291\n",
      "Iteration 630 training loss: 0.573886770610112, test loss: 0.5997051580657192\n",
      "Iteration 631 training loss: 0.5749961751588859, test loss: 0.6027273264960956\n",
      "Iteration 632 training loss: 0.5731663288680703, test loss: 0.5990173003574182\n",
      "Iteration 633 training loss: 0.5742050136857587, test loss: 0.6019917162901531\n",
      "Iteration 634 training loss: 0.5724540694782608, test loss: 0.5983359156756235\n",
      "Iteration 635 training loss: 0.5734229377717276, test loss: 0.6012651119175754\n",
      "Iteration 636 training loss: 0.5717536832172694, test loss: 0.5976645224584352\n",
      "Iteration 637 training loss: 0.5726325340139098, test loss: 0.6005321353545549\n",
      "Iteration 638 training loss: 0.5710606615945374, test loss: 0.5969985343447645\n",
      "Iteration 639 training loss: 0.571839441392551, test loss: 0.5997978109122788\n",
      "Iteration 640 training loss: 0.5703789258118259, test loss: 0.5963400851941728\n",
      "Iteration 641 training loss: 0.5710380783029174, test loss: 0.599056877303039\n",
      "Iteration 642 training loss: 0.569701888052276, test loss: 0.5956839778923966\n",
      "Iteration 643 training loss: 0.5702211940635159, test loss: 0.598301071355157\n",
      "Iteration 644 training loss: 0.5690287684027965, test loss: 0.5950289065332599\n",
      "Iteration 645 training loss: 0.5693953960068076, test loss: 0.5975375573726138\n",
      "Iteration 646 training loss: 0.5683705321850265, test loss: 0.5943858386302419\n",
      "Iteration 647 training loss: 0.5685627048073796, test loss: 0.596768609152511\n",
      "Iteration 648 training loss: 0.5677317775520239, test loss: 0.5937581005332042\n",
      "Iteration 649 training loss: 0.567724255193645, test loss: 0.5959946852058038\n",
      "Iteration 650 training loss: 0.5671224972771723, test loss: 0.5931555024042562\n",
      "Iteration 651 training loss: 0.5668751630102754, test loss: 0.5952122168143101\n",
      "Iteration 652 training loss: 0.56655023650508, test loss: 0.5925843485369291\n",
      "Iteration 653 training loss: 0.566013373651154, test loss: 0.5944190532071425\n",
      "Iteration 654 training loss: 0.5660116338564781, test loss: 0.592041842985659\n",
      "Iteration 655 training loss: 0.5651392683746114, test loss: 0.5936152918514148\n",
      "Iteration 656 training loss: 0.5655257657912051, test loss: 0.5915469431184393\n",
      "Iteration 657 training loss: 0.5642534223666165, test loss: 0.592802689040066\n",
      "Iteration 658 training loss: 0.5651054256119097, test loss: 0.5911115939763578\n",
      "Iteration 659 training loss: 0.5633717521727353, test loss: 0.5919959610800057\n",
      "Iteration 660 training loss: 0.5647654203804934, test loss: 0.5907517537080221\n",
      "Iteration 661 training loss: 0.5624800927264526, test loss: 0.5911815966108134\n",
      "Iteration 662 training loss: 0.5644930409642755, test loss: 0.5904549372996647\n",
      "Iteration 663 training loss: 0.5615798159368137, test loss: 0.5903616680744238\n",
      "Iteration 664 training loss: 0.5643211409826896, test loss: 0.5902551074504482\n",
      "Iteration 665 training loss: 0.560698532612042, test loss: 0.5895631190796273\n",
      "Iteration 666 training loss: 0.5642694627273587, test loss: 0.5901720452976519\n",
      "Iteration 667 training loss: 0.5598617177233284, test loss: 0.5888106426767911\n",
      "Iteration 668 training loss: 0.5643460931243777, test loss: 0.5902162110495767\n",
      "Iteration 669 training loss: 0.5591134693387501, test loss: 0.5881478782364222\n",
      "Iteration 670 training loss: 0.5645695705861765, test loss: 0.5904104917011674\n",
      "Iteration 671 training loss: 0.5584943921113927, test loss: 0.5876172575933317\n",
      "Iteration 672 training loss: 0.5649290909582113, test loss: 0.5907480015180361\n",
      "Iteration 673 training loss: 0.5581184915562453, test loss: 0.587332126019068\n",
      "Iteration 674 training loss: 0.5654316754677047, test loss: 0.5912384573810296\n",
      "Iteration 675 training loss: 0.5581051158356793, test loss: 0.5874112316335669\n",
      "Iteration 676 training loss: 0.566058737047669, test loss: 0.5918717987089497\n",
      "Iteration 677 training loss: 0.5586101267208382, test loss: 0.5880107293871982\n",
      "Iteration 678 training loss: 0.5667593308510375, test loss: 0.592598801930063\n",
      "Iteration 679 training loss: 0.5597350204599145, test loss: 0.5892275279274076\n",
      "Iteration 680 training loss: 0.5674012299835275, test loss: 0.5932888551457327\n",
      "Iteration 681 training loss: 0.5614506046840378, test loss: 0.5910302122392281\n",
      "Iteration 682 training loss: 0.5678609218065331, test loss: 0.5938209507837311\n",
      "Iteration 683 training loss: 0.5635446361424664, test loss: 0.593202404767016\n",
      "Iteration 684 training loss: 0.5679277468806532, test loss: 0.5939823620797852\n",
      "Iteration 685 training loss: 0.5655717056162737, test loss: 0.5952959276445703\n",
      "Iteration 686 training loss: 0.5674589544702198, test loss: 0.5936253518063228\n",
      "Iteration 687 training loss: 0.5670551291064639, test loss: 0.5968261535950158\n",
      "Iteration 688 training loss: 0.5664426710083225, test loss: 0.5927266831005624\n",
      "Iteration 689 training loss: 0.5676620550168829, test loss: 0.5974621916271087\n",
      "Iteration 690 training loss: 0.5649981539120457, test loss: 0.5913993107387444\n",
      "Iteration 691 training loss: 0.5673736306426085, test loss: 0.5971880860364858\n",
      "Iteration 692 training loss: 0.5632747454551532, test loss: 0.58979005696474\n",
      "Iteration 693 training loss: 0.5663590627413471, test loss: 0.596175312352158\n",
      "Iteration 694 training loss: 0.5614107832520984, test loss: 0.5880344135628882\n",
      "Iteration 695 training loss: 0.5648328860286752, test loss: 0.5946449842709038\n",
      "Iteration 696 training loss: 0.5595141542503862, test loss: 0.5862386216627116\n",
      "Iteration 697 training loss: 0.5630260937252828, test loss: 0.5928327223109172\n",
      "Iteration 698 training loss: 0.5576620262828946, test loss: 0.5844775145742616\n",
      "Iteration 699 training loss: 0.5611286227968799, test loss: 0.5909341474586975\n",
      "Iteration 700 training loss: 0.5559018363069965, test loss: 0.5827987886084947\n",
      "Iteration 701 training loss: 0.5592378144407213, test loss: 0.589047068530062\n",
      "Iteration 702 training loss: 0.5542907971049555, test loss: 0.581262323009343\n",
      "Iteration 703 training loss: 0.5574771236936269, test loss: 0.587298775671777\n",
      "Iteration 704 training loss: 0.5528442934046357, test loss: 0.5798830650380109\n",
      "Iteration 705 training loss: 0.5558596402814456, test loss: 0.5857001059796199\n",
      "Iteration 706 training loss: 0.5515455471717047, test loss: 0.5786456639987109\n",
      "Iteration 707 training loss: 0.5543957709905196, test loss: 0.584264313509994\n",
      "Iteration 708 training loss: 0.5503801695700059, test loss: 0.5775344979548087\n",
      "Iteration 709 training loss: 0.5530591547775822, test loss: 0.5829641603101066\n",
      "Iteration 710 training loss: 0.5493405530517902, test loss: 0.5765422467334244\n",
      "Iteration 711 training loss: 0.551856435898171, test loss: 0.5818048330057302\n",
      "Iteration 712 training loss: 0.548432573343965, test loss: 0.57567637259402\n",
      "Iteration 713 training loss: 0.5507863702943864, test loss: 0.5807832559687295\n",
      "Iteration 714 training loss: 0.5476353737169652, test loss: 0.5749165681111218\n",
      "Iteration 715 training loss: 0.5498165473098434, test loss: 0.5798685251441985\n",
      "Iteration 716 training loss: 0.5469337989126091, test loss: 0.5742453823497725\n",
      "Iteration 717 training loss: 0.5489198215615901, test loss: 0.5790323033622897\n",
      "Iteration 718 training loss: 0.5463279916012298, test loss: 0.5736642997181588\n",
      "Iteration 719 training loss: 0.5481050700033844, test loss: 0.578281047468424\n",
      "Iteration 720 training loss: 0.5458094664397279, test loss: 0.5731659299438079\n",
      "Iteration 721 training loss: 0.547346800193063, test loss: 0.5775882695462417\n",
      "Iteration 722 training loss: 0.545363832941573, test loss: 0.5727372337975806\n",
      "Iteration 723 training loss: 0.546628730446239, test loss: 0.5769388721056444\n",
      "Iteration 724 training loss: 0.5449833463394573, test loss: 0.5723714149851538\n",
      "Iteration 725 training loss: 0.5459315667539288, test loss: 0.576311302489874\n",
      "Iteration 726 training loss: 0.5446607685245988, test loss: 0.5720597249394459\n",
      "Iteration 727 training loss: 0.5452440570891044, test loss: 0.5756932578913183\n",
      "Iteration 728 training loss: 0.544373393576047, test loss: 0.5717811789805988\n",
      "Iteration 729 training loss: 0.5445590826254245, test loss: 0.5750766160843486\n",
      "Iteration 730 training loss: 0.5441396558595074, test loss: 0.5715550764890193\n",
      "Iteration 731 training loss: 0.5438904409586102, test loss: 0.5744761599773592\n",
      "Iteration 732 training loss: 0.5439424870965304, test loss: 0.5713648675546092\n",
      "Iteration 733 training loss: 0.5432081822439446, test loss: 0.5738585219631736\n",
      "Iteration 734 training loss: 0.5437588542709864, test loss: 0.5711877176580686\n",
      "Iteration 735 training loss: 0.5424989368848644, test loss: 0.5732108003968762\n",
      "Iteration 736 training loss: 0.5435767749812032, test loss: 0.5710125873021068\n",
      "Iteration 737 training loss: 0.5417722094324812, test loss: 0.5725427465030687\n",
      "Iteration 738 training loss: 0.543407496678728, test loss: 0.5708517743402912\n",
      "Iteration 739 training loss: 0.5410411588928499, test loss: 0.5718669365835258\n",
      "Iteration 740 training loss: 0.5432289461876039, test loss: 0.5706829830641252\n",
      "Iteration 741 training loss: 0.5402961503863682, test loss: 0.5711745543087297\n",
      "Iteration 742 training loss: 0.543039048260054, test loss: 0.5705046436518187\n",
      "Iteration 743 training loss: 0.5395500297651116, test loss: 0.5704780219664184\n",
      "Iteration 744 training loss: 0.5428347112417305, test loss: 0.5703124320606852\n",
      "Iteration 745 training loss: 0.5387973642704643, test loss: 0.5697702007459761\n",
      "Iteration 746 training loss: 0.5426079891843473, test loss: 0.570100574207779\n",
      "Iteration 747 training loss: 0.5380437504601547, test loss: 0.5690590068113663\n",
      "Iteration 748 training loss: 0.5423633783000745, test loss: 0.5698720496290027\n",
      "Iteration 749 training loss: 0.5372974818839603, test loss: 0.5683501990582729\n",
      "Iteration 750 training loss: 0.5421075398084525, test loss: 0.5696340719393548\n",
      "Iteration 751 training loss: 0.5365711675060698, test loss: 0.5676594379258972\n",
      "Iteration 752 training loss: 0.5418429876434282, test loss: 0.5693901952079561\n",
      "Iteration 753 training loss: 0.5358837304500024, test loss: 0.5670029630372113\n",
      "Iteration 754 training loss: 0.5415842126992795, test loss: 0.5691555857051036\n",
      "Iteration 755 training loss: 0.5352435506379929, test loss: 0.5663921575248331\n",
      "Iteration 756 training loss: 0.5413387043875932, test loss: 0.5689350257954269\n",
      "Iteration 757 training loss: 0.5346522956944139, test loss: 0.5658273160477656\n",
      "Iteration 758 training loss: 0.5410966585150795, test loss: 0.5687205582349208\n",
      "Iteration 759 training loss: 0.5341291136966972, test loss: 0.5653270930838525\n",
      "Iteration 760 training loss: 0.5408750604246608, test loss: 0.5685280105256647\n",
      "Iteration 761 training loss: 0.5336784234735941, test loss: 0.5648976462546657\n",
      "Iteration 762 training loss: 0.5406598274679003, test loss: 0.568344935574235\n",
      "Iteration 763 training loss: 0.5333130733143945, test loss: 0.5645510494693714\n",
      "Iteration 764 training loss: 0.5404530153257728, test loss: 0.568176574871439\n",
      "Iteration 765 training loss: 0.5330478399374718, test loss: 0.5643029940518273\n",
      "Iteration 766 training loss: 0.5402651833348474, test loss: 0.5680319601461458\n",
      "Iteration 767 training loss: 0.5329063788038625, test loss: 0.5641749699833419\n",
      "Iteration 768 training loss: 0.5400945108177815, test loss: 0.567909133163271\n",
      "Iteration 769 training loss: 0.5328872175964053, test loss: 0.5641665125198944\n",
      "Iteration 770 training loss: 0.5399450747792377, test loss: 0.5678134313599504\n",
      "Iteration 771 training loss: 0.5329908285782925, test loss: 0.5642760198333383\n",
      "Iteration 772 training loss: 0.5397752403513018, test loss: 0.5677047783353998\n",
      "Iteration 773 training loss: 0.5332270968648868, test loss: 0.5645116463086142\n",
      "Iteration 774 training loss: 0.5395985259219407, test loss: 0.5675963928749731\n",
      "Iteration 775 training loss: 0.5335962875905337, test loss: 0.5648761502539703\n",
      "Iteration 776 training loss: 0.5394077218855828, test loss: 0.5674851712032141\n",
      "Iteration 777 training loss: 0.5340783909064407, test loss: 0.565347564124081\n",
      "Iteration 778 training loss: 0.5391901458171611, test loss: 0.567358649233523\n",
      "Iteration 779 training loss: 0.5346312040890789, test loss: 0.5658820801895991\n",
      "Iteration 780 training loss: 0.5389193018817521, test loss: 0.5671891705669643\n",
      "Iteration 781 training loss: 0.535205738679252, test loss: 0.5664323647911937\n",
      "Iteration 782 training loss: 0.5385751273388439, test loss: 0.5669545847288976\n",
      "Iteration 783 training loss: 0.5357199086965242, test loss: 0.5669172881801503\n",
      "Iteration 784 training loss: 0.5381037326029546, test loss: 0.5665983095058951\n",
      "Iteration 785 training loss: 0.5360844996650176, test loss: 0.567247015780243\n",
      "Iteration 786 training loss: 0.5374974959363008, test loss: 0.5661131023063675\n",
      "Iteration 787 training loss: 0.5362384330119848, test loss: 0.5673614744134539\n",
      "Iteration 788 training loss: 0.5367524402316082, test loss: 0.5654915477501219\n",
      "Iteration 789 training loss: 0.5361173355712074, test loss: 0.5671983325814058\n",
      "Iteration 790 training loss: 0.5358568389645169, test loss: 0.5647203068868998\n",
      "Iteration 791 training loss: 0.535677896705405, test loss: 0.5667157366866483\n",
      "Iteration 792 training loss: 0.5347923398422603, test loss: 0.5637789713692677\n",
      "Iteration 793 training loss: 0.5349369660680205, test loss: 0.5659289623235649\n",
      "Iteration 794 training loss: 0.5335841947361467, test loss: 0.5626905613361044\n",
      "Iteration 795 training loss: 0.5339311321395407, test loss: 0.564876514491561\n",
      "Iteration 796 training loss: 0.5322726893441421, test loss: 0.5614971530079877\n",
      "Iteration 797 training loss: 0.532734058019368, test loss: 0.5636355660579307\n",
      "Iteration 798 training loss: 0.5309161462593922, test loss: 0.5602567811569344\n",
      "Iteration 799 training loss: 0.5314152994939333, test loss: 0.5622736981744924\n",
      "Iteration 800 training loss: 0.5295564339321255, test loss: 0.5590133173341574\n",
      "Iteration 801 training loss: 0.5300540253645931, test loss: 0.5608697058242317\n",
      "Iteration 802 training loss: 0.5282216093589834, test loss: 0.5577964385963003\n",
      "Iteration 803 training loss: 0.5286642724153775, test loss: 0.5594388576416721\n",
      "Iteration 804 training loss: 0.5269442933614553, test loss: 0.5566387496932088\n",
      "Iteration 805 training loss: 0.527301386485485, test loss: 0.5580314783950159\n",
      "Iteration 806 training loss: 0.5257377893193154, test loss: 0.5555578365490477\n",
      "Iteration 807 training loss: 0.5259621815623521, test loss: 0.5566422100785726\n",
      "Iteration 808 training loss: 0.5246204109680326, test loss: 0.5545781862576515\n",
      "Iteration 809 training loss: 0.5246969600523574, test loss: 0.5553191016284844\n",
      "Iteration 810 training loss: 0.523677354574093, test loss: 0.5537889324083205\n",
      "Iteration 811 training loss: 0.5235528345743071, test loss: 0.5541015212379216\n",
      "Iteration 812 training loss: 0.5229660179034153, test loss: 0.5532585389059587\n",
      "Iteration 813 training loss: 0.5225656709853956, test loss: 0.5530139380271119\n",
      "Iteration 814 training loss: 0.5226613671160618, test loss: 0.5531770250611837\n",
      "Iteration 815 training loss: 0.5218636545668136, test loss: 0.5521751076057657\n",
      "Iteration 816 training loss: 0.5230308133102767, test loss: 0.5538316607247956\n",
      "Iteration 817 training loss: 0.5217141637649164, test loss: 0.5518364875264286\n",
      "Iteration 818 training loss: 0.5246272757912964, test loss: 0.5557929747101579\n",
      "Iteration 819 training loss: 0.5226190983907163, test loss: 0.5524846940400735\n",
      "Iteration 820 training loss: 0.5281563665570046, test loss: 0.5597749263820753\n",
      "Iteration 821 training loss: 0.5254819708331944, test loss: 0.5550217620568306\n",
      "Iteration 822 training loss: 0.5341861684937623, test loss: 0.5663179798817799\n",
      "Iteration 823 training loss: 0.531257874740168, test loss: 0.5604349944637819\n",
      "Iteration 824 training loss: 0.5412803447462552, test loss: 0.5738764702282597\n",
      "Iteration 825 training loss: 0.5388288619198105, test loss: 0.5676749268134531\n",
      "Iteration 826 training loss: 0.5448644505735906, test loss: 0.5777140808994181\n",
      "Iteration 827 training loss: 0.5437502945961195, test loss: 0.5723813083299774\n",
      "Iteration 828 training loss: 0.542646311254914, test loss: 0.5755149431926689\n",
      "Iteration 829 training loss: 0.5433618289975051, test loss: 0.5719106453033451\n",
      "Iteration 830 training loss: 0.5377138204859557, test loss: 0.5705002989837226\n",
      "Iteration 831 training loss: 0.5402842034255011, test loss: 0.5688268644087946\n",
      "Iteration 832 training loss: 0.5330027690082275, test loss: 0.565710795499527\n",
      "Iteration 833 training loss: 0.5369452940360194, test loss: 0.5655169162641133\n",
      "Iteration 834 training loss: 0.5293146244142077, test loss: 0.561973155734189\n",
      "Iteration 835 training loss: 0.5340886352057368, test loss: 0.5627070954963144\n",
      "Iteration 836 training loss: 0.5265472928806898, test loss: 0.5591853460662097\n",
      "Iteration 837 training loss: 0.5317488614594587, test loss: 0.560419783611591\n",
      "Iteration 838 training loss: 0.5244334644828411, test loss: 0.5570682634674877\n",
      "Iteration 839 training loss: 0.5298470197713913, test loss: 0.5585732634796676\n",
      "Iteration 840 training loss: 0.5227883872961643, test loss: 0.5554338655174094\n",
      "Iteration 841 training loss: 0.52827701349181, test loss: 0.5570602432456604\n",
      "Iteration 842 training loss: 0.5214852792420703, test loss: 0.5541553374473082\n",
      "Iteration 843 training loss: 0.5269424055443058, test loss: 0.555780720181373\n",
      "Iteration 844 training loss: 0.5204130029605157, test loss: 0.5531083862563936\n",
      "Iteration 845 training loss: 0.5258184375339251, test loss: 0.5547109691087231\n",
      "Iteration 846 training loss: 0.5195311693741617, test loss: 0.5522591755086558\n",
      "Iteration 847 training loss: 0.5248412155147096, test loss: 0.5537856003939754\n",
      "Iteration 848 training loss: 0.5187998954073492, test loss: 0.5515641714580856\n",
      "Iteration 849 training loss: 0.5239937008524828, test loss: 0.552988684921259\n",
      "Iteration 850 training loss: 0.5181770283443536, test loss: 0.5509797817773251\n",
      "Iteration 851 training loss: 0.5232352267601641, test loss: 0.5522795167387926\n",
      "Iteration 852 training loss: 0.5176254721207896, test loss: 0.5504649941635875\n",
      "Iteration 853 training loss: 0.5225197383478649, test loss: 0.5516116970732233\n",
      "Iteration 854 training loss: 0.5170997218515112, test loss: 0.5499731788395195\n",
      "Iteration 855 training loss: 0.5218502039419021, test loss: 0.5509895463467066\n",
      "Iteration 856 training loss: 0.5166159167133431, test loss: 0.5495247179086611\n",
      "Iteration 857 training loss: 0.5212133104920444, test loss: 0.5503993774997666\n",
      "Iteration 858 training loss: 0.5161479784395935, test loss: 0.5490930577354843\n",
      "Iteration 859 training loss: 0.5205982465220724, test loss: 0.5498310621389444\n",
      "Iteration 860 training loss: 0.515713337180473, test loss: 0.5486928127232658\n",
      "Iteration 861 training loss: 0.5200087195376095, test loss: 0.5492869057786407\n",
      "Iteration 862 training loss: 0.5152786928461486, test loss: 0.5482912327905374\n",
      "Iteration 863 training loss: 0.5194226298834348, test loss: 0.5487461905629354\n",
      "Iteration 864 training loss: 0.514837622380679, test loss: 0.5478823184986757\n",
      "Iteration 865 training loss: 0.5188599538229365, test loss: 0.5482293764956161\n",
      "Iteration 866 training loss: 0.5144260136532506, test loss: 0.5475040111516347\n",
      "Iteration 867 training loss: 0.5183054525397556, test loss: 0.5477201912320806\n",
      "Iteration 868 training loss: 0.5140141545315122, test loss: 0.5471239426021907\n",
      "Iteration 869 training loss: 0.5177638243048269, test loss: 0.5472232387456438\n",
      "Iteration 870 training loss: 0.5136015290948235, test loss: 0.5467422754648327\n",
      "Iteration 871 training loss: 0.5172363939477027, test loss: 0.546739137744681\n",
      "Iteration 872 training loss: 0.5131789303725268, test loss: 0.5463505780109362\n",
      "Iteration 873 training loss: 0.5167073270431114, test loss: 0.5462533685094493\n",
      "Iteration 874 training loss: 0.5127540196840845, test loss: 0.5459563832992355\n",
      "Iteration 875 training loss: 0.5161843325776562, test loss: 0.5457727825601966\n",
      "Iteration 876 training loss: 0.512313435971508, test loss: 0.5455455423867277\n",
      "Iteration 877 training loss: 0.5156710958379264, test loss: 0.545300696911072\n",
      "Iteration 878 training loss: 0.5118641735565322, test loss: 0.5451247605466267\n",
      "Iteration 879 training loss: 0.5151508980266878, test loss: 0.5448208309450896\n",
      "Iteration 880 training loss: 0.5114035787368271, test loss: 0.5446917720151797\n",
      "Iteration 881 training loss: 0.5146371337305534, test loss: 0.5443467847259049\n",
      "Iteration 882 training loss: 0.5109340902643065, test loss: 0.5442511399253411\n",
      "Iteration 883 training loss: 0.5141266053737897, test loss: 0.5438755657437023\n",
      "Iteration 884 training loss: 0.5104558026154291, test loss: 0.5438017529803472\n",
      "Iteration 885 training loss: 0.5136258766338854, test loss: 0.5434132334252763\n",
      "Iteration 886 training loss: 0.5099795543700927, test loss: 0.5433548796364451\n",
      "Iteration 887 training loss: 0.513140039234145, test loss: 0.542964806124354\n",
      "Iteration 888 training loss: 0.5095049552494088, test loss: 0.5429101487995677\n",
      "Iteration 889 training loss: 0.5126750868665888, test loss: 0.5425361954961497\n",
      "Iteration 890 training loss: 0.5090321601871539, test loss: 0.5424669703749612\n",
      "Iteration 891 training loss: 0.5122153877239405, test loss: 0.5421120466307463\n",
      "Iteration 892 training loss: 0.5085531940030995, test loss: 0.5420176940739533\n",
      "Iteration 893 training loss: 0.5117710506324077, test loss: 0.5417028018981263\n",
      "Iteration 894 training loss: 0.508084313689552, test loss: 0.541578896716541\n",
      "Iteration 895 training loss: 0.5113406411800037, test loss: 0.5413063521616814\n",
      "Iteration 896 training loss: 0.5076151372622546, test loss: 0.5411397209277833\n",
      "Iteration 897 training loss: 0.5109302457139014, test loss: 0.5409294363378985\n",
      "Iteration 898 training loss: 0.5071501574275656, test loss: 0.5407048586539434\n",
      "Iteration 899 training loss: 0.5105347619019651, test loss: 0.5405677464413\n",
      "Iteration 900 training loss: 0.5066953420380043, test loss: 0.5402802572420897\n",
      "Iteration 901 training loss: 0.5101434859533337, test loss: 0.5402099233619378\n",
      "Iteration 902 training loss: 0.5062409136342603, test loss: 0.5398560352160657\n",
      "Iteration 903 training loss: 0.5097536736486993, test loss: 0.5398527743276559\n",
      "Iteration 904 training loss: 0.5057899992214159, test loss: 0.5394355797421916\n",
      "Iteration 905 training loss: 0.5093776056916378, test loss: 0.5395083632413159\n",
      "Iteration 906 training loss: 0.5053433491154804, test loss: 0.5390198918480519\n",
      "Iteration 907 training loss: 0.5090233642456246, test loss: 0.5391849597110984\n",
      "Iteration 908 training loss: 0.504907859039071, test loss: 0.5386150215396466\n",
      "Iteration 909 training loss: 0.5086752736107629, test loss: 0.538867246422367\n",
      "Iteration 910 training loss: 0.50446797363084, test loss: 0.5382052823674159\n",
      "Iteration 911 training loss: 0.5083277373615492, test loss: 0.5385502694831049\n",
      "Iteration 912 training loss: 0.5040267950857787, test loss: 0.5377938428052035\n",
      "Iteration 913 training loss: 0.5079824921838285, test loss: 0.5382350718716802\n",
      "Iteration 914 training loss: 0.5035825063495594, test loss: 0.5373786263150001\n",
      "Iteration 915 training loss: 0.5076474970313465, test loss: 0.5379296598140312\n",
      "Iteration 916 training loss: 0.5031399803218899, test loss: 0.5369642931323021\n",
      "Iteration 917 training loss: 0.5073101286175663, test loss: 0.5376224242526964\n",
      "Iteration 918 training loss: 0.5026973853643276, test loss: 0.5365491295881563\n",
      "Iteration 919 training loss: 0.5069763181617178, test loss: 0.5373187556415594\n",
      "Iteration 920 training loss: 0.5022672930579472, test loss: 0.5361467166520418\n",
      "Iteration 921 training loss: 0.5066410780544781, test loss: 0.5370131266058744\n",
      "Iteration 922 training loss: 0.5018465663048257, test loss: 0.5357541634689733\n",
      "Iteration 923 training loss: 0.5063260426606916, test loss: 0.5367269908731985\n",
      "Iteration 924 training loss: 0.5014264026736033, test loss: 0.5353614673311394\n",
      "Iteration 925 training loss: 0.5060092564370715, test loss: 0.5364399228678006\n",
      "Iteration 926 training loss: 0.501018202824542, test loss: 0.5349814429979589\n",
      "Iteration 927 training loss: 0.5057014744167041, test loss: 0.5361620667995086\n",
      "Iteration 928 training loss: 0.5006213430021736, test loss: 0.534613880872599\n",
      "Iteration 929 training loss: 0.5053870200611474, test loss: 0.5358779167871338\n",
      "Iteration 930 training loss: 0.5002279946196576, test loss: 0.534248530804806\n",
      "Iteration 931 training loss: 0.5050714403864944, test loss: 0.5355921496167335\n",
      "Iteration 932 training loss: 0.4998331553523504, test loss: 0.5338808457511783\n",
      "Iteration 933 training loss: 0.5047508471206312, test loss: 0.5353021240878975\n",
      "Iteration 934 training loss: 0.4994417651614469, test loss: 0.5335165722045507\n",
      "Iteration 935 training loss: 0.504429787014868, test loss: 0.5350113885726372\n",
      "Iteration 936 training loss: 0.4990554814796102, test loss: 0.5331563373267174\n",
      "Iteration 937 training loss: 0.5041129747800095, test loss: 0.5347243458903822\n",
      "Iteration 938 training loss: 0.49867914946650066, test loss: 0.5328066300727693\n",
      "Iteration 939 training loss: 0.5038027919780703, test loss: 0.5344440399818122\n",
      "Iteration 940 training loss: 0.4983112244078451, test loss: 0.5324668217744826\n",
      "Iteration 941 training loss: 0.503490128447868, test loss: 0.5341612231052415\n",
      "Iteration 942 training loss: 0.4979413311371093, test loss: 0.532123715205657\n",
      "Iteration 943 training loss: 0.5031723342364076, test loss: 0.5338734837376978\n",
      "Iteration 944 training loss: 0.4975680214164264, test loss: 0.5317766775808748\n",
      "Iteration 945 training loss: 0.5028541637216783, test loss: 0.5335855291737561\n",
      "Iteration 946 training loss: 0.497204447508843, test loss: 0.5314385540161162\n",
      "Iteration 947 training loss: 0.5025363943448696, test loss: 0.5332978158469163\n",
      "Iteration 948 training loss: 0.49683614732308434, test loss: 0.531093766585271\n",
      "Iteration 949 training loss: 0.5022110457033848, test loss: 0.5330027508316605\n",
      "Iteration 950 training loss: 0.4964670827638256, test loss: 0.5307474428802618\n",
      "Iteration 951 training loss: 0.5018827575924527, test loss: 0.5327041927863788\n",
      "Iteration 952 training loss: 0.4960898308847175, test loss: 0.530391469325799\n",
      "Iteration 953 training loss: 0.5015427768453318, test loss: 0.5323944299463061\n",
      "Iteration 954 training loss: 0.4957186795024507, test loss: 0.5300425962959341\n",
      "Iteration 955 training loss: 0.5012067020342929, test loss: 0.5320882681324838\n",
      "Iteration 956 training loss: 0.49535418762903044, test loss: 0.52970107242472\n",
      "Iteration 957 training loss: 0.5008689207266082, test loss: 0.5317807510787222\n",
      "Iteration 958 training loss: 0.4949978492666473, test loss: 0.5293677026460623\n",
      "Iteration 959 training loss: 0.5005393782237098, test loss: 0.5314813642803025\n",
      "Iteration 960 training loss: 0.49464269958609125, test loss: 0.5290352253993914\n",
      "Iteration 961 training loss: 0.5002104402390255, test loss: 0.5311828884001619\n",
      "Iteration 962 training loss: 0.49429737113640304, test loss: 0.5287120194956986\n",
      "Iteration 963 training loss: 0.4998874052081706, test loss: 0.5308897885876\n",
      "Iteration 964 training loss: 0.4939518208368171, test loss: 0.528388075242031\n",
      "Iteration 965 training loss: 0.4995603407806333, test loss: 0.5305923722679273\n",
      "Iteration 966 training loss: 0.4936172296043785, test loss: 0.5280752145597856\n",
      "Iteration 967 training loss: 0.49924202944267465, test loss: 0.5303026962560018\n",
      "Iteration 968 training loss: 0.49328540656844305, test loss: 0.5277654536412971\n",
      "Iteration 969 training loss: 0.49892398223764, test loss: 0.5300128749447001\n",
      "Iteration 970 training loss: 0.49295707709072145, test loss: 0.5274593365692217\n",
      "Iteration 971 training loss: 0.4986070560979835, test loss: 0.5297239624708135\n",
      "Iteration 972 training loss: 0.49262863442669563, test loss: 0.527152486204693\n",
      "Iteration 973 training loss: 0.49828522687325627, test loss: 0.5294302991707693\n",
      "Iteration 974 training loss: 0.4923055523166602, test loss: 0.5268506516392466\n",
      "Iteration 975 training loss: 0.49796830490656324, test loss: 0.5291410898241171\n",
      "Iteration 976 training loss: 0.49198924536671496, test loss: 0.5265557533950992\n",
      "Iteration 977 training loss: 0.49765020170743945, test loss: 0.5288507360483204\n",
      "Iteration 978 training loss: 0.4916805132225116, test loss: 0.526268897617416\n",
      "Iteration 979 training loss: 0.4973350713886962, test loss: 0.5285636673617438\n",
      "Iteration 980 training loss: 0.49137376369362923, test loss: 0.525983573172035\n",
      "Iteration 981 training loss: 0.4970200808236924, test loss: 0.5282765836066682\n",
      "Iteration 982 training loss: 0.4910718682590107, test loss: 0.525704598813406\n",
      "Iteration 983 training loss: 0.4967064113672123, test loss: 0.5279906932715785\n",
      "Iteration 984 training loss: 0.49077314394427873, test loss: 0.5254289155655253\n",
      "Iteration 985 training loss: 0.49638417095500764, test loss: 0.5276968623513105\n",
      "Iteration 986 training loss: 0.49047248548308514, test loss: 0.5251512412635438\n",
      "Iteration 987 training loss: 0.49605204827341753, test loss: 0.5273929900901199\n",
      "Iteration 988 training loss: 0.4901814100868283, test loss: 0.5248847128032788\n",
      "Iteration 989 training loss: 0.49571859494363923, test loss: 0.5270880913328473\n",
      "Iteration 990 training loss: 0.4899097978361557, test loss: 0.5246394637621133\n",
      "Iteration 991 training loss: 0.4954031490861982, test loss: 0.5268005019903101\n",
      "Iteration 992 training loss: 0.4896602140060929, test loss: 0.5244175874323952\n",
      "Iteration 993 training loss: 0.49510113011889395, test loss: 0.5265268968040674\n",
      "Iteration 994 training loss: 0.4894541076005228, test loss: 0.5242410177847455\n",
      "Iteration 995 training loss: 0.49483037505834043, test loss: 0.5262848119456385\n",
      "Iteration 996 training loss: 0.489305094924479, test loss: 0.524122732422688\n",
      "Iteration 997 training loss: 0.4945872833520826, test loss: 0.5260703833979372\n",
      "Iteration 998 training loss: 0.4892334079130315, test loss: 0.5240823870844636\n",
      "Iteration 999 training loss: 0.49438647040690126, test loss: 0.5259004029449189\n",
      "Iteration 1000 training loss: 0.4892745943812599, test loss: 0.5241570582524606\n",
      "Iteration 1001 training loss: 0.49424933508362867, test loss: 0.5257964200883626\n",
      "Iteration 1002 training loss: 0.4894880724919216, test loss: 0.5244041264571669\n",
      "Iteration 1003 training loss: 0.4941993742618151, test loss: 0.5257821414824444\n",
      "Iteration 1004 training loss: 0.489933511191944, test loss: 0.524884894659764\n",
      "Iteration 1005 training loss: 0.4942781397700996, test loss: 0.5259039087550136\n",
      "Iteration 1006 training loss: 0.4906901442473216, test loss: 0.5256760788953123\n",
      "Iteration 1007 training loss: 0.49450947233146214, test loss: 0.5261841010301384\n",
      "Iteration 1008 training loss: 0.4918244180511824, test loss: 0.5268369358175092\n",
      "Iteration 1009 training loss: 0.4948521105888042, test loss: 0.5265803193236224\n",
      "Iteration 1010 training loss: 0.49333089177343464, test loss: 0.5283604037773529\n",
      "Iteration 1011 training loss: 0.49529548240503335, test loss: 0.5270862132329184\n",
      "Iteration 1012 training loss: 0.49515636045631894, test loss: 0.530192604803157\n",
      "Iteration 1013 training loss: 0.4957459806420226, test loss: 0.5276040837881306\n",
      "Iteration 1014 training loss: 0.4970814058993623, test loss: 0.5321075918630163\n",
      "Iteration 1015 training loss: 0.49603930858986306, test loss: 0.5279696178682967\n",
      "Iteration 1016 training loss: 0.4986703958644874, test loss: 0.5336631171462617\n",
      "Iteration 1017 training loss: 0.4959466459258319, test loss: 0.5279582170804431\n",
      "Iteration 1018 training loss: 0.49947357868405, test loss: 0.5344010410099962\n",
      "Iteration 1019 training loss: 0.49525767778861246, test loss: 0.5273544768967007\n",
      "Iteration 1020 training loss: 0.4991565544185871, test loss: 0.5339955783840514\n",
      "Iteration 1021 training loss: 0.4938992279503094, test loss: 0.5260765470931059\n",
      "Iteration 1022 training loss: 0.4976315062595132, test loss: 0.5323688889175741\n",
      "Iteration 1023 training loss: 0.49206547065030753, test loss: 0.5243261101840309\n",
      "Iteration 1024 training loss: 0.49530404840272785, test loss: 0.5299366467162128\n",
      "Iteration 1025 training loss: 0.4899509653160052, test loss: 0.5222954800245766\n",
      "Iteration 1026 training loss: 0.4926587188591971, test loss: 0.5271911118305035\n",
      "Iteration 1027 training loss: 0.4878420337863145, test loss: 0.5202619950588749\n",
      "Iteration 1028 training loss: 0.4900535155901052, test loss: 0.5245040543265937\n",
      "Iteration 1029 training loss: 0.48589616519193946, test loss: 0.5183833809943529\n",
      "Iteration 1030 training loss: 0.48773250650060035, test loss: 0.5221248073025672\n",
      "Iteration 1031 training loss: 0.4841994501786458, test loss: 0.5167527075524054\n",
      "Iteration 1032 training loss: 0.485753353474036, test loss: 0.5201148779426747\n",
      "Iteration 1033 training loss: 0.48276963419742264, test loss: 0.5153863297224434\n",
      "Iteration 1034 training loss: 0.4841666393831462, test loss: 0.5185233383521741\n",
      "Iteration 1035 training loss: 0.4816444019235682, test loss: 0.5143192645605326\n",
      "Iteration 1036 training loss: 0.48296163303098666, test loss: 0.5173377363268905\n",
      "Iteration 1037 training loss: 0.4807890476607856, test loss: 0.5135104827463504\n",
      "Iteration 1038 training loss: 0.4820650681093798, test loss: 0.5164795663108874\n",
      "Iteration 1039 training loss: 0.4801703893286242, test loss: 0.512926189783109\n",
      "Iteration 1040 training loss: 0.48143761373805677, test loss: 0.5159103495169367\n",
      "Iteration 1041 training loss: 0.4797857807016431, test loss: 0.5125626197049532\n",
      "Iteration 1042 training loss: 0.4810664391860856, test loss: 0.5156174522521294\n",
      "Iteration 1043 training loss: 0.4796505161775627, test loss: 0.5124315472149784\n",
      "Iteration 1044 training loss: 0.4809307910376626, test loss: 0.5155810732982785\n",
      "Iteration 1045 training loss: 0.47980042752238694, test loss: 0.5125690519379524\n",
      "Iteration 1046 training loss: 0.4810565247306279, test loss: 0.5158277156965245\n",
      "Iteration 1047 training loss: 0.4802996591840023, test loss: 0.5130339436358439\n",
      "Iteration 1048 training loss: 0.4814354941940912, test loss: 0.5163467248701621\n",
      "Iteration 1049 training loss: 0.48122866437718753, test loss: 0.5139063659719483\n",
      "Iteration 1050 training loss: 0.4820491659376031, test loss: 0.5171155911271024\n",
      "Iteration 1051 training loss: 0.48268938849327875, test loss: 0.5152934202665463\n",
      "Iteration 1052 training loss: 0.4828795556457864, test loss: 0.5181088565551041\n",
      "Iteration 1053 training loss: 0.4847422068047343, test loss: 0.5172592105493328\n",
      "Iteration 1054 training loss: 0.48377482179097303, test loss: 0.519163967320196\n",
      "Iteration 1055 training loss: 0.4872370077787019, test loss: 0.519666621133572\n",
      "Iteration 1056 training loss: 0.48454144601161936, test loss: 0.5200697122413915\n",
      "Iteration 1057 training loss: 0.48978281720600403, test loss: 0.5221332862771029\n",
      "Iteration 1058 training loss: 0.48496423928634524, test loss: 0.5205982834199325\n",
      "Iteration 1059 training loss: 0.49195617487552845, test loss: 0.5242499451175539\n",
      "Iteration 1060 training loss: 0.48501636632958717, test loss: 0.5207253637879757\n",
      "Iteration 1061 training loss: 0.49340406719577085, test loss: 0.5256659127366227\n",
      "Iteration 1062 training loss: 0.484838403313651, test loss: 0.5205895077689222\n",
      "Iteration 1063 training loss: 0.4940693865226919, test loss: 0.5263219108891659\n",
      "Iteration 1064 training loss: 0.4846098550282854, test loss: 0.5203660576912323\n",
      "Iteration 1065 training loss: 0.49425464398972374, test loss: 0.5265192790841312\n",
      "Iteration 1066 training loss: 0.4845477235897366, test loss: 0.5202908988886952\n",
      "Iteration 1067 training loss: 0.49422462541205775, test loss: 0.526517081656193\n",
      "Iteration 1068 training loss: 0.484760644042469, test loss: 0.52047712316457\n",
      "Iteration 1069 training loss: 0.4941381501998954, test loss: 0.526473358177056\n",
      "Iteration 1070 training loss: 0.48530798974410577, test loss: 0.5209787417832954\n",
      "Iteration 1071 training loss: 0.49411890774741457, test loss: 0.5265038682650792\n",
      "Iteration 1072 training loss: 0.4861739620489492, test loss: 0.5217773546316128\n",
      "Iteration 1073 training loss: 0.4941561121398847, test loss: 0.5265980298966031\n",
      "Iteration 1074 training loss: 0.4872518975190822, test loss: 0.5227799224312596\n",
      "Iteration 1075 training loss: 0.49417266329696213, test loss: 0.5266815602158565\n",
      "Iteration 1076 training loss: 0.48834201913224295, test loss: 0.5237865372970822\n",
      "Iteration 1077 training loss: 0.4940140173310441, test loss: 0.5265988274806996\n",
      "Iteration 1078 training loss: 0.48912851230890136, test loss: 0.5244798882614247\n",
      "Iteration 1079 training loss: 0.49348841335124805, test loss: 0.5261472938906644\n",
      "Iteration 1080 training loss: 0.4894273500639742, test loss: 0.5246842424345537\n",
      "Iteration 1081 training loss: 0.492542195643818, test loss: 0.525272055146104\n",
      "Iteration 1082 training loss: 0.48908007249136637, test loss: 0.5242490197606939\n",
      "Iteration 1083 training loss: 0.49110471024968855, test loss: 0.5238961263848905\n",
      "Iteration 1084 training loss: 0.48808009028836846, test loss: 0.5231724817152691\n",
      "Iteration 1085 training loss: 0.4893311534803801, test loss: 0.5221737204439587\n",
      "Iteration 1086 training loss: 0.48658262227321963, test loss: 0.5216123032742181\n",
      "Iteration 1087 training loss: 0.48736585531924226, test loss: 0.5202479355431822\n",
      "Iteration 1088 training loss: 0.4848510706919372, test loss: 0.5198368254368334\n",
      "Iteration 1089 training loss: 0.4853924335420099, test loss: 0.5183086757691591\n",
      "Iteration 1090 training loss: 0.483050099259962, test loss: 0.5180144541734093\n",
      "Iteration 1091 training loss: 0.4835311625706778, test loss: 0.5164801510174498\n",
      "Iteration 1092 training loss: 0.4813393624696697, test loss: 0.5163044990228092\n",
      "Iteration 1093 training loss: 0.48188567101182583, test loss: 0.5148609352345733\n",
      "Iteration 1094 training loss: 0.47976208955080785, test loss: 0.5147477284258721\n",
      "Iteration 1095 training loss: 0.48045484259801613, test loss: 0.513451833953025\n",
      "Iteration 1096 training loss: 0.4783578227943737, test loss: 0.5133797868395588\n",
      "Iteration 1097 training loss: 0.47928143678026086, test loss: 0.5122933743674725\n",
      "Iteration 1098 training loss: 0.4771660681775069, test loss: 0.5122378624861912\n",
      "Iteration 1099 training loss: 0.478358512360736, test loss: 0.511378340061293\n",
      "Iteration 1100 training loss: 0.47615545350167693, test loss: 0.5112891420220359\n",
      "Iteration 1101 training loss: 0.47767688385260665, test loss: 0.510698300397156\n",
      "Iteration 1102 training loss: 0.4753256989694639, test loss: 0.5105347103278803\n",
      "Iteration 1103 training loss: 0.4772411914501531, test loss: 0.5102597843003347\n",
      "Iteration 1104 training loss: 0.4746654174228428, test loss: 0.5099604492612918\n",
      "Iteration 1105 training loss: 0.4770464490816027, test loss: 0.5100549543807832\n",
      "Iteration 1106 training loss: 0.4741662754925623, test loss: 0.5095562163494481\n",
      "Iteration 1107 training loss: 0.477091652787495, test loss: 0.5100844345784401\n",
      "Iteration 1108 training loss: 0.47382423497354914, test loss: 0.5093158535699176\n",
      "Iteration 1109 training loss: 0.47738117821119036, test loss: 0.5103532895473146\n",
      "Iteration 1110 training loss: 0.47364834063153616, test loss: 0.5092466409021589\n",
      "Iteration 1111 training loss: 0.4779045543953711, test loss: 0.5108534982887954\n",
      "Iteration 1112 training loss: 0.4736393431162424, test loss: 0.5093418420738844\n",
      "Iteration 1113 training loss: 0.47864689809693095, test loss: 0.5115711809541943\n",
      "Iteration 1114 training loss: 0.4738635103204485, test loss: 0.5096711102783436\n",
      "Iteration 1115 training loss: 0.4796378123053665, test loss: 0.5125395439240552\n",
      "Iteration 1116 training loss: 0.4743859905844839, test loss: 0.5102954112560075\n",
      "Iteration 1117 training loss: 0.4808427026508194, test loss: 0.5137344563533369\n",
      "Iteration 1118 training loss: 0.4752916421575918, test loss: 0.5112967896770476\n",
      "Iteration 1119 training loss: 0.48223935783669836, test loss: 0.5151370990574347\n",
      "Iteration 1120 training loss: 0.4766839276882914, test loss: 0.5127756122117577\n",
      "Iteration 1121 training loss: 0.48374161852305425, test loss: 0.5166595460435168\n",
      "Iteration 1122 training loss: 0.47859883209079757, test loss: 0.5147611283992128\n",
      "Iteration 1123 training loss: 0.48523052526288607, test loss: 0.5181809818602366\n",
      "Iteration 1124 training loss: 0.4808588121571682, test loss: 0.5170727453973986\n",
      "Iteration 1125 training loss: 0.4864329597429827, test loss: 0.5194247699743203\n",
      "Iteration 1126 training loss: 0.4831510658645499, test loss: 0.5193856179618407\n",
      "Iteration 1127 training loss: 0.4870659525709812, test loss: 0.5201036089984694\n",
      "Iteration 1128 training loss: 0.4849205484014191, test loss: 0.5211413951688453\n",
      "Iteration 1129 training loss: 0.48685371843050895, test loss: 0.5199371099579027\n",
      "Iteration 1130 training loss: 0.48563029639450456, test loss: 0.5218069580170613\n",
      "Iteration 1131 training loss: 0.4857430578351847, test loss: 0.5188736971109508\n",
      "Iteration 1132 training loss: 0.4849967190207169, test loss: 0.5211085912832942\n",
      "Iteration 1133 training loss: 0.48388511897568665, test loss: 0.5170626257985899\n",
      "Iteration 1134 training loss: 0.4832843454216708, test loss: 0.5193190568403158\n",
      "Iteration 1135 training loss: 0.48168452557761454, test loss: 0.5149133865284592\n",
      "Iteration 1136 training loss: 0.48100101435438153, test loss: 0.5169598774021619\n",
      "Iteration 1137 training loss: 0.4794097837831343, test loss: 0.5126852426794075\n",
      "Iteration 1138 training loss: 0.4785735984071078, test loss: 0.5144632164578335\n",
      "Iteration 1139 training loss: 0.47732318469821844, test loss: 0.5106422638677912\n",
      "Iteration 1140 training loss: 0.476322050358319, test loss: 0.5121616160270595\n",
      "Iteration 1141 training loss: 0.4755335736196974, test loss: 0.5088974744539402\n",
      "Iteration 1142 training loss: 0.47437301446409863, test loss: 0.5101854754468879\n",
      "Iteration 1143 training loss: 0.4741123876073668, test loss: 0.5075216432846149\n",
      "Iteration 1144 training loss: 0.47281864337297946, test loss: 0.508625828013924\n",
      "Iteration 1145 training loss: 0.47301622477187344, test loss: 0.506468684308546\n",
      "Iteration 1146 training loss: 0.4716104841650436, test loss: 0.5074287549673809\n",
      "Iteration 1147 training loss: 0.4722542256411818, test loss: 0.5057456904869668\n",
      "Iteration 1148 training loss: 0.47070291094604305, test loss: 0.5065451392214599\n",
      "Iteration 1149 training loss: 0.47177097008236907, test loss: 0.5052956033275383\n",
      "Iteration 1150 training loss: 0.4700429518562014, test loss: 0.5059203952716694\n",
      "Iteration 1151 training loss: 0.47151171422985133, test loss: 0.5050656297110288\n",
      "Iteration 1152 training loss: 0.46956654586930574, test loss: 0.5054881548454984\n",
      "Iteration 1153 training loss: 0.4714305368899574, test loss: 0.5050104250457041\n",
      "Iteration 1154 training loss: 0.46924430821377017, test loss: 0.5052153590611631\n",
      "Iteration 1155 training loss: 0.47151468186120177, test loss: 0.5051166880124259\n",
      "Iteration 1156 training loss: 0.4690341588375253, test loss: 0.5050577738592315\n",
      "Iteration 1157 training loss: 0.47167199243637237, test loss: 0.5052930442604164\n",
      "Iteration 1158 training loss: 0.46888336715784135, test loss: 0.50495725274936\n",
      "Iteration 1159 training loss: 0.4718714443098268, test loss: 0.505510912410425\n",
      "Iteration 1160 training loss: 0.4687637122386064, test loss: 0.5048859302649008\n",
      "Iteration 1161 training loss: 0.47207365209585217, test loss: 0.5057296754406341\n",
      "Iteration 1162 training loss: 0.4686497206504737, test loss: 0.5048178222611069\n",
      "Iteration 1163 training loss: 0.47222585268634604, test loss: 0.5058983322319904\n",
      "Iteration 1164 training loss: 0.4685172213164561, test loss: 0.5047266315346747\n",
      "Iteration 1165 training loss: 0.4723309749395827, test loss: 0.5060213840235349\n",
      "Iteration 1166 training loss: 0.4683531223348014, test loss: 0.5046001183333853\n",
      "Iteration 1167 training loss: 0.4723528795323289, test loss: 0.5060626382890562\n",
      "Iteration 1168 training loss: 0.46815911088804113, test loss: 0.5044382237879826\n",
      "Iteration 1169 training loss: 0.4722982484257032, test loss: 0.5060280553890616\n",
      "Iteration 1170 training loss: 0.4679283737534918, test loss: 0.504236457507595\n",
      "Iteration 1171 training loss: 0.4721708626609058, test loss: 0.505922763101426\n",
      "Iteration 1172 training loss: 0.467676584647291, test loss: 0.5040117136032819\n",
      "Iteration 1173 training loss: 0.4719783981264138, test loss: 0.5057528966446629\n",
      "Iteration 1174 training loss: 0.4673996540691091, test loss: 0.5037579558925116\n",
      "Iteration 1175 training loss: 0.4717388428837793, test loss: 0.5055372283788993\n",
      "Iteration 1176 training loss: 0.4671145738981311, test loss: 0.5034917979620945\n",
      "Iteration 1177 training loss: 0.4714888193874073, test loss: 0.5053111166385932\n",
      "Iteration 1178 training loss: 0.4668326434604366, test loss: 0.5032293652971125\n",
      "Iteration 1179 training loss: 0.47124261558560576, test loss: 0.505089047941014\n",
      "Iteration 1180 training loss: 0.46655964869918454, test loss: 0.5029765534659154\n",
      "Iteration 1181 training loss: 0.47099142056890075, test loss: 0.5048623163788541\n",
      "Iteration 1182 training loss: 0.4662787247575614, test loss: 0.5027153771110999\n",
      "Iteration 1183 training loss: 0.47071586403882104, test loss: 0.5046113764720436\n",
      "Iteration 1184 training loss: 0.46600604090459063, test loss: 0.5024618069311213\n",
      "Iteration 1185 training loss: 0.4704490625523654, test loss: 0.5043690792380902\n",
      "Iteration 1186 training loss: 0.465738047306745, test loss: 0.5022111480314528\n",
      "Iteration 1187 training loss: 0.4701799645431054, test loss: 0.5041245905281915\n",
      "Iteration 1188 training loss: 0.4654752671987915, test loss: 0.5019651572150983\n",
      "Iteration 1189 training loss: 0.46992253623939567, test loss: 0.5038919541466337\n",
      "Iteration 1190 training loss: 0.46522319357568237, test loss: 0.5017302971683636\n",
      "Iteration 1191 training loss: 0.4696718274997487, test loss: 0.5036652593614784\n",
      "Iteration 1192 training loss: 0.46497308816936156, test loss: 0.5014979941297272\n",
      "Iteration 1193 training loss: 0.4694109396000011, test loss: 0.5034294149162399\n",
      "Iteration 1194 training loss: 0.4647269891304546, test loss: 0.50126976325611\n",
      "Iteration 1195 training loss: 0.4691572414927149, test loss: 0.5032011775590508\n",
      "Iteration 1196 training loss: 0.46449121265081345, test loss: 0.5010526317801026\n",
      "Iteration 1197 training loss: 0.46890727868559245, test loss: 0.502976779137608\n",
      "Iteration 1198 training loss: 0.4642587267546653, test loss: 0.5008373668536633\n",
      "Iteration 1199 training loss: 0.46865876475439866, test loss: 0.5027534662208298\n",
      "Iteration 1200 training loss: 0.4640267162483112, test loss: 0.5006229237145234\n",
      "Iteration 1201 training loss: 0.4683996896965025, test loss: 0.5025202021695598\n",
      "Iteration 1202 training loss: 0.4638019226182095, test loss: 0.5004158624854016\n",
      "Iteration 1203 training loss: 0.46816006563037943, test loss: 0.5023059771091961\n",
      "Iteration 1204 training loss: 0.46358862090100744, test loss: 0.5002210573109895\n",
      "Iteration 1205 training loss: 0.46792532149976984, test loss: 0.5020962898933974\n",
      "Iteration 1206 training loss: 0.4633818141214308, test loss: 0.5000328999178695\n",
      "Iteration 1207 training loss: 0.4676919169079872, test loss: 0.5018886904870109\n",
      "Iteration 1208 training loss: 0.4631891385013858, test loss: 0.4998581087945189\n",
      "Iteration 1209 training loss: 0.4674677326544592, test loss: 0.5016896549074398\n",
      "Iteration 1210 training loss: 0.46300717113672896, test loss: 0.4996944487871939\n",
      "Iteration 1211 training loss: 0.4672448625392143, test loss: 0.5014918344973728\n",
      "Iteration 1212 training loss: 0.4628314914131158, test loss: 0.499536587505485\n",
      "Iteration 1213 training loss: 0.46702534441155635, test loss: 0.5012978834633319\n",
      "Iteration 1214 training loss: 0.46266386066246673, test loss: 0.4993874546957514\n",
      "Iteration 1215 training loss: 0.4668067536258681, test loss: 0.5011056188046615\n",
      "Iteration 1216 training loss: 0.4625191256864329, test loss: 0.4992615283802312\n",
      "Iteration 1217 training loss: 0.466594369594429, test loss: 0.5009200053953841\n",
      "Iteration 1218 training loss: 0.4623897145338433, test loss: 0.49914979683939675\n",
      "Iteration 1219 training loss: 0.46637347035764093, test loss: 0.500726962345893\n",
      "Iteration 1220 training loss: 0.46228068250668186, test loss: 0.49905762863676245\n",
      "Iteration 1221 training loss: 0.46616072031971695, test loss: 0.5005415751425265\n",
      "Iteration 1222 training loss: 0.46220677921477493, test loss: 0.49900005077736087\n",
      "Iteration 1223 training loss: 0.4659650960508472, test loss: 0.5003737491024786\n",
      "Iteration 1224 training loss: 0.4621614462372402, test loss: 0.4989714535220128\n",
      "Iteration 1225 training loss: 0.4657624196727061, test loss: 0.5002001451296658\n",
      "Iteration 1226 training loss: 0.46214875389333676, test loss: 0.49897374795375854\n",
      "Iteration 1227 training loss: 0.46557900461585205, test loss: 0.5000467137812565\n",
      "Iteration 1228 training loss: 0.46218238860027977, test loss: 0.4990234199901055\n",
      "Iteration 1229 training loss: 0.4653988033408498, test loss: 0.49989806627766104\n",
      "Iteration 1230 training loss: 0.46224246423536186, test loss: 0.4990962296691155\n",
      "Iteration 1231 training loss: 0.4652327479749615, test loss: 0.49976340409730075\n",
      "Iteration 1232 training loss: 0.46234909593920187, test loss: 0.4992132694094399\n",
      "Iteration 1233 training loss: 0.465065703133576, test loss: 0.49962847744402944\n",
      "Iteration 1234 training loss: 0.46247891715604167, test loss: 0.49935187402166475\n",
      "Iteration 1235 training loss: 0.46489682657838405, test loss: 0.4994926345179475\n",
      "Iteration 1236 training loss: 0.4626073752209649, test loss: 0.4994867718672849\n",
      "Iteration 1237 training loss: 0.46469656637411366, test loss: 0.4993267049214259\n",
      "Iteration 1238 training loss: 0.4627360017378985, test loss: 0.4996218519656217\n",
      "Iteration 1239 training loss: 0.4644891606984976, test loss: 0.49915557283517353\n",
      "Iteration 1240 training loss: 0.46286257673792836, test loss: 0.4997556549402819\n",
      "Iteration 1241 training loss: 0.4642394440640699, test loss: 0.4989415481922294\n",
      "Iteration 1242 training loss: 0.46294515603461417, test loss: 0.49984472339266633\n",
      "Iteration 1243 training loss: 0.46393477699305946, test loss: 0.4986730766631802\n",
      "Iteration 1244 training loss: 0.46293017495937694, test loss: 0.49983140718958186\n",
      "Iteration 1245 training loss: 0.46354975024368483, test loss: 0.4983235478015282\n",
      "Iteration 1246 training loss: 0.4627723780436795, test loss: 0.49967384436378914\n",
      "Iteration 1247 training loss: 0.4630547750825585, test loss: 0.4978641674917022\n",
      "Iteration 1248 training loss: 0.46244778674307996, test loss: 0.49934681938469955\n",
      "Iteration 1249 training loss: 0.4624681666096637, test loss: 0.49731272759300443\n",
      "Iteration 1250 training loss: 0.4620076433607357, test loss: 0.49890298907546804\n",
      "Iteration 1251 training loss: 0.46185747446005254, test loss: 0.49673550487296725\n",
      "Iteration 1252 training loss: 0.4614604061690659, test loss: 0.4983510928226342\n",
      "Iteration 1253 training loss: 0.4611952969502926, test loss: 0.4961054541499976\n",
      "Iteration 1254 training loss: 0.46082127963700703, test loss: 0.4977084499915485\n",
      "Iteration 1255 training loss: 0.46053527996397187, test loss: 0.49547547277202436\n",
      "Iteration 1256 training loss: 0.46013678764349814, test loss: 0.4970223277422638\n",
      "Iteration 1257 training loss: 0.45990889351553893, test loss: 0.4948763988026986\n",
      "Iteration 1258 training loss: 0.4594330306102867, test loss: 0.49632317397104736\n",
      "Iteration 1259 training loss: 0.4593298627788883, test loss: 0.49432391213170157\n",
      "Iteration 1260 training loss: 0.4587639698649715, test loss: 0.4956652344789096\n",
      "Iteration 1261 training loss: 0.4588431348493571, test loss: 0.4938609033865261\n",
      "Iteration 1262 training loss: 0.45815914172847777, test loss: 0.49507789044544187\n",
      "Iteration 1263 training loss: 0.458470566696441, test loss: 0.4935077413373773\n",
      "Iteration 1264 training loss: 0.45761733877092986, test loss: 0.4945600761745395\n",
      "Iteration 1265 training loss: 0.4582218281219181, test loss: 0.49327567164324754\n",
      "Iteration 1266 training loss: 0.45715698513510755, test loss: 0.494131762173305\n",
      "Iteration 1267 training loss: 0.4581053587970582, test loss: 0.49317169237909064\n",
      "Iteration 1268 training loss: 0.45677134592840907, test loss: 0.4937820268260024\n",
      "Iteration 1269 training loss: 0.4581388414961904, test loss: 0.493214050456777\n",
      "Iteration 1270 training loss: 0.4564994015055698, test loss: 0.49355120928494045\n",
      "Iteration 1271 training loss: 0.4583617177203028, test loss: 0.49343958945263766\n",
      "Iteration 1272 training loss: 0.4563140794289672, test loss: 0.49340933925624714\n",
      "Iteration 1273 training loss: 0.45872765868939414, test loss: 0.49380439781985735\n",
      "Iteration 1274 training loss: 0.45623093209588067, test loss: 0.49336952385782235\n",
      "Iteration 1275 training loss: 0.45927570080664937, test loss: 0.49434750094262464\n",
      "Iteration 1276 training loss: 0.45625083623359364, test loss: 0.49343627413170454\n",
      "Iteration 1277 training loss: 0.45996621900532414, test loss: 0.4950302881664358\n",
      "Iteration 1278 training loss: 0.45636790948887257, test loss: 0.4935970123320033\n",
      "Iteration 1279 training loss: 0.46074196245473165, test loss: 0.49579892381779417\n",
      "Iteration 1280 training loss: 0.4565758719613738, test loss: 0.4938430374031315\n",
      "Iteration 1281 training loss: 0.4615637720887536, test loss: 0.49661724427925136\n",
      "Iteration 1282 training loss: 0.45692412379546166, test loss: 0.4942219405590065\n",
      "Iteration 1283 training loss: 0.4624358610008493, test loss: 0.49749115811683414\n",
      "Iteration 1284 training loss: 0.45744612067297036, test loss: 0.4947702207154794\n",
      "Iteration 1285 training loss: 0.4633428111525394, test loss: 0.4984066029301448\n",
      "Iteration 1286 training loss: 0.4581627607062959, test loss: 0.49550838851590295\n",
      "Iteration 1287 training loss: 0.4642323103069762, test loss: 0.49931448722927624\n",
      "Iteration 1288 training loss: 0.4591030900480871, test loss: 0.49644341120330554\n",
      "Iteration 1289 training loss: 0.4651072032204948, test loss: 0.500225046593884\n",
      "Iteration 1290 training loss: 0.460302838934966, test loss: 0.49762682358011845\n",
      "Iteration 1291 training loss: 0.4659891704140738, test loss: 0.5011520003923416\n",
      "Iteration 1292 training loss: 0.46172059435641766, test loss: 0.4990105507984521\n",
      "Iteration 1293 training loss: 0.46683844023868626, test loss: 0.5020565806831367\n",
      "Iteration 1294 training loss: 0.4632456538996419, test loss: 0.5004787338578534\n",
      "Iteration 1295 training loss: 0.46756231838174983, test loss: 0.50283704859318\n",
      "Iteration 1296 training loss: 0.4646737352090976, test loss: 0.5018313307975982\n",
      "Iteration 1297 training loss: 0.4679707375558698, test loss: 0.5033089960690398\n",
      "Iteration 1298 training loss: 0.4656409477276218, test loss: 0.5027102330263072\n",
      "Iteration 1299 training loss: 0.46782549051601086, test loss: 0.5032265017738472\n",
      "Iteration 1300 training loss: 0.4659109299227363, test loss: 0.5028859516389462\n",
      "Iteration 1301 training loss: 0.4670481963231526, test loss: 0.5025044850233371\n",
      "Iteration 1302 training loss: 0.4653195064388844, test loss: 0.5022002655799825\n",
      "Iteration 1303 training loss: 0.46561639909574265, test loss: 0.5011156993983729\n",
      "Iteration 1304 training loss: 0.4640172888093716, test loss: 0.5008156041067849\n",
      "Iteration 1305 training loss: 0.4637323621681685, test loss: 0.4992661520714054\n",
      "Iteration 1306 training loss: 0.46223859963767394, test loss: 0.49897367555363936\n",
      "Iteration 1307 training loss: 0.4616629888273521, test loss: 0.4972269901145046\n",
      "Iteration 1308 training loss: 0.46022989584851015, test loss: 0.49691590390303\n",
      "Iteration 1309 training loss: 0.45954202086611295, test loss: 0.4951297860413096\n",
      "Iteration 1310 training loss: 0.45822363624392864, test loss: 0.4948823834741658\n",
      "Iteration 1311 training loss: 0.45757001130357217, test loss: 0.49317998393225004\n",
      "Iteration 1312 training loss: 0.45634503687139283, test loss: 0.4929984503707943\n",
      "Iteration 1313 training loss: 0.4558137439779906, test loss: 0.49144507296081474\n",
      "Iteration 1314 training loss: 0.45471990575676935, test loss: 0.4913889704848947\n",
      "Iteration 1315 training loss: 0.45435522118645794, test loss: 0.4900114382219388\n",
      "Iteration 1316 training loss: 0.4533664362805278, test loss: 0.4900659767532505\n",
      "Iteration 1317 training loss: 0.45315664580896287, test loss: 0.48883418681595997\n",
      "Iteration 1318 training loss: 0.45222371531147454, test loss: 0.48896344404207887\n",
      "Iteration 1319 training loss: 0.45219336173849206, test loss: 0.4878887688015057\n",
      "Iteration 1320 training loss: 0.45130349110535967, test loss: 0.48809526578259654\n",
      "Iteration 1321 training loss: 0.45145723277414956, test loss: 0.487166242275027\n",
      "Iteration 1322 training loss: 0.4505563227336116, test loss: 0.48740635214127703\n",
      "Iteration 1323 training loss: 0.4509308392566713, test loss: 0.486644926830318\n",
      "Iteration 1324 training loss: 0.4499946481529709, test loss: 0.4869122746595656\n",
      "Iteration 1325 training loss: 0.45061225538392347, test loss: 0.4863260260292355\n",
      "Iteration 1326 training loss: 0.4495943563389972, test loss: 0.48658669801888826\n",
      "Iteration 1327 training loss: 0.4505125480024024, test loss: 0.48621912991108746\n",
      "Iteration 1328 training loss: 0.4493484766382363, test loss: 0.4864241546372191\n",
      "Iteration 1329 training loss: 0.45063703145097633, test loss: 0.48632795206785423\n",
      "Iteration 1330 training loss: 0.4492495548503938, test loss: 0.4864149082189391\n",
      "Iteration 1331 training loss: 0.45099851493302123, test loss: 0.48666661880934664\n",
      "Iteration 1332 training loss: 0.4493024418923128, test loss: 0.48656404599001774\n",
      "Iteration 1333 training loss: 0.4516123201309829, test loss: 0.4872541195548869\n",
      "Iteration 1334 training loss: 0.4494915288756602, test loss: 0.4868522715248033\n",
      "Iteration 1335 training loss: 0.4524619158542354, test loss: 0.4880769113071274\n",
      "Iteration 1336 training loss: 0.4498273531880573, test loss: 0.4872890496899511\n",
      "Iteration 1337 training loss: 0.453521517641071, test loss: 0.489111333749586\n",
      "Iteration 1338 training loss: 0.4503337919846174, test loss: 0.4878909825744897\n",
      "Iteration 1339 training loss: 0.4547861111802009, test loss: 0.490353671760608\n",
      "Iteration 1340 training loss: 0.45105557285718995, test loss: 0.4886997548403717\n",
      "Iteration 1341 training loss: 0.45624698321863133, test loss: 0.4917958417591838\n",
      "Iteration 1342 training loss: 0.45202744663644845, test loss: 0.4897475674877587\n",
      "Iteration 1343 training loss: 0.45776950643966124, test loss: 0.49331040418902294\n",
      "Iteration 1344 training loss: 0.4533093456265524, test loss: 0.491092572625558\n",
      "Iteration 1345 training loss: 0.4593284685648494, test loss: 0.4948737429208423\n",
      "Iteration 1346 training loss: 0.45484953032845715, test loss: 0.49268280586923663\n",
      "Iteration 1347 training loss: 0.4606708650258122, test loss: 0.49622903184540046\n",
      "Iteration 1348 training loss: 0.45656079679777894, test loss: 0.4944206433844967\n",
      "Iteration 1349 training loss: 0.4616945705592715, test loss: 0.49728202622626605\n",
      "Iteration 1350 training loss: 0.458221676912613, test loss: 0.49608308307847565\n",
      "Iteration 1351 training loss: 0.46216238197697107, test loss: 0.49778214606135773\n",
      "Iteration 1352 training loss: 0.45944611227834703, test loss: 0.49729503357205757\n",
      "Iteration 1353 training loss: 0.46193811376229443, test loss: 0.4975870029161016\n",
      "Iteration 1354 training loss: 0.4598820369442442, test loss: 0.4976987030223922\n",
      "Iteration 1355 training loss: 0.46098998933013063, test loss: 0.4966615195232227\n",
      "Iteration 1356 training loss: 0.45940912827558106, test loss: 0.49717228385667234\n",
      "Iteration 1357 training loss: 0.45947217725878, test loss: 0.49516709599184744\n",
      "Iteration 1358 training loss: 0.45815741046912867, test loss: 0.4958579461629765\n",
      "Iteration 1359 training loss: 0.457649897194899, test loss: 0.4933687637742017\n",
      "Iteration 1360 training loss: 0.4564511740948436, test loss: 0.4940947060934653\n",
      "Iteration 1361 training loss: 0.4557576680330609, test loss: 0.4915039174657991\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (≈ 10 lines of code)\n",
    "layer_sizes = (features_train.shape[1], 128, 10)\n",
    "params = init_params(layer_sizes)\n",
    "num_iters = 2000\n",
    "learning_rate = 2e-6\n",
    "losses_train, losses_test = [], []\n",
    "# Optimization loop\n",
    "for i in range(num_iters):\n",
    "    preds_train, cache = forward(features_train, params)\n",
    "    preds_test, _ = forward(features_test, params)\n",
    "    loss_train = ce_loss(preds_train, labels_train)\n",
    "    loss_test = ce_loss(preds_test, labels_test)\n",
    "    print(f\"Iteration {i+1} training loss: {loss_train}, test loss: {loss_test}\")\n",
    "    losses_train.append(loss_train)\n",
    "    losses_test.append(loss_test)\n",
    "    grads = grad(preds_train, labels_train, params, cache)\n",
    "    for j in range(len(layer_sizes) -1):\n",
    "        params['W'+str(j+1)] = params['W'+str(j+1)] - learning_rate * grads['dW'+str(j+1)]\n",
    "        params['b'+str(j+1)] = params['b'+str(j+1)] - learning_rate * grads['db'+str(j+1)]\n",
    "### END CODE HERE ###\n",
    "\n",
    "plt.plot(range(num_iters), losses_train, 'b--', range(num_iters), losses_test, 'r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Choose the activation function wisely. A poorly activated layer may cause longer training duration as well as poorer model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "Besides cross entropy loss, accuracy is usually used to evaluate the performance of a model. We can predict an image's class based on the index of the maximum value in the prediction.\n",
    "### $\\color{violet}{\\textbf{(10\\%) Exercise 7: Accuracy Evaluation}}$\n",
    "- **Bring test accuracy up to be higher than 80%**\n",
    "- You'll find the index of the maximum value in an array using `np.argmax()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### START CODE HERE ### (≈ 8 lines of code)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Evaluate with train\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m pred_classes_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mpreds_train\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Get indices of max values in Yhat_train row-wise\u001b[39;00m\n\u001b[1;32m      4\u001b[0m is_correct_train \u001b[38;5;241m=\u001b[39m pred_classes_train \u001b[38;5;241m==\u001b[39m labels_train  \u001b[38;5;66;03m# Find out which predictions are correct\u001b[39;00m\n\u001b[1;32m      5\u001b[0m num_correct_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(is_correct_train)  \u001b[38;5;66;03m# Calculate how many correct predictions are made\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds_train' is not defined"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (≈ 8 lines of code)\n",
    "# Evaluate with train\n",
    "pred_classes_train = np.argmax(preds_train, axis=1)  # Get indices of max values in Yhat_train row-wise\n",
    "is_correct_train = pred_classes_train == labels_train  # Find out which predictions are correct\n",
    "num_correct_train = np.sum(is_correct_train)  # Calculate how many correct predictions are made\n",
    "accuracy_train = num_correct_train / labels_train.shape[0]  # Calculate accuracy rate: correct # / total #\n",
    "# Evaluate with test\n",
    "pred_classes_test = preds_test.argmax(axis=1)  # # Get indices of max values in Yhat_test row-wise\n",
    "is_correct_test = pred_classes_test == labels_test  # Find out which predictions are correct\n",
    "num_correct_test = is_correct_test.sum()  # Calculate how many correct predictions are made\n",
    "accuracy_test = num_correct_test / labels_test.shape[0]  # Calculate accuracy rate: correct # / total #\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(f\"prediction accuracy on train set: {accuracy_train}\")\n",
    "print(f\"prediction accuracy on test set: {accuracy_test}\")\n",
    "\n",
    "# Compare prediction and target\n",
    "figure = plt.figure(figsize=(10, 10))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = np.random.randint(0, len(test_set))\n",
    "    img, label_true = test_set[sample_idx]\n",
    "    label_pred = pred_classes_test[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(f\"true: {category_keys[label_true]}, pred: {category_keys[label_pred]}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Congrats on finishing this assignment! "
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "3321",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
