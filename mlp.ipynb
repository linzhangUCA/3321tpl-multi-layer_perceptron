{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron Neural Network\n",
    "\n",
    "**Due: Wednesday, 11/22/2023, 2:15 PM**\n",
    "\n",
    "Welcome to your fourth assignment. You will build a multi-layer perceptron neural network in this assignment. The goal of building such a  is to classify hand-written digits.\n",
    "\n",
    "## Exercises:\n",
    "1. $\\color{violet}{\\textbf{(20\\%) Data Preprocessing}}$\n",
    "2. $\\color{violet}{\\textbf{(5\\%) Logistic Regression Model}}$\n",
    "3. $\\color{violet}{\\textbf{(5\\%) Cross Entropy Loss}}$\n",
    "4. $\\color{violet}{\\textbf{(40\\%) Gradient Descent Optimization}}$\n",
    "5. $\\color{violet}{\\textbf{(15\\%) Evaluation on Test Dataset}}$\n",
    "6. $\\color{violet}{\\textbf{(15\\%) Test Model with New Image}}$\n",
    "\n",
    "## Instructions:\n",
    "- Write your code only between the $\\color{green}{\\textbf{\\small \\#\\#\\# START CODE HERE \\#\\#\\#}}$ and $\\color{green}{\\textbf{\\small \\#\\#\\# END CODE HERE \\#\\#\\#}}$ commented lines. $\\color{red}{\\textbf{Do not modify code out of the designated area.}}$\n",
    "- Reference answers are provided after a certain coding blocks. Be aware if your answer is different from the reference..\n",
    "- **Need to install [Torchvision](https://pytorch.org/vision/stable/index.html)**\n",
    "    ```console\n",
    "    pip install torchvision\n",
    "    ```\n",
    "**You will learn:**\n",
    "- Usage of Rectified Linear Unit (ReLU) activation function.\n",
    "- Generalize number and dimension of the hidden layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "**NOTE: math representations of forward and backward propogation has been updated. Please use the equations in the [updated slides](https://linzhanguca.github.io/_docs/applied_deep_learning-2022/0921/nn_p2.pdf) or the follows if you prefer no transopose in the forward propagation.**\n",
    "\n",
    "To build your neural network, you will complete several \"helper functions\". These helper functions will be used to realize the forward and backward propagation when training a K-layer neural network. Each small helper function you will implement will have detailed instructions that will walk you through the necessary steps. Here is an outline of this assignment, you will:\n",
    "\n",
    "- Initialize the parameters for K-layer neural network.\n",
    "- Implement the forward propagation. \n",
    "     - Compute linear transformation $\\mathbf{Z}^{[k]} = \\mathbf{X}^{[k-1]} \\cdot \\mathbf{W}^{[k]} + \\mathbf{b}^{[k]}$.\n",
    "     - Compute activation: $X^{[k]} = g(\\mathbf{Z}^{[k]})$.\n",
    "     - Stack the \"linear transfortmation\" and \"activation\" to compute predictions in the final layer.\n",
    "- Compute the cross entropy loss: \n",
    "    $$\\mathcal{L(\\hat{\\mathbf{y}}, \\mathbf{y}) = \\frac{1}{M}\\sum_{i=1}^M (-\\mathbf{y}log(\\hat{\\mathbf{y}}) - (1 - \\mathbf{y})log(1 - \\hat{\\mathbf{y}}))}$$\n",
    "- Compute gradients of the parameters for backward propagation.\n",
    "    $$d\\mathbf{Z}^{[k]} = \\frac{\\partial{J}}{\\partial{\\mathbf{Z}^{[k]}}} = d\\mathbf{X}^{[k]} * g'^{[k]}(\\mathbf{Z}^{[k]})$$\n",
    "    $$d\\mathbf{W}^{[k]} = \\frac{\\partial{J}}{\\partial{\\mathbf{W}^{[k]}}} = \\frac{1}{M}\\mathbf{X}^{[k-1]\\mathbf{T}} \\cdot d\\mathbf{Z}^{[k]}$$\n",
    "    $$d\\mathbf{b}^{[k]} = \\frac{\\partial{J}}{\\partial{\\mathbf{b}^{[k]}}} = \\frac{1}{M} \\Sigma d\\mathbf{Z}^{[k]}$$\n",
    "    $$d\\mathbf{X}^{[k-1]} = \\frac{\\partial{J}}{\\partial{\\mathbf{X}^{[k-1]}}} = d\\mathbf{Z}^{[k]} \\cdot \\mathbf{W}^{[k]\\mathbf{T}}$$\n",
    "- Update the parameters using gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset\n",
    "Torchvision provides a descent pool of datasets. We'll load one of the built-in dataset, [FashionMNIST](https://pytorch.org/vision/stable/datasets.html) to investigate multi-class classification using a generic Multi-Layer Perceptron (MLP) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training feature array shape: (60000, 28, 28), test feature array shape: (10000, 28, 28)\n",
      "training target array shape: (60000,), test target array shape: (10000,)\n",
      "{'T-shirt/top': 0, 'Trouser': 1, 'Pullover': 2, 'Dress': 3, 'Coat': 4, 'Sandal': 5, 'Shirt': 6, 'Sneaker': 7, 'Bag': 8, 'Ankle boot': 9}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "train_set = datasets.FashionMNIST('./data', train=True, download=True)\n",
    "test_set = datasets.FashionMNIST('./data', train=False, download=True)\n",
    "\n",
    "# Extract important arrays\n",
    "train_feature_array = train_set.data.numpy()\n",
    "train_target_array = train_set.targets.numpy()\n",
    "test_feature_array = test_set.data.numpy()\n",
    "test_target_array = test_set.targets.numpy()\n",
    "category_keys = train_set.classes\n",
    "category_vals = range(len(category_keys))\n",
    "category_dict = dict((map(lambda i,j : (i,j), category_keys, category_vals)))\n",
    "\n",
    "print(f\"training feature array shape: {train_feature_array.shape}, test feature array shape: {test_feature_array.shape}\")\n",
    "print(f\"training target array shape: {train_target_array.shape}, test target array shape: {test_target_array.shape}\")\n",
    "print(category_dict)\n",
    "\n",
    "# Visualize\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 4, 4\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = np.random.randint(0, len(train_set))\n",
    "    img, label = train_set[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(category_keys[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img, cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess the Data\n",
    "A $\\mathcal{C}$-calss dataset with   is made up with a feature matrix: $\\mathbf{X} = [^{(1)}\\mathbf{x}, ^{(2)}\\mathbf{x}, ..., ^{(M)}\\mathbf{x}]^T$. and a one-hot encoded target matrix $\\mathbf{Y} = [^{(1)}\\mathbf{y}, ^{(2)}\\mathbf{y}, ..., ^{(M)}\\mathbf{y}]^T$. Where $M$ is the total number of instances in the dataset, $^{(m)}\\mathbf{x} = [^{(m)}x_1, ..., ^{(m)}x_n]$ is a normalized and flattened row vector bears $n$ feature values, and $^{(m)}\\mathbf{y} = [0, ..., 0, 1, 0, ..., 0]$ is a one-hot encoded row vector.\n",
    "\n",
    "- A grey-scale image can be represented by a **2-dimensional array with shape $(width, height)$**. Where, $width$ indicates number of pixels on horizontal direction, $height$ indicates number of pixels on vertical direction.\n",
    "- We can use an **integer ranged 0~255** to describe a pixel's color intensity. However, it is easier for your computer to handlle float values.\n",
    "- We would like to convert an image array into a row vector, or a **2d array with shape $(1, width*height)$**. So that, we can stack these row vectors vertically to form a feature matrix.\n",
    "- We also would like to encode target array into one-hot format.\n",
    "\n",
    "\n",
    "We will access the raw data by extracting it from the dataloaders. Then, process and prepare the raw data so that it can be used in later steps.\n",
    "\n",
    "### $\\color{violet}{\\textbf{(20\\%) Exercise 1: Data Preprocessing}}$\n",
    "1. Reshape feature array.\n",
    "2. One-hot encode target array\n",
    "3. Rescale feature arrary, represent each pixel with a float numbers in range 0~1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training feature shape: (60000, 784), test feature shape: (10000, 784), training target shape: (60000, 10), test target shape: (10000, 10)\n",
      "sample feature portion: \n",
      "[0.99607843 0.79215686 0.82745098 0.79215686 0.79607843 0.79215686\n",
      " 0.83921569 0.84313725 0.83921569 0.84705882 0.82352941 0.92941176\n",
      " 0.57254902 0.         0.00392157 0.         0.         0.\n",
      " 0.         0.         0.00784314 0.01568627 0.01176471 0.\n",
      " 0.         0.         0.38039216 0.98823529 0.80784314 0.83529412\n",
      " 0.80392157 0.81176471 0.8        0.80784314 0.83529412 0.83921569\n",
      " 0.84313725 0.84705882 0.85490196 0.90980392 0.77254902 0.\n",
      " 0.         0.00784314 0.01960784 0.02745098 0.00784314 0.00392157\n",
      " 0.         0.        ]\n",
      "sample target: \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### ( 10 lines of code)\n",
    "M_train = train_target_array.shape[0]  # number of sampels in training data\n",
    "M_test = test_target_array.shape[0]  # number of samples in test data\n",
    "# Reshape feature and target arrays\n",
    "flatten_feature_train = train_feature_array.reshape(M_train, -1)  # (60000, 28, 28) -> (60000, 1)\n",
    "flatten_feature_test = test_feature_array.reshape(M_test, -1)  # (10000, 28, 28) -> (10000, 1)\n",
    "# One hot encode targets\n",
    "onehot_target_train = np.zeros((M_train, len(category_dict)))\n",
    "onehot_target_train[np.arange(M_train), train_target_array] = 1\n",
    "onehot_target_test = np.zeros((M_test, len(category_dict)))\n",
    "onehot_target_test[np.arange(M_test), test_target_array] = 1\n",
    "# print(np.sum(onehot_target_test, axis=0))\n",
    "# Rescale features\n",
    "rescale_feature_train = flatten_feature_train / 255\n",
    "rescale_feature_test = flatten_feature_test / 255\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Rename\n",
    "feature_train = rescale_feature_train\n",
    "feature_test = rescale_feature_test\n",
    "target_train = onehot_target_train\n",
    "target_test = onehot_target_test\n",
    "print(f\"training feature shape: {feature_train.shape}, test feature shape: {feature_test.shape}, training target shape: {target_train.shape}, test target shape: {target_test.shape}\")\n",
    "print(f\"sample feature portion: \\n{feature_train[3321][350:400]}\")\n",
    "print(f\"sample target: \\n{target_train[3321]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "training feature shape: (60000, 784), test feature shape: (10000, 784), training target shape: (60000, 10), test target shape: (10000, 10)\n",
    "sample feature portion: \n",
    "[0.99607843 0.79215686 0.82745098 0.79215686 0.79607843 0.79215686\n",
    " 0.83921569 0.84313725 0.83921569 0.84705882 0.82352941 0.92941176\n",
    " 0.57254902 0.         0.00392157 0.         0.         0.\n",
    " 0.         0.         0.00784314 0.01568627 0.01176471 0.\n",
    " 0.         0.         0.38039216 0.98823529 0.80784314 0.83529412\n",
    " 0.80392157 0.81176471 0.8        0.80784314 0.83529412 0.83921569\n",
    " 0.84313725 0.84705882 0.85490196 0.90980392 0.77254902 0.\n",
    " 0.         0.00784314 0.01960784 0.02745098 0.00784314 0.00392157\n",
    " 0.         0.        ]\n",
    "sample target: \n",
    "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forward Pass\n",
    "A Multi-Layer Perceptron (MLP) model is featured with multiple layers of transformed features. Any two adjacent layer are connected by a linear model and an activation function. The linear model is governed by a set of weight parameters and a set of bias parameters. The general structure of an MLP model is shown below.\n",
    "![](./model.png)\n",
    "\n",
    "### 2.1. Initialize Parameters\n",
    "A linear model governed by weights $\\mathbf{W}^{[l]}$ and $\\mathbf{b}^{[l]}$ transforms features, $\\mathbf{X}^{[l-1]}$ in the $l-1$-th layer to $\\mathbf{Z}^{[l]}$ in the $l$-th layer.\n",
    "\n",
    "$\\mathbf{Z}^{[l]} = \\mathbf{X}^{[l-1]} \\cdot \\mathbf{W}^{[l]T} + \\mathbf{b}^{[l]}$\n",
    "\n",
    "Assume $\\mathbf{X}^{[l-1]}$ has $N_{l-1}$ features and $\\mathbf{X}^{[l]}$ has $N_{l}$ features, then $\\mathbf{W}^{[l]}$ is with shape $(N_l, N_{l-1})$, $\\mathbf{b}^{[l]}$ is with shape $(1, N_l)$\n",
    "\n",
    "### $\\color{violet}{\\textbf{(10\\%) Exercise 2: Parameter Initialization}}$\n",
    "Define a function to initialize weights and biases parameters and save these parameters in a **dictionary**. \n",
    "- Input sizes of all the layers (**include the input layer**) using a **list**.\n",
    "- Use a **`for` loop** to randomly initialize $\\mathbf{W}^{[l]}$ and $\\mathbf{b}^{[l]}$ for the $l$-th layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['W1', 'b1', 'W2', 'b2', 'W3', 'b3'])\n",
      "{'W1': array([[ 3.68455981e-05, -1.38782645e-04, -7.23142013e-05,\n",
      "         9.20641119e-05, -1.95355264e-05,  3.87255258e-05],\n",
      "       [-6.00629078e-05,  3.25435259e-06,  6.46665063e-05,\n",
      "        -7.84748031e-05,  1.41884652e-04, -6.15479349e-05],\n",
      "       [-1.10127440e-04, -6.82385574e-05,  7.96622349e-05,\n",
      "         5.25934678e-05, -1.78411966e-04,  1.28629057e-04],\n",
      "       [ 5.66879246e-05,  8.95999019e-05, -1.62874713e-05,\n",
      "         3.51109174e-05, -1.03338181e-04, -8.88623865e-05],\n",
      "       [ 5.10860443e-05,  1.19661547e-04,  8.64757553e-05,\n",
      "        -1.41101724e-04,  1.38003186e-04, -1.06812125e-04]]), 'b1': array([[ 6.38004843e-05, -6.19666755e-05,  1.12931166e-04,\n",
      "        -3.12290798e-05,  5.59708416e-05]]), 'W2': array([[ 1.02685210e-04, -1.28307992e-04, -4.03018811e-05,\n",
      "        -4.09270011e-05, -1.84834135e-05],\n",
      "       [ 1.71466862e-04, -9.69611346e-05,  4.40873689e-05,\n",
      "        -1.10516782e-04, -5.17959570e-05],\n",
      "       [-1.87055902e-04, -6.31341925e-05, -5.47992268e-05,\n",
      "         3.67804566e-05, -1.05795654e-04],\n",
      "       [ 1.24964681e-04,  3.60854003e-05,  1.30113815e-04,\n",
      "         1.01177937e-04,  2.22286230e-05]]), 'b2': array([[ 1.16709537e-04, -1.12496462e-04, -9.47925077e-05,\n",
      "        -4.49789001e-05]]), 'W3': array([[ 1.01049508e-04, -3.52965408e-05,  2.06672986e-04,\n",
      "        -1.67359842e-05],\n",
      "       [-7.34788781e-05,  1.28189925e-05,  3.67334763e-05,\n",
      "        -3.00482492e-05],\n",
      "       [ 9.36343566e-05, -3.80351405e-05,  6.59823461e-06,\n",
      "         5.61206851e-05]]), 'b3': array([[ 2.57409859e-04,  1.69968799e-05, -7.22321090e-05]])}\n"
     ]
    }
   ],
   "source": [
    "def init_params(layer_sizes):\n",
    "    \"\"\" Parameter initialization function\n",
    "    Args:\n",
    "        layer_sizes -- list/tuple, (input size, ..., hidden layer size, ..., output size)\n",
    "    Returns:\n",
    "        parameters -- dictionary, contains parameters: Wi and bi, i is the i-th layer\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    ### START CODE HERE ### ( 2 lines of code)\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        parameters['W'+str(i+1)] = np.random.normal(0, 0.0001, size=(layer_sizes[i+1], layer_sizes[i]))\n",
    "        parameters['b'+str(i+1)] = np.random.normal(0, 0.0001, size=(1, layer_sizes[i+1]))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return parameters\n",
    "\n",
    "# Sanity check\n",
    "np.random.seed(3321)\n",
    "dummy_layer_sizes = (6, 5, 4, 3)  # (input size, layer1 size, layer2 size, output size)\n",
    "dummy_params = init_params(dummy_layer_sizes)\n",
    "print(dummy_params.keys())\n",
    "print(dummy_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "dict_keys(['W1', 'b1', 'W2', 'b2', 'W3', 'b3'])\n",
    "{'W1': array([[ 3.68455981e-05, -1.38782645e-04, -7.23142013e-05,\n",
    "         9.20641119e-05, -1.95355264e-05,  3.87255258e-05],\n",
    "       [-6.00629078e-05,  3.25435259e-06,  6.46665063e-05,\n",
    "        -7.84748031e-05,  1.41884652e-04, -6.15479349e-05],\n",
    "       [-1.10127440e-04, -6.82385574e-05,  7.96622349e-05,\n",
    "         5.25934678e-05, -1.78411966e-04,  1.28629057e-04],\n",
    "       [ 5.66879246e-05,  8.95999019e-05, -1.62874713e-05,\n",
    "         3.51109174e-05, -1.03338181e-04, -8.88623865e-05],\n",
    "       [ 5.10860443e-05,  1.19661547e-04,  8.64757553e-05,\n",
    "        -1.41101724e-04,  1.38003186e-04, -1.06812125e-04]]), 'b1': array([[ 6.38004843e-05, -6.19666755e-05,  1.12931166e-04,\n",
    "        -3.12290798e-05,  5.59708416e-05]]), 'W2': array([[ 1.02685210e-04, -1.28307992e-04, -4.03018811e-05,\n",
    "        -4.09270011e-05, -1.84834135e-05],\n",
    "       [ 1.71466862e-04, -9.69611346e-05,  4.40873689e-05,\n",
    "        -1.10516782e-04, -5.17959570e-05],\n",
    "       [-1.87055902e-04, -6.31341925e-05, -5.47992268e-05,\n",
    "         3.67804566e-05, -1.05795654e-04],\n",
    "       [ 1.24964681e-04,  3.60854003e-05,  1.30113815e-04,\n",
    "         1.01177937e-04,  2.22286230e-05]]), 'b2': array([[ 1.16709537e-04, -1.12496462e-04, -9.47925077e-05,\n",
    "        -4.49789001e-05]]), 'W3': array([[ 1.01049508e-04, -3.52965408e-05,  2.06672986e-04,\n",
    "        -1.67359842e-05],\n",
    "       [-7.34788781e-05,  1.28189925e-05,  3.67334763e-05,\n",
    "        -3.00482492e-05],\n",
    "       [ 9.36343566e-05, -3.80351405e-05,  6.59823461e-06,\n",
    "         5.61206851e-05]]), 'b3': array([[ 2.57409859e-04,  1.69968799e-05, -7.22321090e-05]])}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Forward Propagation\n",
    "A linear model transforms features, $\\mathbf{X}^{[l-1]}$ in the $l-1$-th layer to $\\mathbf{Z}^{[l]}$ in the $l$-th layer. Then we apply a Linear Rectified Unit (ReLU) function on $\\mathbf{Z}^{[l]}$ to form new features $\\mathbf{Z}^{[l]}$ in the $l$-th layer.\n",
    "\n",
    "$$\\mathbf{Z}^{[l]} = \\mathbf{X}^{[l-1]} \\cdot \\mathbf{W}^{[l]T} + \\mathbf{b}^{[l]}$$\n",
    "\n",
    "$$\\mathbf{X}^{[l]} = ReLU(\\mathbf{Z}^{[l]}) = \n",
    "    \\begin{cases}\n",
    "        0   & z \\leq 0 \\\\\n",
    "        z   & z > 0\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "The last layer needs to be activcated by a softmax function.\n",
    "\n",
    "#### $$\\hat{y}_i = \\frac{e^{z^{[L]}_i}}{\\sum^C_{i=1} e^{z^{[L]}_i}}$$\n",
    "\n",
    "The maxtrix $\\mathbf{Z}^{[L]}$ has shape: $(M, C)$, where $M$ is the number of samples, $C$ is the number of classes. When applying softmax activation, we only want to apply it on the 2nd dimension (1st axis in numpy). So that each row in $\\mathbf{Z}^{[L]}$ will be converted to probabilities.\n",
    "### $\\color{violet}{\\textbf{(40\\%) Exercise 3: Linear Model and Activation}}$\n",
    "- Define ReLU activation function and softmax activation function.\n",
    "- Define linear model.\n",
    "- Define forward propagation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]]\n",
      "cache dictionary keys: dict_keys(['X0', 'Z1', 'X1', 'Z2', 'X2', 'Z3'])\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### ( 6 lines of code)\n",
    "def relu(x):\n",
    "    \"\"\" Rectified linear unit function\n",
    "    Args:\n",
    "        x -- scalar/array\n",
    "    Returns:\n",
    "        y -- scalar/array, 0 if x <= 0, x if x >0\n",
    "    \"\"\"\n",
    "    y = np.maximum(0, x)\n",
    "\n",
    "    return y\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\" Softmax function\n",
    "    Args:\n",
    "        x -- array\n",
    "    Returns:\n",
    "        prob -- array\n",
    "    \"\"\"\n",
    "    prob = np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
    "    \n",
    "    return prob\n",
    "\n",
    "def linear(feature, weight, bias):\n",
    "    \"\"\" Linear model\n",
    "    Args:\n",
    "        feature (matrix): 2d array with shape (M, N^[l-1])\n",
    "        weight (matrix): 2d array with shape (N^[l], N^[l-1])\n",
    "        bias (row vector): 2d array with shape (1, N^[l])\n",
    "    Returns:\n",
    "        Z (matrix): 2d array with shape (M, , N^[l])\n",
    "    \"\"\"\n",
    "    Z = np.dot(feature, weight.T) + bias\n",
    "        \n",
    "    return Z\n",
    "\n",
    "def forward(input_feature, params):\n",
    "    \"\"\" Forward propagation process\n",
    "    Args:\n",
    "        input_feature (matrix): 2d array with shape (M, N^[0])\n",
    "        params: dictionary, stores W's and b's\n",
    "    Returns:\n",
    "        prediction: 2d array with shape (M, C)\n",
    "        cache: dictionary, stores intemediate X's and Z's.\n",
    "    \"\"\"\n",
    "    cache = {'X0': input_feature}\n",
    "    for i in range(int(len(params) / 2) - 1):\n",
    "        cache['Z' + str(i+1)] = linear(cache['X' + str(i)], params['W' + str(i+1)], params['b' + str(i+1)])\n",
    "        cache['X' + str(i+1)] = relu(cache['Z' + str(i+1)])\n",
    "    cache['Z' + str(i+2)] = linear(cache['X' + str(i+1)], params['W' + str(i+2)], params['b' + str(i+2)])\n",
    "    prediction = softmax(cache['Z' + str(i+2)])\n",
    "\n",
    "    return prediction, cache\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Sanity check\n",
    "np.random.seed(3321)\n",
    "dummy_input = np.random.randn(8,6)\n",
    "dummy_pred, dummy_cache = forward(dummy_input, dummy_params)\n",
    "print(dummy_pred)\n",
    "print(f\"cache dictionary keys: {dummy_cache.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "[[0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]]\n",
    "cache dictionary keys: dict_keys(['X0', 'Z1', 'X1', 'Z2', 'X2', 'Z3'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Class Cross Entropy Loss\n",
    "For multi-class classification problem, it is quite standard to use a general form of cross entropy function to evaluate model prediction vs. target. \n",
    "#### $\\mathcal{L}(\\mathbf{\\hat{Y}}, \\mathbf{Y}) = -\\frac{1}{M} \\sum_{m=1}^M \\sum_{c=1}^C {^{(m)} y_c} \\log {^{(m)} \\hat{y}_c}$\n",
    "\n",
    "### $\\color{violet}{\\textbf{(5\\%) Exercise 4: Cross Entropy Loss}}$\n",
    "Define a cross entropy function to compute the average loss between prediction matrix and target matrix. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.098605997066486\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### ( 1 lines of code)\n",
    "def cross_entropy_loss_fn(prediction, target):\n",
    "    \"\"\" Cross entropy loss function\n",
    "    Args:\n",
    "        prediction (matrix): 2d array with shape (M, C)\n",
    "        target (matrix): 2d array with shape (M, C)\n",
    "    Returns:\n",
    "        loss_value: scalar, averaged ce loss\n",
    "    \"\"\"\n",
    "    loss_value = -np.sum(target * np.log(prediction + 1e-10), axis=1)\n",
    "\n",
    "    return loss_value.mean()\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Sanity check\n",
    "dummy_target = np.zeros((8, 3))\n",
    "dummy_target[np.arange(8), np.random.randint(0, 3, (8,))] = 1\n",
    "dummy_loss = cross_entropy_loss_fn(dummy_pred, dummy_target)\n",
    "print(dummy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Back-Propagation\n",
    "In order to know how to update weights and biases, we need to compute the gradient of the loss. This requires compute gradient of loss w.r.t. the variables in the last layer first. Then compute gradient of loss w.r.t. the variables in the previous layer next. And so on, until the gradient of loss w.r.t. the first layer is computed. \n",
    "\n",
    "Due to the fact that the last layer is softmax activated, $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}^{[L]}}$ can be computed differently without explicitly solve for derivative of softmax function.\n",
    "$$d\\mathbf{Z}^{[L]} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}^{[L]}} = \\mathbf{\\hat{Y}} - \\mathbf{Y} $$\n",
    "\n",
    "Then, from last layer $L$ to first layer, we need to repeatedly computing the gradient of loss according to the chain rule. The computation of a general layer $[l]$ is as follows.\n",
    "$$d\\mathbf{W}^{[l]} = d\\mathbf{Z}^{[l]} \\frac{\\partial \\mathbf{Z}^{[l]}}{\\partial{\\mathbf{W}^{[l]}}} = d\\mathbf{Z}^{[l]T} \\cdot \\mathbf{X}^{[l-1]}$$\n",
    "$$d\\mathbf{b}^{[l]} = d\\mathbf{Z}^{[l]} \\frac{\\partial \\mathbf{Z}^{[l]}}{\\partial{\\mathbf{b}^{[l]}}} = mean(d\\mathbf{Z}^{[l]}, axis=0, keepdims=True)$$\n",
    "$$d\\mathbf{X}^{[l-1]} = d\\mathbf{Z}^{[l]} \\frac{\\partial \\mathbf{Z}^{[l]}}{\\partial{\\mathbf{X}^{[l-1]}}} = d\\mathbf{Z}^{[l]} \\cdot \\mathbf{W}^{[l]}$$\n",
    "$$d\\mathbf{Z}^{[l-1]} = d\\mathbf{X}^{[l-1]} * relu'(\\mathbf{Z}^{[l-1]})$$\n",
    "\n",
    "\n",
    "### $\\color{violet}{\\textbf{(30\\%) Exercise 5: Gradient Computation}}$\n",
    "- Define derivative of ReLU function.\n",
    "- Define a function to perform backward propagation to compute gradient of the (cross entropy) loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['dZ3', 'dW3', 'db3', 'dX2', 'dZ2', 'dW2', 'db2', 'dX1', 'dZ1', 'dW1', 'db1'])\n",
      "{'dZ3': array([[ 0.33339668, -0.66668347,  0.33328679],\n",
      "       [ 0.33339668,  0.33331653, -0.66671321],\n",
      "       [-0.66660332,  0.33331653,  0.33328679],\n",
      "       [ 0.33339668, -0.66668347,  0.33328679],\n",
      "       [ 0.33339668, -0.66668347,  0.33328679],\n",
      "       [-0.66660332,  0.33331653,  0.33328679],\n",
      "       [ 0.33339668,  0.33331653, -0.66671321],\n",
      "       [ 0.33339668,  0.33331653, -0.66671321]]), 'dW3': array([[ 7.78342782e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [-3.89266269e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [-3.89076513e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00]]), 'db3': array([[ 0.08339668, -0.04168347, -0.04171321]]), 'dX2': array([[ 1.13883818e-04, -3.29905699e-05,  4.66135899e-05,\n",
      "         3.31572328e-05],\n",
      "       [-5.32294163e-05,  1.78635631e-05,  7.67488316e-05,\n",
      "        -5.30117015e-05],\n",
      "       [-6.06445677e-05,  1.51249634e-05, -1.23325920e-04,\n",
      "         1.98449678e-05],\n",
      "       [ 1.13883818e-04, -3.29905699e-05,  4.66135899e-05,\n",
      "         3.31572328e-05],\n",
      "       [ 1.13883818e-04, -3.29905699e-05,  4.66135899e-05,\n",
      "         3.31572328e-05],\n",
      "       [-6.06445677e-05,  1.51249634e-05, -1.23325920e-04,\n",
      "         1.98449678e-05],\n",
      "       [-5.32294163e-05,  1.78635631e-05,  7.67488316e-05,\n",
      "        -5.30117015e-05],\n",
      "       [-5.32294163e-05,  1.78635631e-05,  7.67488316e-05,\n",
      "        -5.30117015e-05]]), 'dZ2': array([[ 1.13883818e-04, -0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [-5.32294163e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "        -0.00000000e+00],\n",
      "       [-6.06445677e-05,  0.00000000e+00, -0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [ 1.13883818e-04, -0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [ 1.13883818e-04, -0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [-6.06445677e-05,  0.00000000e+00, -0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [-5.32294163e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "        -0.00000000e+00],\n",
      "       [-5.32294163e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "        -0.00000000e+00]]), 'dW2': array([[ 2.56820452e-08,  4.87531652e-09, -3.06951575e-08,\n",
      "         3.21062592e-08,  4.70647012e-08],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00]]), 'db2': array([[7.58425885e-06, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]), 'dX1': array([[ 1.16941838e-08, -1.46122041e-08, -4.58973210e-09,\n",
      "        -4.66092316e-09, -2.10496171e-09],\n",
      "       [-5.46587377e-09,  6.82975954e-09,  2.14524561e-09,\n",
      "         2.17852038e-09,  9.83861313e-10],\n",
      "       [-6.22730015e-09,  7.78118273e-09,  2.44409016e-09,\n",
      "         2.48200029e-09,  1.12091862e-09],\n",
      "       [ 1.16941838e-08, -1.46122041e-08, -4.58973210e-09,\n",
      "        -4.66092316e-09, -2.10496171e-09],\n",
      "       [ 1.16941838e-08, -1.46122041e-08, -4.58973210e-09,\n",
      "        -4.66092316e-09, -2.10496171e-09],\n",
      "       [-6.22730015e-09,  7.78118273e-09,  2.44409016e-09,\n",
      "         2.48200029e-09,  1.12091862e-09],\n",
      "       [-5.46587377e-09,  6.82975954e-09,  2.14524561e-09,\n",
      "         2.17852038e-09,  9.83861313e-10],\n",
      "       [-5.46587377e-09,  6.82975954e-09,  2.14524561e-09,\n",
      "         2.17852038e-09,  9.83861313e-10]]), 'dZ1': array([[ 1.16941838e-08, -0.00000000e+00, -4.58973210e-09,\n",
      "        -0.00000000e+00, -0.00000000e+00],\n",
      "       [-0.00000000e+00,  6.82975954e-09,  0.00000000e+00,\n",
      "         0.00000000e+00,  9.83861313e-10],\n",
      "       [-6.22730015e-09,  0.00000000e+00,  2.44409016e-09,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00, -0.00000000e+00, -4.58973210e-09,\n",
      "        -4.66092316e-09, -2.10496171e-09],\n",
      "       [ 0.00000000e+00, -1.46122041e-08, -0.00000000e+00,\n",
      "        -0.00000000e+00, -2.10496171e-09],\n",
      "       [-6.22730015e-09,  7.78118273e-09,  2.44409016e-09,\n",
      "         0.00000000e+00,  1.12091862e-09],\n",
      "       [-5.46587377e-09,  6.82975954e-09,  0.00000000e+00,\n",
      "         0.00000000e+00,  9.83861313e-10],\n",
      "       [-5.46587377e-09,  0.00000000e+00,  2.14524561e-09,\n",
      "         0.00000000e+00,  9.83861313e-10]]), 'dW1': array([[ 1.17971035e-08,  1.22330914e-10, -1.53818049e-08,\n",
      "         2.06702414e-08, -5.81071002e-10, -1.58096973e-09],\n",
      "       [-1.53656443e-08, -2.48371868e-08, -2.22728238e-09,\n",
      "         1.15660671e-08,  5.59104117e-09,  1.27719412e-08],\n",
      "       [-4.47942835e-09, -3.29583356e-09,  7.66258527e-09,\n",
      "        -9.32763676e-09,  1.29261904e-09,  6.77909880e-09],\n",
      "       [-2.64218060e-09, -4.17618258e-09,  7.59146522e-10,\n",
      "        -1.63649288e-09,  4.81651321e-09,  4.14180755e-09],\n",
      "       [-2.97299911e-09, -6.55129754e-09, -4.87606610e-10,\n",
      "        -9.13291707e-10,  2.35949393e-09,  3.17123337e-09]]), 'db1': array([[-1.46152051e-09,  8.53562214e-10, -2.68254787e-10,\n",
      "        -5.82615394e-10, -1.71776069e-11]])}\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### ( 6 lines of code)\n",
    "def d_relu(x):\n",
    "    \"\"\" Rectified linear unit function\n",
    "    Args:\n",
    "        x -- scalar/array\n",
    "    Returns:\n",
    "        y -- scalar/array, 0 if x <= 0, x if x >0\n",
    "    \"\"\"\n",
    "    dydx = np.ones_like(x)\n",
    "    dydx[x < 0] = 0\n",
    "    return dydx\n",
    "\n",
    "def grad(prediction, target, params, cache):\n",
    "    \"\"\" Backward propogating gradient computation\n",
    "    Args:\n",
    "        prediction (matrix): 2d array with shape (M, C)\n",
    "        target (matrix): 2d array with shape (M, C)\n",
    "        params: dictionary, stores W's and b's.\n",
    "        cache: dictionary, stores intemediate X's and Z's.\n",
    "    Returns:\n",
    "        grads -- dictionary, stores dW's and db's\n",
    "    \"\"\"\n",
    "    num_layers = int(len(params) / 2)\n",
    "    grads = {'dZ' + str(num_layers): prediction - target}\n",
    "    for i in reversed(range(num_layers)):\n",
    "        grads['dW'+ str(i+1)] = np.dot(grads['dZ' + str(i+1)].T, cache['X' + str(i)])\n",
    "        grads['db' + str(i+1)] = np.mean(grads['dZ' + str(i+1)], axis=0, keepdims=True)\n",
    "        if not i:\n",
    "            break\n",
    "        grads['dX' + str(i)] = np.dot(grads['dZ' + str(i+1)], params['W' + str(i+1)])\n",
    "        grads['dZ' + str(i)] =  grads['dX' + str(i)] * d_relu(cache['Z' + str(i)])\n",
    "    return grads\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Sanity check\n",
    "dummy_grads = grad(dummy_pred, dummy_target, dummy_params, dummy_cache)\n",
    "print(dummy_grads.keys())\n",
    "print(dummy_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimization\n",
    "We have been able to compute the gradient of the cross entropy loss. Now, it's time to perform gradient descent optimization to bring the loss down.\n",
    "\n",
    "![](./gradient_dscent.png)\n",
    "\n",
    "\n",
    "### $\\color{violet}{\\textbf{(30\\%) Exercise 6: Gradient Descent Optimization}}$\n",
    "Train your model with a `for` loop. Bring both training loss and test loss down.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 training loss: 2.3025851693967514, test loss: 2.302585175105499\n",
      "Iteration 2 training loss: 2.3025849997037455, test loss: 2.3025850059359048\n",
      "Iteration 3 training loss: 2.302584828591234, test loss: 2.302584835230665\n",
      "Iteration 4 training loss: 2.302584655002608, test loss: 2.3025846619916694\n",
      "Iteration 5 training loss: 2.302584477015182, test loss: 2.302584484428183\n",
      "Iteration 6 training loss: 2.3025842920736235, test loss: 2.302584299861787\n",
      "Iteration 7 training loss: 2.302584097355924, test loss: 2.3025841055254923\n",
      "Iteration 8 training loss: 2.3025838901102067, test loss: 2.302583898736851\n",
      "Iteration 9 training loss: 2.3025836677015192, test loss: 2.3025836769213464\n",
      "Iteration 10 training loss: 2.3025834277149646, test loss: 2.302583437728197\n",
      "Iteration 11 training loss: 2.302583167507992, test loss: 2.3025831785847455\n",
      "Iteration 12 training loss: 2.3025828839330864, test loss: 2.3025828963025146\n",
      "Iteration 13 training loss: 2.3025825733535963, test loss: 2.3025825870597947\n",
      "Iteration 14 training loss: 2.302582231666901, test loss: 2.3025822467331807\n",
      "Iteration 15 training loss: 2.3025818542432317, test loss: 2.3025818707300125\n",
      "Iteration 16 training loss: 2.3025814359938006, test loss: 2.3025814539170235\n",
      "Iteration 17 training loss: 2.3025809712106553, test loss: 2.302580990687211\n",
      "Iteration 18 training loss: 2.302580453487035, test loss: 2.3025804746488547\n",
      "Iteration 19 training loss: 2.302579875823554, test loss: 2.3025798987707318\n",
      "Iteration 20 training loss: 2.30257923036446, test loss: 2.3025792551609827\n",
      "Iteration 21 training loss: 2.30257850851754, test loss: 2.302578535127879\n",
      "Iteration 22 training loss: 2.3025777007035404, test loss: 2.302577729179802\n",
      "Iteration 23 training loss: 2.302576795900939, test loss: 2.3025768262755237\n",
      "Iteration 24 training loss: 2.3025757815054115, test loss: 2.302575813783066\n",
      "Iteration 25 training loss: 2.302574643036231, test loss: 2.302574677394159\n",
      "Iteration 26 training loss: 2.302573364141841, test loss: 2.302573400730389\n",
      "Iteration 27 training loss: 2.3025719263034574, test loss: 2.302571965192416\n",
      "Iteration 28 training loss: 2.3025703087770846, test loss: 2.3025703499896064\n",
      "Iteration 29 training loss: 2.3025684879747037, test loss: 2.3025685315516715\n",
      "Iteration 30 training loss: 2.302566437110222, test loss: 2.302566483218308\n",
      "Iteration 31 training loss: 2.302564125859648, test loss: 2.3025641746196936\n",
      "Iteration 32 training loss: 2.302561519903742, test loss: 2.3025615714407226\n",
      "Iteration 33 training loss: 2.3025585804039554, test loss: 2.302558634773646\n",
      "Iteration 34 training loss: 2.3025552632411386, test loss: 2.30255532051914\n",
      "Iteration 35 training loss: 2.3025515183233485, test loss: 2.3025515785276647\n",
      "Iteration 36 training loss: 2.3025472887991123, test loss: 2.3025473520268136\n",
      "Iteration 37 training loss: 2.3025425100661208, test loss: 2.3025425763825975\n",
      "Iteration 38 training loss: 2.3025371087274245, test loss: 2.3025371781364052\n",
      "Iteration 39 training loss: 2.3025310016166682, test loss: 2.302531074064011\n",
      "Iteration 40 training loss: 2.3025240944010608, test loss: 2.3025241698058836\n",
      "Iteration 41 training loss: 2.3025162800387533, test loss: 2.3025163582635475\n",
      "Iteration 42 training loss: 2.3025074370845484, test loss: 2.3025075178649925\n",
      "Iteration 43 training loss: 2.30249742760897, test loss: 2.302497510560124\n",
      "Iteration 44 training loss: 2.3024860949596633, test loss: 2.3024861795735614\n",
      "Iteration 45 training loss: 2.3024732611409187, test loss: 2.3024733467953067\n",
      "Iteration 46 training loss: 2.3024587239350134, test loss: 2.302458809831847\n",
      "Iteration 47 training loss: 2.302442253616199, test loss: 2.3024423387672783\n",
      "Iteration 48 training loss: 2.302423589265505, test loss: 2.3024236723888247\n",
      "Iteration 49 training loss: 2.302402434548152, test loss: 2.3024025142033353\n",
      "Iteration 50 training loss: 2.3023784527412534, test loss: 2.3023785273194095\n",
      "Iteration 51 training loss: 2.302351261344442, test loss: 2.3023513287001256\n",
      "Iteration 52 training loss: 2.3023204258295578, test loss: 2.3023204836534465\n",
      "Iteration 53 training loss: 2.3022854524241985, test loss: 2.302285497802692\n",
      "Iteration 54 training loss: 2.3022457805250243, test loss: 2.302245809946589\n",
      "Iteration 55 training loss: 2.302200772965874, test loss: 2.302200782492836\n",
      "Iteration 56 training loss: 2.3021497063057312, test loss: 2.3021496911461714\n",
      "Iteration 57 training loss: 2.3020917592143095, test loss: 2.302091713765925\n",
      "Iteration 58 training loss: 2.302025998948375, test loss: 2.3020259166085375\n",
      "Iteration 59 training loss: 2.3019513668478653, test loss: 2.3019512401573734\n",
      "Iteration 60 training loss: 2.301866661331067, test loss: 2.3018664817255\n",
      "Iteration 61 training loss: 2.301770519566494, test loss: 2.3017702768658945\n",
      "Iteration 62 training loss: 2.3016613959958443, test loss: 2.301661077978701\n",
      "Iteration 63 training loss: 2.3015375389251136, test loss: 2.301537131398951\n",
      "Iteration 64 training loss: 2.301396963627269, test loss: 2.30139645051735\n",
      "Iteration 65 training loss: 2.301237423918872, test loss: 2.3012367862138907\n",
      "Iteration 66 training loss: 2.301056378183257, test loss: 2.3010555936825576\n",
      "Iteration 67 training loss: 2.3008509538685806, test loss: 2.3008499974548817\n",
      "Iteration 68 training loss: 2.300617907212838, test loss: 2.300616750010166\n",
      "Iteration 69 training loss: 2.3003535797339, test loss: 2.3003521883504683\n",
      "Iteration 70 training loss: 2.3000538507016333, test loss: 2.300052186441083\n",
      "Iteration 71 training loss: 2.299714085799258, test loss: 2.299712104170245\n",
      "Iteration 72 training loss: 2.2993290827759902, test loss: 2.299326733490773\n",
      "Iteration 73 training loss: 2.2988930124942963, test loss: 2.2988902379189717\n",
      "Iteration 74 training loss: 2.2983993597273633, test loss: 2.2983960940796755\n",
      "Iteration 75 training loss: 2.2978408619112236, test loss: 2.2978370299933304\n",
      "Iteration 76 training loss: 2.2972094484458396, test loss: 2.297204966329483\n",
      "Iteration 77 training loss: 2.296496182422974, test loss: 2.2964909539231306\n",
      "Iteration 78 training loss: 2.2956912104640645, test loss: 2.295685126717871\n",
      "Iteration 79 training loss: 2.29478371715456, test loss: 2.2947766561732488\n",
      "Iteration 80 training loss: 2.2937619001587275, test loss: 2.2937537253495215\n",
      "Iteration 81 training loss: 2.2926129622518774, test loss: 2.2926035203590334\n",
      "Iteration 82 training loss: 2.2913231271056103, test loss: 2.2913122430731185\n",
      "Iteration 83 training loss: 2.2898776994537955, test loss: 2.2898651791623896\n",
      "Iteration 84 training loss: 2.2882611588609025, test loss: 2.2882467897198855\n",
      "Iteration 85 training loss: 2.2864573175762763, test loss: 2.286440864313292\n",
      "Iteration 86 training loss: 2.284449529631723, test loss: 2.2844307289047063\n",
      "Iteration 87 training loss: 2.2822209751864087, test loss: 2.2821995465747325\n",
      "Iteration 88 training loss: 2.2797550240211213, test loss: 2.279730671255757\n",
      "Iteration 89 training loss: 2.277035670909366, test loss: 2.2770080827752346\n",
      "Iteration 90 training loss: 2.2740480652298856, test loss: 2.2740169120488667\n",
      "Iteration 91 training loss: 2.27077906876964, test loss: 2.2707440136043897\n",
      "Iteration 92 training loss: 2.2672178725988816, test loss: 2.2671786186783547\n",
      "Iteration 93 training loss: 2.263356671788022, test loss: 2.2633129590104955\n",
      "Iteration 94 training loss: 2.259191240490892, test loss: 2.259142782122362\n",
      "Iteration 95 training loss: 2.2547213801374557, test loss: 2.254667872937176\n",
      "Iteration 96 training loss: 2.2499512393988526, test loss: 2.2498923610987047\n",
      "Iteration 97 training loss: 2.2448895005828846, test loss: 2.2448251658414717\n",
      "Iteration 98 training loss: 2.239549151167778, test loss: 2.239479477028003\n",
      "Iteration 99 training loss: 2.2339474858486152, test loss: 2.2338725519323552\n",
      "Iteration 100 training loss: 2.2281047966920857, test loss: 2.2280249205719334\n",
      "Iteration 101 training loss: 2.2220435552939963, test loss: 2.221959338502592\n",
      "Iteration 102 training loss: 2.2157871223976042, test loss: 2.2156994401041636\n",
      "Iteration 103 training loss: 2.2093586773258194, test loss: 2.2092680329089274\n",
      "Iteration 104 training loss: 2.2027796339276016, test loss: 2.202686754141821\n",
      "Iteration 105 training loss: 2.196067636952649, test loss: 2.1959740790819096\n",
      "Iteration 106 training loss: 2.189236382487795, test loss: 2.189144091002285\n",
      "Iteration 107 training loss: 2.182294833427053, test loss: 2.182205663358603\n",
      "Iteration 108 training loss: 2.1752448949360144, test loss: 2.17516099217219\n",
      "Iteration 109 training loss: 2.1680815517641103, test loss: 2.16800335059649\n",
      "Iteration 110 training loss: 2.160793278169877, test loss: 2.1607219648606333\n",
      "Iteration 111 training loss: 2.1533617637079514, test loss: 2.1532994833740173\n",
      "Iteration 112 training loss: 2.145761619187584, test loss: 2.145709773448195\n",
      "Iteration 113 training loss: 2.1379619351073167, test loss: 2.1379215275758434\n",
      "Iteration 114 training loss: 2.1299265515990413, test loss: 2.1299013228552273\n",
      "Iteration 115 training loss: 2.1216163626179076, test loss: 2.121608637894367\n",
      "Iteration 116 training loss: 2.112988842501913, test loss: 2.1130026687683814\n",
      "Iteration 117 training loss: 2.104000224282807, test loss: 2.1040409626312484\n",
      "Iteration 118 training loss: 2.094607713276706, test loss: 2.094679233881377\n",
      "Iteration 119 training loss: 2.084772144756645, test loss: 2.0848778718335015\n",
      "Iteration 120 training loss: 2.074456937234784, test loss: 2.0746018440201683\n",
      "Iteration 121 training loss: 2.063632373796705, test loss: 2.063821448299219\n",
      "Iteration 122 training loss: 2.0522780527455997, test loss: 2.0525154045264165\n",
      "Iteration 123 training loss: 2.0403845043480913, test loss: 2.040674537904218\n",
      "Iteration 124 training loss: 2.027955663327116, test loss: 2.028301942067324\n",
      "Iteration 125 training loss: 2.0150101095523927, test loss: 2.015415731668867\n",
      "Iteration 126 training loss: 2.0015808328635187, test loss: 2.0020484168792874\n",
      "Iteration 127 training loss: 1.987715588019819, test loss: 1.9882474745937693\n",
      "Iteration 128 training loss: 1.9734757671836751, test loss: 1.9740738671780396\n",
      "Iteration 129 training loss: 1.9589334160340035, test loss: 1.9595992990798032\n",
      "Iteration 130 training loss: 1.9441685188222002, test loss: 1.9449030488741839\n",
      "Iteration 131 training loss: 1.9292650363827248, test loss: 1.930068626137282\n",
      "Iteration 132 training loss: 1.914306278275379, test loss: 1.9151793299335724\n",
      "Iteration 133 training loss: 1.8993708174301314, test loss: 1.9003129126272245\n",
      "Iteration 134 training loss: 1.8845294071089969, test loss: 1.8855398279083928\n",
      "Iteration 135 training loss: 1.869842012663537, test loss: 1.8709196324999102\n",
      "Iteration 136 training loss: 1.8553569290757188, test loss: 1.8565006491813902\n",
      "Iteration 137 training loss: 1.8411115977356864, test loss: 1.8423200404084972\n",
      "Iteration 138 training loss: 1.8271318279542987, test loss: 1.8284035263411396\n",
      "Iteration 139 training loss: 1.813434394946966, test loss: 1.8147674627148604\n",
      "Iteration 140 training loss: 1.8000278823918952, test loss: 1.8014208681761927\n",
      "Iteration 141 training loss: 1.7869150359118233, test loss: 1.7883658956726518\n",
      "Iteration 142 training loss: 1.7740946703390454, test loss: 1.7756023597002066\n",
      "Iteration 143 training loss: 1.7615626812184575, test loss: 1.763126428486951\n",
      "Iteration 144 training loss: 1.749313077138783, test loss: 1.7509325371510684\n",
      "Iteration 145 training loss: 1.7373390134314555, test loss: 1.7390139924795351\n",
      "Iteration 146 training loss: 1.725633788001551, test loss: 1.7273641516933533\n",
      "Iteration 147 training loss: 1.7141913518464178, test loss: 1.7159770407898003\n",
      "Iteration 148 training loss: 1.7030062918603242, test loss: 1.7048473714251904\n",
      "Iteration 149 training loss: 1.6920733756918596, test loss: 1.6939701781366159\n",
      "Iteration 150 training loss: 1.6813879570202224, test loss: 1.6833408547514714\n",
      "Iteration 151 training loss: 1.67094528411375, test loss: 1.6729549186567994\n",
      "Iteration 152 training loss: 1.6607408728497532, test loss: 1.6628077215759829\n",
      "Iteration 153 training loss: 1.6507705349474406, test loss: 1.6528952814173485\n",
      "Iteration 154 training loss: 1.6410297765017487, test loss: 1.6432136143301632\n",
      "Iteration 155 training loss: 1.6315140474922796, test loss: 1.6337577290144283\n",
      "Iteration 156 training loss: 1.6222188006668183, test loss: 1.6245232344330944\n",
      "Iteration 157 training loss: 1.6131394838744246, test loss: 1.6155055840130026\n",
      "Iteration 158 training loss: 1.6042713314778894, test loss: 1.6067000520306842\n",
      "Iteration 159 training loss: 1.5956094424248264, test loss: 1.5981016452097332\n",
      "Iteration 160 training loss: 1.5871485727206052, test loss: 1.5897051206357518\n",
      "Iteration 161 training loss: 1.578883660776187, test loss: 1.5815052038309614\n",
      "Iteration 162 training loss: 1.5708094228835292, test loss: 1.573496719761117\n",
      "Iteration 163 training loss: 1.5629206726357765, test loss: 1.5656742866475015\n",
      "Iteration 164 training loss: 1.555212108375735, test loss: 1.558032494103728\n",
      "Iteration 165 training loss: 1.5476785524572876, test loss: 1.5505661407443807\n",
      "Iteration 166 training loss: 1.540314670287019, test loss: 1.5432697822457093\n",
      "Iteration 167 training loss: 1.5331151548405848, test loss: 1.5361381593070726\n",
      "Iteration 168 training loss: 1.526074684631919, test loss: 1.5291659940113373\n",
      "Iteration 169 training loss: 1.5191879301084759, test loss: 1.5223477243828523\n",
      "Iteration 170 training loss: 1.5124495992812192, test loss: 1.515678117926994\n",
      "Iteration 171 training loss: 1.5058544934454956, test loss: 1.5091519587900895\n",
      "Iteration 172 training loss: 1.4993974790853237, test loss: 1.502764170716175\n",
      "Iteration 173 training loss: 1.4930736618690879, test loss: 1.4965098065681277\n",
      "Iteration 174 training loss: 1.4868783061777933, test loss: 1.4903839503527914\n",
      "Iteration 175 training loss: 1.4808066932876416, test loss: 1.484381962548583\n",
      "Iteration 176 training loss: 1.4748541887128435, test loss: 1.4784991773376306\n",
      "Iteration 177 training loss: 1.4690163720895444, test loss: 1.4727311333112294\n",
      "Iteration 178 training loss: 1.463288875061923, test loss: 1.4670735252712634\n",
      "Iteration 179 training loss: 1.4576674816899926, test loss: 1.4615220654928118\n",
      "Iteration 180 training loss: 1.4521481664443343, test loss: 1.456072699447662\n",
      "Iteration 181 training loss: 1.446726952492092, test loss: 1.4507213302851318\n",
      "Iteration 182 training loss: 1.441400058258915, test loss: 1.4454642214603772\n",
      "Iteration 183 training loss: 1.4361637520875354, test loss: 1.4402977740107585\n",
      "Iteration 184 training loss: 1.4310143715662258, test loss: 1.4352182511171516\n",
      "Iteration 185 training loss: 1.4259484640615796, test loss: 1.4302220929025986\n",
      "Iteration 186 training loss: 1.4209626485328415, test loss: 1.4253058793778313\n",
      "Iteration 187 training loss: 1.4160536631677256, test loss: 1.4204664660365787\n",
      "Iteration 188 training loss: 1.4112183742201394, test loss: 1.4157006126048728\n",
      "Iteration 189 training loss: 1.4064537619588604, test loss: 1.4110053649816945\n",
      "Iteration 190 training loss: 1.401756868673115, test loss: 1.406377723723233\n",
      "Iteration 191 training loss: 1.3971248666911553, test loss: 1.401814892565845\n",
      "Iteration 192 training loss: 1.3925549464577687, test loss: 1.3973140055196112\n",
      "Iteration 193 training loss: 1.3880445494672469, test loss: 1.3928723415397009\n",
      "Iteration 194 training loss: 1.383591075410707, test loss: 1.3884873761207763\n",
      "Iteration 195 training loss: 1.3791920536148987, test loss: 1.3841566350380818\n",
      "Iteration 196 training loss: 1.374845028763746, test loss: 1.379877670149147\n",
      "Iteration 197 training loss: 1.3705476539039805, test loss: 1.3756481663092341\n",
      "Iteration 198 training loss: 1.3662976965228613, test loss: 1.3714657712055036\n",
      "Iteration 199 training loss: 1.3620929508043396, test loss: 1.367328337712782\n",
      "Iteration 200 training loss: 1.357931305003302, test loss: 1.3632337447250942\n",
      "Iteration 201 training loss: 1.3538107270423747, test loss: 1.3591799000655522\n",
      "Iteration 202 training loss: 1.3497292450435683, test loss: 1.3551650320580206\n",
      "Iteration 203 training loss: 1.3456850663905051, test loss: 1.3511871887077482\n",
      "Iteration 204 training loss: 1.3416762840388332, test loss: 1.3472445247495257\n",
      "Iteration 205 training loss: 1.3377014434085241, test loss: 1.343335629501715\n",
      "Iteration 206 training loss: 1.333758926051122, test loss: 1.3394589028434083\n",
      "Iteration 207 training loss: 1.3298470584375572, test loss: 1.3356127508423412\n",
      "Iteration 208 training loss: 1.3259643431204247, test loss: 1.331795653405711\n",
      "Iteration 209 training loss: 1.3221093590063273, test loss: 1.3280059444185541\n",
      "Iteration 210 training loss: 1.318280446931422, test loss: 1.3242420148068728\n",
      "Iteration 211 training loss: 1.3144761259631301, test loss: 1.3205024048684335\n",
      "Iteration 212 training loss: 1.31069490289936, test loss: 1.316785733820088\n",
      "Iteration 213 training loss: 1.3069354253466876, test loss: 1.31309040772933\n",
      "Iteration 214 training loss: 1.3031964775764793, test loss: 1.3094151911203742\n",
      "Iteration 215 training loss: 1.2994764976508106, test loss: 1.3057588788889614\n",
      "Iteration 216 training loss: 1.2957740916172658, test loss: 1.3021198855480092\n",
      "Iteration 217 training loss: 1.2920880547751754, test loss: 1.2984970232881536\n",
      "Iteration 218 training loss: 1.2884171301830185, test loss: 1.2948888564133754\n",
      "Iteration 219 training loss: 1.2847600812027808, test loss: 1.291294141883282\n",
      "Iteration 220 training loss: 1.2811157297952473, test loss: 1.287711966969021\n",
      "Iteration 221 training loss: 1.277483051065148, test loss: 1.284141197434015\n",
      "Iteration 222 training loss: 1.273860983701006, test loss: 1.2805808951822681\n",
      "Iteration 223 training loss: 1.2702484183003877, test loss: 1.2770299201918613\n",
      "Iteration 224 training loss: 1.2666444126847274, test loss: 1.2734873886861882\n",
      "Iteration 225 training loss: 1.2630480155646073, test loss: 1.2699523530831631\n",
      "Iteration 226 training loss: 1.2594584639282433, test loss: 1.266423798592668\n",
      "Iteration 227 training loss: 1.2558748826936588, test loss: 1.262900826689732\n",
      "Iteration 228 training loss: 1.252296439385263, test loss: 1.2593825265086773\n",
      "Iteration 229 training loss: 1.2487223432857673, test loss: 1.2558683146803975\n",
      "Iteration 230 training loss: 1.2451519249487677, test loss: 1.2523575920984893\n",
      "Iteration 231 training loss: 1.2415846146164964, test loss: 1.248849780022899\n",
      "Iteration 232 training loss: 1.238019887362471, test loss: 1.2453441900122617\n",
      "Iteration 233 training loss: 1.2344571711931858, test loss: 1.2418402488851294\n",
      "Iteration 234 training loss: 1.2308961961516383, test loss: 1.2383375941077266\n",
      "Iteration 235 training loss: 1.2273366088370299, test loss: 1.2348359583138906\n",
      "Iteration 236 training loss: 1.2237780972656995, test loss: 1.2313351141806947\n",
      "Iteration 237 training loss: 1.2202203545772294, test loss: 1.2278348386115663\n",
      "Iteration 238 training loss: 1.2166633135396137, test loss: 1.2243349977456675\n",
      "Iteration 239 training loss: 1.2131070001414976, test loss: 1.220835519838181\n",
      "Iteration 240 training loss: 1.2095513202405923, test loss: 1.2173364646228937\n",
      "Iteration 241 training loss: 1.2059963070504258, test loss: 1.21383778604009\n",
      "Iteration 242 training loss: 1.2024422018641916, test loss: 1.2103395339982874\n",
      "Iteration 243 training loss: 1.198889202624965, test loss: 1.206842089843846\n",
      "Iteration 244 training loss: 1.1953375416507175, test loss: 1.2033455900822887\n",
      "Iteration 245 training loss: 1.1917874597434361, test loss: 1.1998504421701206\n",
      "Iteration 246 training loss: 1.1882393511017546, test loss: 1.19635688052684\n",
      "Iteration 247 training loss: 1.1846936690325405, test loss: 1.1928654437979676\n",
      "Iteration 248 training loss: 1.181150930751956, test loss: 1.1893765981477686\n",
      "Iteration 249 training loss: 1.1776116421689933, test loss: 1.1858909201178551\n",
      "Iteration 250 training loss: 1.174076485659568, test loss: 1.1824089930345238\n",
      "Iteration 251 training loss: 1.1705460993287478, test loss: 1.178931521972215\n",
      "Iteration 252 training loss: 1.1670212401304954, test loss: 1.175459327871334\n",
      "Iteration 253 training loss: 1.1635026650970954, test loss: 1.1719931192048572\n",
      "Iteration 254 training loss: 1.159991107245236, test loss: 1.1685337343726714\n",
      "Iteration 255 training loss: 1.1564874474023816, test loss: 1.1650819865887727\n",
      "Iteration 256 training loss: 1.1529925504057825, test loss: 1.1616387549226157\n",
      "Iteration 257 training loss: 1.1495073353911485, test loss: 1.15820509256883\n",
      "Iteration 258 training loss: 1.1460327710026708, test loss: 1.1547819380347224\n",
      "Iteration 259 training loss: 1.1425697394221834, test loss: 1.1513699758496792\n",
      "Iteration 260 training loss: 1.1391191895034587, test loss: 1.1479702682176753\n",
      "Iteration 261 training loss: 1.1356820260848466, test loss: 1.1445838318205475\n",
      "Iteration 262 training loss: 1.1322592404731686, test loss: 1.1412115627171662\n",
      "Iteration 263 training loss: 1.1288517091493837, test loss: 1.1378545107605034\n",
      "Iteration 264 training loss: 1.1254604562482868, test loss: 1.13451367512181\n",
      "Iteration 265 training loss: 1.1220865040459385, test loss: 1.1311899396813991\n",
      "Iteration 266 training loss: 1.118730736940341, test loss: 1.127884329019166\n",
      "Iteration 267 training loss: 1.1153940489889107, test loss: 1.124597713712939\n",
      "Iteration 268 training loss: 1.1120773967104658, test loss: 1.121331089303114\n",
      "Iteration 269 training loss: 1.1087816015478245, test loss: 1.1180852322500905\n",
      "Iteration 270 training loss: 1.1055074722219578, test loss: 1.1148610678359168\n",
      "Iteration 271 training loss: 1.1022558538390252, test loss: 1.1116594188170623\n",
      "Iteration 272 training loss: 1.0990275078072578, test loss: 1.1084809147103405\n",
      "Iteration 273 training loss: 1.0958230964691142, test loss: 1.1053263584126498\n",
      "Iteration 274 training loss: 1.0926432739298262, test loss: 1.102196441811446\n",
      "Iteration 275 training loss: 1.0894887462179015, test loss: 1.0990917692530084\n",
      "Iteration 276 training loss: 1.0863601286521058, test loss: 1.0960129817148783\n",
      "Iteration 277 training loss: 1.0832579371303586, test loss: 1.0929607270782047\n",
      "Iteration 278 training loss: 1.080182689212093, test loss: 1.089935455218895\n",
      "Iteration 279 training loss: 1.0771348533241891, test loss: 1.086937651101676\n",
      "Iteration 280 training loss: 1.0741148814146024, test loss: 1.0839677755452581\n",
      "Iteration 281 training loss: 1.071123225666976, test loss: 1.081026257693384\n",
      "Iteration 282 training loss: 1.0681600777766516, test loss: 1.0781132900995638\n",
      "Iteration 283 training loss: 1.0652258741053058, test loss: 1.075229314856815\n",
      "Iteration 284 training loss: 1.0623208059599363, test loss: 1.0723744473066517\n",
      "Iteration 285 training loss: 1.0594450666392692, test loss: 1.0695489606756645\n",
      "Iteration 286 training loss: 1.056598807432284, test loss: 1.0667531735024702\n",
      "Iteration 287 training loss: 1.0537823027779565, test loss: 1.063987345503192\n",
      "Iteration 288 training loss: 1.0509956213859677, test loss: 1.0612515028043432\n",
      "Iteration 289 training loss: 1.0482388165166778, test loss: 1.058545590527988\n",
      "Iteration 290 training loss: 1.045512083770297, test loss: 1.055869877622141\n",
      "Iteration 291 training loss: 1.0428153348144968, test loss: 1.0532241030032923\n",
      "Iteration 292 training loss: 1.0401485211671688, test loss: 1.0506082635431002\n",
      "Iteration 293 training loss: 1.037511722249708, test loss: 1.0480226191615767\n",
      "Iteration 294 training loss: 1.0349048560850351, test loss: 1.0454668709051704\n",
      "Iteration 295 training loss: 1.0323278225855086, test loss: 1.0429410179332463\n",
      "Iteration 296 training loss: 1.0297804796569703, test loss: 1.0404448744439474\n",
      "Iteration 297 training loss: 1.027262665529002, test loss: 1.0379781880747334\n",
      "Iteration 298 training loss: 1.02477425275315, test loss: 1.0355411187257546\n",
      "Iteration 299 training loss: 1.0223148528553447, test loss: 1.0331331431840032\n",
      "Iteration 300 training loss: 1.0198843706433103, test loss: 1.0307543514389408\n",
      "Iteration 301 training loss: 1.0174823833234525, test loss: 1.0284041043615446\n",
      "Iteration 302 training loss: 1.0151086227772261, test loss: 1.0260821438067405\n",
      "Iteration 303 training loss: 1.0127628781299918, test loss: 1.0237883586777323\n",
      "Iteration 304 training loss: 1.010444723935018, test loss: 1.0215221250888906\n",
      "Iteration 305 training loss: 1.008153840668957, test loss: 1.0192830486038462\n",
      "Iteration 306 training loss: 1.005889950570177, test loss: 1.017071019748\n",
      "Iteration 307 training loss: 1.0036527015474603, test loss: 1.014885564412308\n",
      "Iteration 308 training loss: 1.0014418101574265, test loss: 1.0127266659688607\n",
      "Iteration 309 training loss: 0.9992568700244026, test loss: 1.0105936400550335\n",
      "Iteration 310 training loss: 0.997097486621214, test loss: 1.0084861459099181\n",
      "Iteration 311 training loss: 0.994963346992625, test loss: 1.0064037111668058\n",
      "Iteration 312 training loss: 0.9928540741723764, test loss: 1.0043462273607773\n",
      "Iteration 313 training loss: 0.9907692806158145, test loss: 1.0023132118213627\n",
      "Iteration 314 training loss: 0.9887085818121593, test loss: 1.0003042752637863\n",
      "Iteration 315 training loss: 0.9866716148354535, test loss: 0.9983190778412638\n",
      "Iteration 316 training loss: 0.984657973070039, test loss: 0.996357164052852\n",
      "Iteration 317 training loss: 0.982667244837318, test loss: 0.9944180446685026\n",
      "Iteration 318 training loss: 0.9806990670984425, test loss: 0.9925015532066537\n",
      "Iteration 319 training loss: 0.9787531089772253, test loss: 0.9906070641641179\n",
      "Iteration 320 training loss: 0.9768290219053242, test loss: 0.9887344806878465\n",
      "Iteration 321 training loss: 0.9749264095688941, test loss: 0.986883194691698\n",
      "Iteration 322 training loss: 0.9730448285502149, test loss: 0.9850528325016877\n",
      "Iteration 323 training loss: 0.9711839427572527, test loss: 0.983242929197965\n",
      "Iteration 324 training loss: 0.9693433966279322, test loss: 0.9814533188474895\n",
      "Iteration 325 training loss: 0.9675228001429178, test loss: 0.9796833649568787\n",
      "Iteration 326 training loss: 0.9657217836248769, test loss: 0.9779329880182013\n",
      "Iteration 327 training loss: 0.963939968976085, test loss: 0.9762015207910979\n",
      "Iteration 328 training loss: 0.9621770140518487, test loss: 0.9744888901547644\n",
      "Iteration 329 training loss: 0.9604325214005637, test loss: 0.972794495091215\n",
      "Iteration 330 training loss: 0.9587061451388351, test loss: 0.9711181959118057\n",
      "Iteration 331 training loss: 0.9569975390711013, test loss: 0.969459237054519\n",
      "Iteration 332 training loss: 0.9553063647691625, test loss: 0.9678176879876508\n",
      "Iteration 333 training loss: 0.9536322482222817, test loss: 0.9661928288896086\n",
      "Iteration 334 training loss: 0.9519748552186197, test loss: 0.9645846470279174\n",
      "Iteration 335 training loss: 0.9503338693948892, test loss: 0.9629923260674482\n",
      "Iteration 336 training loss: 0.9487089371723357, test loss: 0.961416315347688\n",
      "Iteration 337 training loss: 0.947099701037423, test loss: 0.9598553899708592\n",
      "Iteration 338 training loss: 0.945505813229515, test loss: 0.9583101419409641\n",
      "Iteration 339 training loss: 0.9439270262690979, test loss: 0.9567791659726119\n",
      "Iteration 340 training loss: 0.9423630036047548, test loss: 0.9552635558004285\n",
      "Iteration 341 training loss: 0.9408134296375877, test loss: 0.9537613144258361\n",
      "Iteration 342 training loss: 0.9392779563430917, test loss: 0.952273938360336\n",
      "Iteration 343 training loss: 0.9377563654769719, test loss: 0.950799042494216\n",
      "Iteration 344 training loss: 0.9362483390524113, test loss: 0.9493388915364193\n",
      "Iteration 345 training loss: 0.9347535899256212, test loss: 0.9478899883355054\n",
      "Iteration 346 training loss: 0.9332718245284946, test loss: 0.9464559800549025\n",
      "Iteration 347 training loss: 0.9318027429953212, test loss: 0.9450318384657413\n",
      "Iteration 348 training loss: 0.9303460886559805, test loss: 0.9436231236644302\n",
      "Iteration 349 training loss: 0.9289015682859797, test loss: 0.9422222944188045\n",
      "Iteration 350 training loss: 0.9274689365252473, test loss: 0.9408379777711702\n",
      "Iteration 351 training loss: 0.9260479055608388, test loss: 0.9394590957262231\n",
      "Iteration 352 training loss: 0.9246382245165615, test loss: 0.9380985474308643\n",
      "Iteration 353 training loss: 0.9232396651560816, test loss: 0.9367397293852863\n",
      "Iteration 354 training loss: 0.9218519848135373, test loss: 0.9354028950504276\n",
      "Iteration 355 training loss: 0.9204749566726281, test loss: 0.9340621367835494\n",
      "Iteration 356 training loss: 0.9191083404958209, test loss: 0.9327495317504263\n",
      "Iteration 357 training loss: 0.9177520259635775, test loss: 0.9314243387340273\n",
      "Iteration 358 training loss: 0.9164057775638753, test loss: 0.9301375667095378\n",
      "Iteration 359 training loss: 0.9150695127612278, test loss: 0.9288242795739261\n",
      "Iteration 360 training loss: 0.9137431523068679, test loss: 0.9275667980489685\n",
      "Iteration 361 training loss: 0.9124268382489772, test loss: 0.9262602934208908\n",
      "Iteration 362 training loss: 0.9111207543944196, test loss: 0.9250389415539022\n",
      "Iteration 363 training loss: 0.909825651400023, test loss: 0.9237318117758848\n",
      "Iteration 364 training loss: 0.9085425835573769, test loss: 0.9225607227454933\n",
      "Iteration 365 training loss: 0.9072738561198367, test loss: 0.921243598132438\n",
      "Iteration 366 training loss: 0.9060233837309296, test loss: 0.9201519565396872\n",
      "Iteration 367 training loss: 0.9047975244798, test loss: 0.9188157360194349\n",
      "Iteration 368 training loss: 0.9036097063916211, test loss: 0.9178677936789263\n",
      "Iteration 369 training loss: 0.9024768329685314, test loss: 0.9165182847246685\n",
      "Iteration 370 training loss: 0.9014443359410693, test loss: 0.9158671500383676\n",
      "Iteration 371 training loss: 0.9005523473354022, test loss: 0.9145756057793447\n",
      "Iteration 372 training loss: 0.8999568386103104, test loss: 0.9146091069376729\n",
      "Iteration 373 training loss: 0.8997168439892067, test loss: 0.913656817565846\n",
      "Iteration 374 training loss: 0.9003676879672275, test loss: 0.9153667954060546\n",
      "Iteration 375 training loss: 0.9017144149534839, test loss: 0.9154816244474506\n",
      "Iteration 376 training loss: 0.9054857528999298, test loss: 0.9210186034542192\n",
      "Iteration 377 training loss: 0.9092386111869922, test loss: 0.922746780242344\n",
      "Iteration 378 training loss: 0.9179895969253898, test loss: 0.9342242484154769\n",
      "Iteration 379 training loss: 0.9204527460340852, test loss: 0.9336858091706014\n",
      "Iteration 380 training loss: 0.930947877422442, test loss: 0.9477347688452632\n",
      "Iteration 381 training loss: 0.9252983180412135, test loss: 0.9383740288311757\n",
      "Iteration 382 training loss: 0.9331228032154628, test loss: 0.9500751998037895\n",
      "Iteration 383 training loss: 0.9233111649193008, test loss: 0.9364094438316312\n",
      "Iteration 384 training loss: 0.929632440711696, test loss: 0.9466210916704165\n",
      "Iteration 385 training loss: 0.9200647170882793, test loss: 0.9332604971569419\n",
      "Iteration 386 training loss: 0.9259215471456975, test loss: 0.9429598458388797\n",
      "Iteration 387 training loss: 0.9169272768345299, test loss: 0.9302268174777426\n",
      "Iteration 388 training loss: 0.9225162410439484, test loss: 0.9396139911987293\n",
      "Iteration 389 training loss: 0.9139140640099268, test loss: 0.9273136414920875\n",
      "Iteration 390 training loss: 0.9192948787057058, test loss: 0.9364546764041044\n",
      "Iteration 391 training loss: 0.910987686673619, test loss: 0.924483635714691\n",
      "Iteration 392 training loss: 0.9161934839383971, test loss: 0.933415779510531\n",
      "Iteration 393 training loss: 0.9081315083387812, test loss: 0.9217205460053624\n",
      "Iteration 394 training loss: 0.913181042607448, test loss: 0.9304653556260373\n",
      "Iteration 395 training loss: 0.905335157745947, test loss: 0.9190147034952848\n",
      "Iteration 396 training loss: 0.910241943785361, test loss: 0.9275875091361804\n",
      "Iteration 397 training loss: 0.902594919269858, test loss: 0.9163626612515935\n",
      "Iteration 398 training loss: 0.9073690044771434, test loss: 0.9247751324982372\n",
      "Iteration 399 training loss: 0.899907772322999, test loss: 0.9137614327809362\n",
      "Iteration 400 training loss: 0.9045556441779361, test loss: 0.9220213343868376\n",
      "Iteration 401 training loss: 0.8972688334219114, test loss: 0.9112061905473993\n",
      "Iteration 402 training loss: 0.9017965371198243, test loss: 0.9193210340238361\n",
      "Iteration 403 training loss: 0.8946757120282492, test loss: 0.908694731176167\n",
      "Iteration 404 training loss: 0.8990879107625003, test loss: 0.9166704604103899\n",
      "Iteration 405 training loss: 0.8921249009952613, test loss: 0.9062236398251942\n",
      "Iteration 406 training loss: 0.8964264823114798, test loss: 0.9140662228655485\n",
      "Iteration 407 training loss: 0.8896153510098349, test loss: 0.9037919844072271\n",
      "Iteration 408 training loss: 0.8938101702772789, test loss: 0.9115062126756407\n",
      "Iteration 409 training loss: 0.8871432860928578, test loss: 0.9013958256774911\n",
      "Iteration 410 training loss: 0.8912358799684056, test loss: 0.9089876024399586\n",
      "Iteration 411 training loss: 0.88470777412602, test loss: 0.8990347998743109\n",
      "Iteration 412 training loss: 0.8887011688167845, test loss: 0.9065080303218603\n",
      "Iteration 413 training loss: 0.8823069192443914, test loss: 0.8967065687155922\n",
      "Iteration 414 training loss: 0.8862043607634769, test loss: 0.9040655272307869\n",
      "Iteration 415 training loss: 0.8799390482155084, test loss: 0.8944099164423842\n",
      "Iteration 416 training loss: 0.8837423675816385, test loss: 0.9016569874753437\n",
      "Iteration 417 training loss: 0.8776017542473569, test loss: 0.8921421161764176\n",
      "Iteration 418 training loss: 0.8813141003772932, test loss: 0.8992816158734497\n",
      "Iteration 419 training loss: 0.8752956009311272, test loss: 0.8899040321059385\n",
      "Iteration 420 training loss: 0.8789196234183305, test loss: 0.8969395115353674\n",
      "Iteration 421 training loss: 0.8730203454186145, test loss: 0.8876956980573708\n",
      "Iteration 422 training loss: 0.8765585493210039, test loss: 0.8946301690063659\n",
      "Iteration 423 training loss: 0.8707730764269168, test loss: 0.8855139463239422\n",
      "Iteration 424 training loss: 0.8742280447473045, test loss: 0.8923507614400175\n",
      "Iteration 425 training loss: 0.8685528012101056, test loss: 0.8833578392323216\n",
      "Iteration 426 training loss: 0.8719266062782273, test loss: 0.8900996719034251\n",
      "Iteration 427 training loss: 0.866359944083339, test loss: 0.8812277534468832\n",
      "Iteration 428 training loss: 0.8696547935115906, test loss: 0.8878775635406828\n",
      "Iteration 429 training loss: 0.8641924119550255, test loss: 0.8791218057382384\n",
      "Iteration 430 training loss: 0.8674097143178661, test loss: 0.8856818439357989\n",
      "Iteration 431 training loss: 0.8620496732512392, test loss: 0.8770397532512653\n",
      "Iteration 432 training loss: 0.8651923971499555, test loss: 0.883513407473129\n",
      "Iteration 433 training loss: 0.859931393310181, test loss: 0.8749812779428192\n",
      "Iteration 434 training loss: 0.8630004615060152, test loss: 0.881369471807499\n",
      "Iteration 435 training loss: 0.8578368621180538, test loss: 0.872945795696822\n",
      "Iteration 436 training loss: 0.8608345667384424, test loss: 0.8792511617629403\n",
      "Iteration 437 training loss: 0.8557654728645615, test loss: 0.870932488006226\n",
      "Iteration 438 training loss: 0.8586934833435669, test loss: 0.8771573724165413\n",
      "Iteration 439 training loss: 0.8537171896172341, test loss: 0.8689414236165814\n",
      "Iteration 440 training loss: 0.8565771219011903, test loss: 0.875087646080104\n",
      "Iteration 441 training loss: 0.8516904757313003, test loss: 0.8669711634979224\n",
      "Iteration 442 training loss: 0.8544841889261241, test loss: 0.8730410858853719\n",
      "Iteration 443 training loss: 0.8496851446770111, test loss: 0.86502132774627\n",
      "Iteration 444 training loss: 0.8524137239753933, test loss: 0.8710164074080256\n",
      "Iteration 445 training loss: 0.8477011350016147, test loss: 0.8630921067390904\n",
      "Iteration 446 training loss: 0.8503661738632846, test loss: 0.8690141840406886\n",
      "Iteration 447 training loss: 0.8457366672784924, test loss: 0.861181558328799\n",
      "Iteration 448 training loss: 0.8483382027425314, test loss: 0.867030875879982\n",
      "Iteration 449 training loss: 0.843792314449581, test loss: 0.8592902655817932\n",
      "Iteration 450 training loss: 0.846332134868666, test loss: 0.8650690609605134\n",
      "Iteration 451 training loss: 0.8418682482828552, test loss: 0.8574186090901654\n",
      "Iteration 452 training loss: 0.8443491400946354, test loss: 0.8631302101505997\n",
      "Iteration 453 training loss: 0.8399649679452831, test loss: 0.8555671578662972\n",
      "Iteration 454 training loss: 0.8423872615856575, test loss: 0.8612118783372169\n",
      "Iteration 455 training loss: 0.8380806270300879, test loss: 0.8537339927674653\n",
      "Iteration 456 training loss: 0.8404458066693189, test loss: 0.8593134870846474\n",
      "Iteration 457 training loss: 0.8362150798418074, test loss: 0.8519188353557542\n",
      "Iteration 458 training loss: 0.8385238186907457, test loss: 0.8574338871131943\n",
      "Iteration 459 training loss: 0.8343681824327033, test loss: 0.8501217578456892\n",
      "Iteration 460 training loss: 0.8366227124003885, test loss: 0.8555747066726439\n",
      "Iteration 461 training loss: 0.8325393845603595, test loss: 0.8483422922036717\n",
      "Iteration 462 training loss: 0.8347410051361105, test loss: 0.8537345710863247\n",
      "Iteration 463 training loss: 0.8307298403299421, test loss: 0.8465816307023423\n",
      "Iteration 464 training loss: 0.8328796885839574, test loss: 0.8519144884853369\n",
      "Iteration 465 training loss: 0.8289375326408008, test loss: 0.8448377751153002\n",
      "Iteration 466 training loss: 0.8310366197285062, test loss: 0.8501120853139084\n",
      "Iteration 467 training loss: 0.8271616216738346, test loss: 0.8431098828073815\n",
      "Iteration 468 training loss: 0.8292120829552386, test loss: 0.8483277041563326\n",
      "Iteration 469 training loss: 0.8254022858622247, test loss: 0.8413980395385959\n",
      "Iteration 470 training loss: 0.8274048506948333, test loss: 0.846560216039551\n",
      "Iteration 471 training loss: 0.8236593936462874, test loss: 0.839702091406364\n",
      "Iteration 472 training loss: 0.8256143045847357, test loss: 0.8448087292587304\n",
      "Iteration 473 training loss: 0.8219330003493328, test loss: 0.8380222558401994\n",
      "Iteration 474 training loss: 0.8238424095509022, test loss: 0.8430753811696672\n",
      "Iteration 475 training loss: 0.8202223216882603, test loss: 0.8363575953079184\n",
      "Iteration 476 training loss: 0.8220878074290562, test loss: 0.8413590161612928\n",
      "Iteration 477 training loss: 0.8185286298588543, test loss: 0.8347093816362331\n",
      "Iteration 478 training loss: 0.8203505815354405, test loss: 0.8396596346472796\n",
      "Iteration 479 training loss: 0.8168505787876998, test loss: 0.8330763301124707\n",
      "Iteration 480 training loss: 0.81863009116108, test loss: 0.8379766448900212\n",
      "Iteration 481 training loss: 0.8151873117677819, test loss: 0.8314578101738027\n",
      "Iteration 482 training loss: 0.8169267905257377, test loss: 0.8363102565763922\n",
      "Iteration 483 training loss: 0.8135414312331258, test loss: 0.8298560080471712\n",
      "Iteration 484 training loss: 0.8152414038009401, test loss: 0.8346611405970064\n",
      "Iteration 485 training loss: 0.8119103916378125, test loss: 0.8282685457277775\n",
      "Iteration 486 training loss: 0.8135713793581308, test loss: 0.8330269999835357\n",
      "Iteration 487 training loss: 0.8102926706540603, test loss: 0.8266938642585429\n",
      "Iteration 488 training loss: 0.8119163454652901, test loss: 0.8314073401021773\n",
      "Iteration 489 training loss: 0.8086913802666238, test loss: 0.8251351102132285\n",
      "Iteration 490 training loss: 0.8102787224788479, test loss: 0.8298047316112481\n",
      "Iteration 491 training loss: 0.8071049526452884, test loss: 0.8235908011695641\n",
      "Iteration 492 training loss: 0.808657400565934, test loss: 0.8282180683900804\n",
      "Iteration 493 training loss: 0.8055323897835, test loss: 0.8220599888677788\n",
      "Iteration 494 training loss: 0.807050497728493, test loss: 0.8266453308688275\n",
      "Iteration 495 training loss: 0.8039741719467778, test loss: 0.8205430542838367\n",
      "Iteration 496 training loss: 0.8054605428409052, test loss: 0.8250888149064476\n",
      "Iteration 497 training loss: 0.8024299407628098, test loss: 0.8190395784741344\n",
      "Iteration 498 training loss: 0.803884736011372, test loss: 0.8235459146538776\n",
      "Iteration 499 training loss: 0.8008989307550014, test loss: 0.8175489314164274\n",
      "Iteration 500 training loss: 0.8023232161524643, test loss: 0.8220170591004599\n",
      "Iteration 501 training loss: 0.7993809145666674, test loss: 0.8160708130222903\n",
      "Iteration 502 training loss: 0.8007751258555615, test loss: 0.8205010838627719\n",
      "Iteration 503 training loss: 0.7978757308072807, test loss: 0.8146051641077068\n",
      "Iteration 504 training loss: 0.7992417657249364, test loss: 0.8189992685471323\n",
      "Iteration 505 training loss: 0.7963844731890982, test loss: 0.8131530640228402\n",
      "Iteration 506 training loss: 0.7977224800242785, test loss: 0.8175109746007905\n",
      "Iteration 507 training loss: 0.7949060575439927, test loss: 0.811713312259056\n",
      "Iteration 508 training loss: 0.7962163498484878, test loss: 0.8160352628362316\n",
      "Iteration 509 training loss: 0.7934391949082716, test loss: 0.8102847621767975\n",
      "Iteration 510 training loss: 0.7947237076925315, test loss: 0.8145725044254377\n",
      "Iteration 511 training loss: 0.7919848340607251, test loss: 0.8088682983580234\n",
      "Iteration 512 training loss: 0.7932443201779379, test loss: 0.8131225595187842\n",
      "Iteration 513 training loss: 0.7905424793754466, test loss: 0.807463359838417\n",
      "Iteration 514 training loss: 0.791778438643777, test loss: 0.811685747417059\n",
      "Iteration 515 training loss: 0.7891132266633655, test loss: 0.8060711067287374\n",
      "Iteration 516 training loss: 0.7903267372967885, test loss: 0.8102627574280803\n",
      "Iteration 517 training loss: 0.7876967292156066, test loss: 0.8046911877239671\n",
      "Iteration 518 training loss: 0.7888876578950725, test loss: 0.8088520642938969\n",
      "Iteration 519 training loss: 0.7862914665032676, test loss: 0.8033222785727455\n",
      "Iteration 520 training loss: 0.7874604657997192, test loss: 0.8074526230040062\n",
      "Iteration 521 training loss: 0.7848978427406698, test loss: 0.8019643666955548\n",
      "Iteration 522 training loss: 0.7860468616010595, test loss: 0.8060663937842159\n",
      "Iteration 523 training loss: 0.7835162986181127, test loss: 0.8006181699486449\n",
      "Iteration 524 training loss: 0.7846459206628358, test loss: 0.8046924435396233\n",
      "Iteration 525 training loss: 0.7821465240287802, test loss: 0.7992834973501882\n",
      "Iteration 526 training loss: 0.7832584122753112, test loss: 0.8033316257183309\n",
      "Iteration 527 training loss: 0.7807884628873062, test loss: 0.7979601133403964\n",
      "Iteration 528 training loss: 0.7818823900173717, test loss: 0.8019818674474279\n",
      "Iteration 529 training loss: 0.7794413104331919, test loss: 0.7966472656327566\n",
      "Iteration 530 training loss: 0.780517601570382, test loss: 0.8006429421901299\n",
      "Iteration 531 training loss: 0.7781039056903843, test loss: 0.795343731535018\n",
      "Iteration 532 training loss: 0.7791638470298446, test loss: 0.7993146188792418\n",
      "Iteration 533 training loss: 0.7767776910609615, test loss: 0.7940509893690685\n",
      "Iteration 534 training loss: 0.7778212585113133, test loss: 0.7979971552526269\n",
      "Iteration 535 training loss: 0.7754616840397287, test loss: 0.792767952810522\n",
      "Iteration 536 training loss: 0.7764897786862277, test loss: 0.7966902131687292\n",
      "Iteration 537 training loss: 0.7741559570122734, test loss: 0.791494759364478\n",
      "Iteration 538 training loss: 0.7751699944705631, test loss: 0.7953946073089001\n",
      "Iteration 539 training loss: 0.7728603188624464, test loss: 0.7902313628063793\n",
      "Iteration 540 training loss: 0.773860642169721, test loss: 0.7941090888463145\n",
      "Iteration 541 training loss: 0.7715757622871116, test loss: 0.7889787136047919\n",
      "Iteration 542 training loss: 0.7725624943234882, test loss: 0.7928345302064632\n",
      "Iteration 543 training loss: 0.7703022199065835, test loss: 0.7877368997965131\n",
      "Iteration 544 training loss: 0.7712773255764707, test loss: 0.7915727845627573\n",
      "Iteration 545 training loss: 0.7690394488043321, test loss: 0.7865054056709874\n",
      "Iteration 546 training loss: 0.7700020921199929, test loss: 0.7903206134963952\n",
      "Iteration 547 training loss: 0.7677858711984564, test loss: 0.7852827758204077\n",
      "Iteration 548 training loss: 0.7687377958101523, test loss: 0.7890790617095154\n",
      "Iteration 549 training loss: 0.7665431497767641, test loss: 0.7840707307332919\n",
      "Iteration 550 training loss: 0.7674844798643777, test loss: 0.7878480731855115\n",
      "Iteration 551 training loss: 0.765310708369649, test loss: 0.7828685874712417\n",
      "Iteration 552 training loss: 0.76624293467617, test loss: 0.786628582764531\n",
      "Iteration 553 training loss: 0.7640884020433856, test loss: 0.7816761595336229\n",
      "Iteration 554 training loss: 0.7650108774499856, test loss: 0.7854181454980622\n",
      "Iteration 555 training loss: 0.7628749234517922, test loss: 0.7804920879927502\n",
      "Iteration 556 training loss: 0.7637870809831883, test loss: 0.7842155805748516\n",
      "Iteration 557 training loss: 0.761670128144579, test loss: 0.7793163296998502\n",
      "Iteration 558 training loss: 0.7625732888248389, test loss: 0.7830227395609544\n",
      "Iteration 559 training loss: 0.7604743751526694, test loss: 0.7781493520050542\n",
      "Iteration 560 training loss: 0.7613698361882808, test loss: 0.7818399771837684\n",
      "Iteration 561 training loss: 0.7592880934329977, test loss: 0.7769915453715223\n",
      "Iteration 562 training loss: 0.760175457345253, test loss: 0.7806660424922461\n",
      "Iteration 563 training loss: 0.7581107888907243, test loss: 0.7758423998068891\n",
      "Iteration 564 training loss: 0.7589913744356591, test loss: 0.7795022187402254\n",
      "Iteration 565 training loss: 0.7569425144071655, test loss: 0.7747021037100703\n",
      "Iteration 566 training loss: 0.757817304331265, test loss: 0.7783483094478697\n",
      "Iteration 567 training loss: 0.7557841750500234, test loss: 0.7735715137503832\n",
      "Iteration 568 training loss: 0.7566527260925824, test loss: 0.7772036114515803\n",
      "Iteration 569 training loss: 0.7546348795614299, test loss: 0.7724495545436404\n",
      "Iteration 570 training loss: 0.755497720929249, test loss: 0.7760683742990376\n",
      "Iteration 571 training loss: 0.7534935255042238, test loss: 0.7713351709486663\n",
      "Iteration 572 training loss: 0.7543504733261109, test loss: 0.7749407244489166\n",
      "Iteration 573 training loss: 0.7523605232266127, test loss: 0.770228942452789\n",
      "Iteration 574 training loss: 0.7532121097037343, test loss: 0.7738216800810503\n",
      "Iteration 575 training loss: 0.7512360098901167, test loss: 0.7691310175402807\n",
      "Iteration 576 training loss: 0.752082198306166, test loss: 0.7727108378756933\n",
      "Iteration 577 training loss: 0.7501189528831275, test loss: 0.7680402464392019\n",
      "Iteration 578 training loss: 0.7509600559913105, test loss: 0.7716073740783972\n",
      "Iteration 579 training loss: 0.7490097087089205, test loss: 0.7669570645427609\n",
      "Iteration 580 training loss: 0.7498465811799773, test loss: 0.770512609893451\n",
      "Iteration 581 training loss: 0.7479077560092354, test loss: 0.7658810343357347\n",
      "Iteration 582 training loss: 0.7487401686130105, test loss: 0.7694247688078172\n",
      "Iteration 583 training loss: 0.7468135972984484, test loss: 0.7648125926192944\n",
      "Iteration 584 training loss: 0.7476426008298371, test loss: 0.7683455702972087\n",
      "Iteration 585 training loss: 0.7457275709380198, test loss: 0.7637523012527554\n",
      "Iteration 586 training loss: 0.7465525520485755, test loss: 0.7672735876193756\n",
      "Iteration 587 training loss: 0.7446495041200462, test loss: 0.7626997591281129\n",
      "Iteration 588 training loss: 0.7454709301805951, test loss: 0.7662099661498479\n",
      "Iteration 589 training loss: 0.7435790355644047, test loss: 0.7616545957793658\n",
      "Iteration 590 training loss: 0.7443970954481293, test loss: 0.7651538654312788\n",
      "Iteration 591 training loss: 0.7425163146987902, test loss: 0.7606169199264481\n",
      "Iteration 592 training loss: 0.7433310609340174, test loss: 0.7641054333455006\n",
      "Iteration 593 training loss: 0.7414616330977793, test loss: 0.7595870912782986\n",
      "Iteration 594 training loss: 0.7422739040114855, test loss: 0.7630658161024121\n",
      "Iteration 595 training loss: 0.7404139753387, test loss: 0.7585641910745273\n",
      "Iteration 596 training loss: 0.7412237067550809, test loss: 0.7620329857307816\n",
      "Iteration 597 training loss: 0.7393733111946662, test loss: 0.757548229601102\n",
      "Iteration 598 training loss: 0.7401812817623318, test loss: 0.761007814408487\n",
      "Iteration 599 training loss: 0.7383408979729864, test loss: 0.7565403393604249\n",
      "Iteration 600 training loss: 0.7391470307063557, test loss: 0.7599906137407322\n",
      "Iteration 601 training loss: 0.7373158249039562, test loss: 0.755539711847945\n",
      "Iteration 602 training loss: 0.7381201144143131, test loss: 0.7589806235916239\n",
      "Iteration 603 training loss: 0.7362973976651739, test loss: 0.7545455547604086\n",
      "Iteration 604 training loss: 0.7370997793018468, test loss: 0.7579768527875332\n",
      "Iteration 605 training loss: 0.7352856112606853, test loss: 0.7535579265842252\n",
      "Iteration 606 training loss: 0.7360860836380625, test loss: 0.7569794306302771\n",
      "Iteration 607 training loss: 0.7342806694183193, test loss: 0.7525770520267597\n",
      "Iteration 608 training loss: 0.7350783386803793, test loss: 0.7559877681601168\n",
      "Iteration 609 training loss: 0.7332811068762404, test loss: 0.7516013845306891\n",
      "Iteration 610 training loss: 0.7340778344094171, test loss: 0.7550033653572996\n",
      "Iteration 611 training loss: 0.7322884873585238, test loss: 0.7506323987089749\n",
      "Iteration 612 training loss: 0.7330838710409381, test loss: 0.7540253854267012\n",
      "Iteration 613 training loss: 0.7313023902807678, test loss: 0.7496699540348545\n",
      "Iteration 614 training loss: 0.7320964808729394, test loss: 0.7530540147451026\n",
      "Iteration 615 training loss: 0.7303221334842847, test loss: 0.7487132695737292\n",
      "Iteration 616 training loss: 0.7311150818477471, test loss: 0.7520884720556087\n",
      "Iteration 617 training loss: 0.7293488425591377, test loss: 0.7477634222116433\n",
      "Iteration 618 training loss: 0.7301405885115986, test loss: 0.7511298029843182\n",
      "Iteration 619 training loss: 0.7283817828632876, test loss: 0.7468197378709256\n",
      "Iteration 620 training loss: 0.7291735080357954, test loss: 0.7501784906620941\n",
      "Iteration 621 training loss: 0.727422268196614, test loss: 0.7458834262921715\n",
      "Iteration 622 training loss: 0.7282132904760988, test loss: 0.7492339412175232\n",
      "Iteration 623 training loss: 0.7264688896779983, test loss: 0.7449531765784263\n",
      "Iteration 624 training loss: 0.7272589129716915, test loss: 0.7482951866719035\n",
      "Iteration 625 training loss: 0.7255214862946101, test loss: 0.744028840048374\n",
      "Iteration 626 training loss: 0.7263108607125518, test loss: 0.7473626806297503\n",
      "Iteration 627 training loss: 0.724580478825545, test loss: 0.7431108009363656\n",
      "Iteration 628 training loss: 0.7253690294999117, test loss: 0.7464362629087505\n",
      "Iteration 629 training loss: 0.7236452856183174, test loss: 0.7421985709800174\n",
      "Iteration 630 training loss: 0.7244330843380149, test loss: 0.7455157276458162\n",
      "Iteration 631 training loss: 0.7227150409098729, test loss: 0.741291291445561\n",
      "Iteration 632 training loss: 0.7235021984052536, test loss: 0.7446001922400064\n",
      "Iteration 633 training loss: 0.721791034394455, test loss: 0.7403901791485074\n",
      "Iteration 634 training loss: 0.7225775370939996, test loss: 0.7436908748138978\n",
      "Iteration 635 training loss: 0.7208727462411444, test loss: 0.7394947702881457\n",
      "Iteration 636 training loss: 0.7216589462694892, test loss: 0.742787458879667\n",
      "Iteration 637 training loss: 0.7199608463253131, test loss: 0.7386057913568684\n",
      "Iteration 638 training loss: 0.7207468297979033, test loss: 0.7418905464262261\n",
      "Iteration 639 training loss: 0.7190543403501822, test loss: 0.7377221289357092\n",
      "Iteration 640 training loss: 0.7198396488327955, test loss: 0.7409984838945612\n",
      "Iteration 641 training loss: 0.7181532998267155, test loss: 0.7368439634931372\n",
      "Iteration 642 training loss: 0.7189383930201035, test loss: 0.7401123220483917\n",
      "Iteration 643 training loss: 0.7172581607458435, test loss: 0.735971655314738\n",
      "Iteration 644 training loss: 0.7180423556224911, test loss: 0.739231408519473\n",
      "Iteration 645 training loss: 0.7163685791995132, test loss: 0.7351049444498502\n",
      "Iteration 646 training loss: 0.7171525442489197, test loss: 0.7383567107423925\n",
      "Iteration 647 training loss: 0.7154857687393593, test loss: 0.7342449465986022\n",
      "Iteration 648 training loss: 0.7162694791626637, test loss: 0.7374887911637589\n",
      "Iteration 649 training loss: 0.7146081701631706, test loss: 0.7333901490904547\n",
      "Iteration 650 training loss: 0.715392524356786, test loss: 0.7366269704864337\n",
      "Iteration 651 training loss: 0.7137366975991313, test loss: 0.7325414561261079\n",
      "Iteration 652 training loss: 0.7145213538339431, test loss: 0.735770914553614\n",
      "Iteration 653 training loss: 0.7128708816971195, test loss: 0.7316983816019377\n",
      "Iteration 654 training loss: 0.7136552981500557, test loss: 0.7349198693998293\n",
      "Iteration 655 training loss: 0.7120104370879922, test loss: 0.7308607316138677\n",
      "Iteration 656 training loss: 0.7127938384227873, test loss: 0.7340735479668823\n",
      "Iteration 657 training loss: 0.7111541826858652, test loss: 0.7300273588786352\n",
      "Iteration 658 training loss: 0.7119372336396751, test loss: 0.7332320001883376\n",
      "Iteration 659 training loss: 0.7103031273322221, test loss: 0.7291991744645125\n",
      "Iteration 660 training loss: 0.7110864252794595, test loss: 0.7323963627027443\n",
      "Iteration 661 training loss: 0.7094574535098543, test loss: 0.7283764274194644\n",
      "Iteration 662 training loss: 0.7102412335700135, test loss: 0.7315663632850082\n",
      "Iteration 663 training loss: 0.7086177573419813, test loss: 0.7275597226362367\n",
      "Iteration 664 training loss: 0.7094016411568386, test loss: 0.7307420361016683\n",
      "Iteration 665 training loss: 0.7077837217407427, test loss: 0.7267486992908673\n",
      "Iteration 666 training loss: 0.7085682427062042, test loss: 0.7299239622168294\n",
      "Iteration 667 training loss: 0.7069545552779373, test loss: 0.7259425543841\n",
      "Iteration 668 training loss: 0.7077391645851009, test loss: 0.7291101301928542\n",
      "Iteration 669 training loss: 0.7061303027340143, test loss: 0.7251413803161196\n",
      "Iteration 670 training loss: 0.7069147265573202, test loss: 0.7283010565117809\n",
      "Iteration 671 training loss: 0.7053100600762645, test loss: 0.7243443824210173\n",
      "Iteration 672 training loss: 0.7060941215422005, test loss: 0.7274957558867109\n",
      "Iteration 673 training loss: 0.7044946815847068, test loss: 0.7235523567903954\n",
      "Iteration 674 training loss: 0.7052778776717066, test loss: 0.7266948603718848\n",
      "Iteration 675 training loss: 0.7036829378926025, test loss: 0.7227639548106312\n",
      "Iteration 676 training loss: 0.704466073228086, test loss: 0.7258984562721673\n",
      "Iteration 677 training loss: 0.7028762190049826, test loss: 0.7219806695834061\n",
      "Iteration 678 training loss: 0.703658564120552, test loss: 0.725106281520773\n",
      "Iteration 679 training loss: 0.7020732763200935, test loss: 0.7212011839209432\n",
      "Iteration 680 training loss: 0.7028553821541934, test loss: 0.7243184750026385\n",
      "Iteration 681 training loss: 0.7012751160200243, test loss: 0.720426623408265\n",
      "Iteration 682 training loss: 0.7020562749522641, test loss: 0.7235347720639689\n",
      "Iteration 683 training loss: 0.7004813861952104, test loss: 0.7196566938930803\n",
      "Iteration 684 training loss: 0.7012630776991198, test loss: 0.7227571219999362\n",
      "Iteration 685 training loss: 0.6996937587411602, test loss: 0.7188929527190163\n",
      "Iteration 686 training loss: 0.7004755430268097, test loss: 0.7219852327901178\n",
      "Iteration 687 training loss: 0.6989110702740868, test loss: 0.7181342252204015\n",
      "Iteration 688 training loss: 0.6996923488294903, test loss: 0.7212176101363481\n",
      "Iteration 689 training loss: 0.6981330197973373, test loss: 0.7173802028810927\n",
      "Iteration 690 training loss: 0.6989138885933003, test loss: 0.7204547317637355\n",
      "Iteration 691 training loss: 0.6973593027934836, test loss: 0.716630548646918\n",
      "Iteration 692 training loss: 0.6981406191926502, test loss: 0.7196970673221535\n",
      "Iteration 693 training loss: 0.696591212649739, test loss: 0.7158864899683213\n",
      "Iteration 694 training loss: 0.6973723424994158, test loss: 0.7189444228217285\n",
      "Iteration 695 training loss: 0.6958275839479356, test loss: 0.7151470002387255\n",
      "Iteration 696 training loss: 0.6966087744397408, test loss: 0.7181965682933944\n",
      "Iteration 697 training loss: 0.6950682366589218, test loss: 0.7144118925223666\n",
      "Iteration 698 training loss: 0.6958496567740589, test loss: 0.7174531672826302\n",
      "Iteration 699 training loss: 0.6943138942535808, test loss: 0.7136817945378015\n",
      "Iteration 700 training loss: 0.6950949347867924, test loss: 0.7167141551643801\n",
      "Iteration 701 training loss: 0.693563742010771, test loss: 0.7129558653806208\n",
      "Iteration 702 training loss: 0.6943443414868458, test loss: 0.7159792681124132\n",
      "Iteration 703 training loss: 0.6928182418580893, test loss: 0.7122345155942226\n",
      "Iteration 704 training loss: 0.6935987092775686, test loss: 0.7152494279570502\n",
      "Iteration 705 training loss: 0.6920768114281722, test loss: 0.7115171880171892\n",
      "Iteration 706 training loss: 0.6928564436667533, test loss: 0.7145229679766463\n",
      "Iteration 707 training loss: 0.6913390689315987, test loss: 0.7108036602830973\n",
      "Iteration 708 training loss: 0.6921182658345302, test loss: 0.7138006599672948\n",
      "Iteration 709 training loss: 0.690605481236498, test loss: 0.7100943551725402\n",
      "Iteration 710 training loss: 0.6913839687562819, test loss: 0.7130822534170613\n",
      "Iteration 711 training loss: 0.6898753742383286, test loss: 0.7093886211288989\n",
      "Iteration 712 training loss: 0.6906533112945501, test loss: 0.7123675619846377\n",
      "Iteration 713 training loss: 0.6891497579434841, test loss: 0.7086873567833085\n",
      "Iteration 714 training loss: 0.6899261493487401, test loss: 0.7116563237799995\n",
      "Iteration 715 training loss: 0.6884275238346812, test loss: 0.7079895280899763\n",
      "Iteration 716 training loss: 0.6892031824494611, test loss: 0.7109494901878873\n",
      "Iteration 717 training loss: 0.6877099180632437, test loss: 0.7072964023668675\n",
      "Iteration 718 training loss: 0.6884854069241401, test loss: 0.7102478961061143\n",
      "Iteration 719 training loss: 0.6869969430288223, test loss: 0.7066078763526569\n",
      "Iteration 720 training loss: 0.6877715461062915, test loss: 0.7095501796426722\n",
      "Iteration 721 training loss: 0.6862874057387157, test loss: 0.7059228657470218\n",
      "Iteration 722 training loss: 0.6870620321469265, test loss: 0.7088567695338399\n",
      "Iteration 723 training loss: 0.6855824369557568, test loss: 0.7052424464510294\n",
      "Iteration 724 training loss: 0.686355866594679, test loss: 0.7081666262841524\n",
      "Iteration 725 training loss: 0.6848797344224973, test loss: 0.704564363917532\n",
      "Iteration 726 training loss: 0.6856519126724154, test loss: 0.707478636378361\n",
      "Iteration 727 training loss: 0.6841810645579746, test loss: 0.7038903686492373\n",
      "Iteration 728 training loss: 0.6849521060380288, test loss: 0.7067949740025206\n",
      "Iteration 729 training loss: 0.6834866291351946, test loss: 0.7032207720759835\n",
      "Iteration 730 training loss: 0.6842565165754906, test loss: 0.7061156367128404\n",
      "Iteration 731 training loss: 0.6827961183645092, test loss: 0.7025551397556061\n",
      "Iteration 732 training loss: 0.6835646524952903, test loss: 0.7054401182131965\n",
      "Iteration 733 training loss: 0.6821091914438795, test loss: 0.7018932144831649\n",
      "Iteration 734 training loss: 0.6828767820136051, test loss: 0.7047687257182653\n",
      "Iteration 735 training loss: 0.6814261321048184, test loss: 0.7012352788933905\n",
      "Iteration 736 training loss: 0.6821926650131458, test loss: 0.7041011869855506\n",
      "Iteration 737 training loss: 0.6807466378658287, test loss: 0.7005809376171015\n",
      "Iteration 738 training loss: 0.681512154918029, test loss: 0.7034372700388299\n",
      "Iteration 739 training loss: 0.6800709856833699, test loss: 0.6999304107237053\n",
      "Iteration 740 training loss: 0.6808349864506859, test loss: 0.7027766481975491\n",
      "Iteration 741 training loss: 0.6793989704087853, test loss: 0.6992836821891951\n",
      "Iteration 742 training loss: 0.680160763358982, test loss: 0.7021190535202795\n",
      "Iteration 743 training loss: 0.6787292660359124, test loss: 0.6986393425255363\n",
      "Iteration 744 training loss: 0.6794894600964474, test loss: 0.701464392967804\n",
      "Iteration 745 training loss: 0.6780628325380206, test loss: 0.6979983638411662\n",
      "Iteration 746 training loss: 0.6788213988077777, test loss: 0.7008129634772384\n",
      "Iteration 747 training loss: 0.6774001213696244, test loss: 0.6973611382761772\n",
      "Iteration 748 training loss: 0.6781569691454613, test loss: 0.7001653496124359\n",
      "Iteration 749 training loss: 0.6767409876682632, test loss: 0.6967275536745344\n",
      "Iteration 750 training loss: 0.6774958972383534, test loss: 0.6995211021717951\n",
      "Iteration 751 training loss: 0.6760848264332476, test loss: 0.6960969391595493\n",
      "Iteration 752 training loss: 0.6768386099329965, test loss: 0.6988806835789471\n",
      "Iteration 753 training loss: 0.6754336428384625, test loss: 0.6954715455903835\n",
      "Iteration 754 training loss: 0.676186652813002, test loss: 0.6982457880009284\n",
      "Iteration 755 training loss: 0.6747862179048234, test loss: 0.6948499035965618\n",
      "Iteration 756 training loss: 0.6755376678527705, test loss: 0.6976138949794659\n",
      "Iteration 757 training loss: 0.6741420936239902, test loss: 0.6942316142997622\n",
      "Iteration 758 training loss: 0.6748915727049917, test loss: 0.6969849988207083\n",
      "Iteration 759 training loss: 0.6735010433448938, test loss: 0.6936165237252963\n",
      "Iteration 760 training loss: 0.674248708645946, test loss: 0.6963594341279031\n",
      "Iteration 761 training loss: 0.6728628995356992, test loss: 0.6930044002072588\n",
      "Iteration 762 training loss: 0.6736085984401645, test loss: 0.6957365469489499\n",
      "Iteration 763 training loss: 0.672228328893605, test loss: 0.6923958379156973\n",
      "Iteration 764 training loss: 0.6729719721247012, test loss: 0.6951171426280041\n",
      "Iteration 765 training loss: 0.6715967631267618, test loss: 0.6917903521345244\n",
      "Iteration 766 training loss: 0.6723383951029266, test loss: 0.6945008380942204\n",
      "Iteration 767 training loss: 0.6709686818901521, test loss: 0.6911884847725814\n",
      "Iteration 768 training loss: 0.6717085257883563, test loss: 0.6938884196895022\n",
      "Iteration 769 training loss: 0.6703431633905204, test loss: 0.6905891295480578\n",
      "Iteration 770 training loss: 0.6710810384336169, test loss: 0.6932783889344403\n",
      "Iteration 771 training loss: 0.6697205965692838, test loss: 0.689992724015825\n",
      "Iteration 772 training loss: 0.6704571480628789, test loss: 0.692672106500047\n",
      "Iteration 773 training loss: 0.669101579440796, test loss: 0.689399923076197\n",
      "Iteration 774 training loss: 0.6698365772560805, test loss: 0.6920692645922494\n",
      "Iteration 775 training loss: 0.6684859076578367, test loss: 0.6888105505359894\n",
      "Iteration 776 training loss: 0.6692195609236956, test loss: 0.691469990704862\n",
      "Iteration 777 training loss: 0.6678737069654942, test loss: 0.6882247068041284\n",
      "Iteration 778 training loss: 0.668605948511587, test loss: 0.6908743027311428\n",
      "Iteration 779 training loss: 0.6672655082900016, test loss: 0.6876429664475389\n",
      "Iteration 780 training loss: 0.6679959089512982, test loss: 0.690282289953388\n",
      "Iteration 781 training loss: 0.6666603433556803, test loss: 0.6870643159746851\n",
      "Iteration 782 training loss: 0.6673883622818486, test loss: 0.6896928764839813\n",
      "Iteration 783 training loss: 0.6660577797166422, test loss: 0.686488413186346\n",
      "Iteration 784 training loss: 0.6667839049645943, test loss: 0.6891065500256507\n",
      "Iteration 785 training loss: 0.6654576391564294, test loss: 0.6859149306209308\n",
      "Iteration 786 training loss: 0.6661813923673778, test loss: 0.6885221049115624\n",
      "Iteration 787 training loss: 0.6648594425700226, test loss: 0.6853434398875129\n",
      "Iteration 788 training loss: 0.6655811264373903, test loss: 0.6879399426013498\n",
      "Iteration 789 training loss: 0.6642644945504814, test loss: 0.6847751929891968\n",
      "Iteration 790 training loss: 0.6649835460972546, test loss: 0.6873604587626545\n",
      "Iteration 791 training loss: 0.6636718753171966, test loss: 0.6842092717657652\n",
      "Iteration 792 training loss: 0.66438874144338, test loss: 0.6867837957332403\n",
      "Iteration 793 training loss: 0.663082212181561, test loss: 0.6836464676594377\n",
      "Iteration 794 training loss: 0.6637964135377619, test loss: 0.6862098297009653\n",
      "Iteration 795 training loss: 0.6624952033253181, test loss: 0.6830864308259328\n",
      "Iteration 796 training loss: 0.6632070217895707, test loss: 0.6856389314968921\n",
      "Iteration 797 training loss: 0.6619109078123696, test loss: 0.6825292014032212\n",
      "Iteration 798 training loss: 0.6626200991569108, test loss: 0.6850705878044006\n",
      "Iteration 799 training loss: 0.6613290756037224, test loss: 0.6819744955965906\n",
      "Iteration 800 training loss: 0.6620355136356881, test loss: 0.684504582442186\n",
      "Iteration 801 training loss: 0.6607493998662934, test loss: 0.6814220464401319\n",
      "Iteration 802 training loss: 0.6614532601330027, test loss: 0.683940982523083\n",
      "Iteration 803 training loss: 0.6601723165317347, test loss: 0.6808722226371059\n",
      "Iteration 804 training loss: 0.6608732478022464, test loss: 0.6833796418009908\n",
      "Iteration 805 training loss: 0.6595971602684612, test loss: 0.6803243666339834\n",
      "Iteration 806 training loss: 0.6602946165146525, test loss: 0.6828196983180702\n",
      "Iteration 807 training loss: 0.6590238776053053, test loss: 0.6797784617122874\n",
      "Iteration 808 training loss: 0.6597184792104602, test loss: 0.6822623328869218\n",
      "Iteration 809 training loss: 0.6584533084217944, test loss: 0.6792353399322735\n",
      "Iteration 810 training loss: 0.6591454918864758, test loss: 0.6817082484263594\n",
      "Iteration 811 training loss: 0.6578859590993805, test loss: 0.6786954584922473\n",
      "Iteration 812 training loss: 0.6585748793105116, test loss: 0.6811565767101214\n",
      "Iteration 813 training loss: 0.657321104831089, test loss: 0.6781580712208056\n",
      "Iteration 814 training loss: 0.6580080043362772, test loss: 0.6806087368312244\n",
      "Iteration 815 training loss: 0.6567589525829025, test loss: 0.6776233920235526\n",
      "Iteration 816 training loss: 0.6574423842881626, test loss: 0.6800621498547127\n",
      "Iteration 817 training loss: 0.6561989245365109, test loss: 0.6770910687702973\n",
      "Iteration 818 training loss: 0.6568794387466371, test loss: 0.6795183589908969\n",
      "Iteration 819 training loss: 0.6556419216105744, test loss: 0.6765618634484257\n",
      "Iteration 820 training loss: 0.6563201022046777, test loss: 0.6789782984228631\n",
      "Iteration 821 training loss: 0.655087852547147, test loss: 0.6760356918907025\n",
      "Iteration 822 training loss: 0.6557632739556561, test loss: 0.6784407839995823\n",
      "Iteration 823 training loss: 0.6545361385900637, test loss: 0.675511799897218\n",
      "Iteration 824 training loss: 0.6552083343230694, test loss: 0.6779051682452245\n",
      "Iteration 825 training loss: 0.6539868538111345, test loss: 0.6749905443761389\n",
      "Iteration 826 training loss: 0.6546556190504529, test loss: 0.6773718348714088\n",
      "Iteration 827 training loss: 0.6534398593762647, test loss: 0.6744715436320863\n",
      "Iteration 828 training loss: 0.6541058525719594, test loss: 0.6768414684320476\n",
      "Iteration 829 training loss: 0.6528953679783696, test loss: 0.6739551952457056\n",
      "Iteration 830 training loss: 0.6535584066240498, test loss: 0.6763136095446959\n",
      "Iteration 831 training loss: 0.6523527842155661, test loss: 0.6734406513768618\n",
      "Iteration 832 training loss: 0.6530126460204955, test loss: 0.6757874387447621\n",
      "Iteration 833 training loss: 0.6518121746146134, test loss: 0.6729281327336067\n",
      "Iteration 834 training loss: 0.6524689021919966, test loss: 0.6752633073107032\n",
      "Iteration 835 training loss: 0.6512734739140916, test loss: 0.6724174988864877\n",
      "Iteration 836 training loss: 0.6519273803875416, test loss: 0.6747415115044537\n",
      "Iteration 837 training loss: 0.6507368893026048, test loss: 0.6719090425826922\n",
      "Iteration 838 training loss: 0.6513877627772495, test loss: 0.6742215907728762\n",
      "Iteration 839 training loss: 0.6502018323005487, test loss: 0.6714021244990243\n",
      "Iteration 840 training loss: 0.6508499981929675, test loss: 0.6737035863415264\n",
      "Iteration 841 training loss: 0.6496692028303659, test loss: 0.6708977301277256\n",
      "Iteration 842 training loss: 0.6503138553483739, test loss: 0.6731871592262202\n",
      "Iteration 843 training loss: 0.6491381852330458, test loss: 0.6703949566340167\n",
      "Iteration 844 training loss: 0.6497796623327836, test loss: 0.6726728183443007\n",
      "Iteration 845 training loss: 0.6486089169051452, test loss: 0.669893919066398\n",
      "Iteration 846 training loss: 0.6492471250427602, test loss: 0.6721601794255645\n",
      "Iteration 847 training loss: 0.6480814373378846, test loss: 0.6693948031607011\n",
      "Iteration 848 training loss: 0.6487162924483449, test loss: 0.6716493134084163\n",
      "Iteration 849 training loss: 0.6475559127387072, test loss: 0.668897646978053\n",
      "Iteration 850 training loss: 0.6481868224891051, test loss: 0.6711399014805628\n",
      "Iteration 851 training loss: 0.6470315600162121, test loss: 0.6684017284216361\n",
      "Iteration 852 training loss: 0.647658776662794, test loss: 0.6706319700257991\n",
      "Iteration 853 training loss: 0.6465093482504431, test loss: 0.6679079557852236\n",
      "Iteration 854 training loss: 0.6471338617233953, test loss: 0.670127203054537\n",
      "Iteration 855 training loss: 0.6459898212884223, test loss: 0.6674168776360853\n",
      "Iteration 856 training loss: 0.6466106782999202, test loss: 0.6696241569780719\n",
      "Iteration 857 training loss: 0.6454719059963935, test loss: 0.6669273621234311\n",
      "Iteration 858 training loss: 0.6460887012525481, test loss: 0.6691222745159568\n",
      "Iteration 859 training loss: 0.6449551147289859, test loss: 0.6664390687183678\n",
      "Iteration 860 training loss: 0.6455675059365491, test loss: 0.6686212123140608\n",
      "Iteration 861 training loss: 0.6444394039446455, test loss: 0.6659519713385625\n",
      "Iteration 862 training loss: 0.6450484255722814, test loss: 0.6681223464952135\n",
      "Iteration 863 training loss: 0.6439268262873062, test loss: 0.6654680328128462\n",
      "Iteration 864 training loss: 0.6445322010708184, test loss: 0.66762642588611\n",
      "Iteration 865 training loss: 0.643416469922216, test loss: 0.6649863324739468\n",
      "Iteration 866 training loss: 0.6440183985722118, test loss: 0.6671329424879952\n",
      "Iteration 867 training loss: 0.6429080758597292, test loss: 0.6645065291937355\n",
      "Iteration 868 training loss: 0.6435066623548596, test loss: 0.6666415565791479\n",
      "Iteration 869 training loss: 0.64240206416739, test loss: 0.6640291656633093\n",
      "Iteration 870 training loss: 0.6429970719902213, test loss: 0.6661523954739501\n",
      "Iteration 871 training loss: 0.6418987955690962, test loss: 0.6635545680394489\n",
      "Iteration 872 training loss: 0.6424905706223029, test loss: 0.6656664209783864\n",
      "Iteration 873 training loss: 0.6413971169067499, test loss: 0.6630815628925905\n",
      "Iteration 874 training loss: 0.641985870337617, test loss: 0.665182310192078\n",
      "Iteration 875 training loss: 0.6408980685158229, test loss: 0.6626112180610467\n",
      "Iteration 876 training loss: 0.6414839493499345, test loss: 0.6647012065795197\n",
      "Iteration 877 training loss: 0.6404013534917068, test loss: 0.6621431469898852\n",
      "Iteration 878 training loss: 0.6409838190296278, test loss: 0.6642218949556903\n",
      "Iteration 879 training loss: 0.6399062921125555, test loss: 0.6616767825705759\n",
      "Iteration 880 training loss: 0.6404859342612128, test loss: 0.6637449401197957\n",
      "Iteration 881 training loss: 0.6394134149606197, test loss: 0.6612125974283375\n",
      "Iteration 882 training loss: 0.6399897648471355, test loss: 0.663269693832055\n",
      "Iteration 883 training loss: 0.6389215969168953, test loss: 0.6607494489918588\n",
      "Iteration 884 training loss: 0.6394940947852424, test loss: 0.6627948918064616\n",
      "Iteration 885 training loss: 0.6384310485604259, test loss: 0.6602876200264282\n",
      "Iteration 886 training loss: 0.6389999924932831, test loss: 0.6623216530239413\n",
      "Iteration 887 training loss: 0.637941704716272, test loss: 0.6598269613695757\n",
      "Iteration 888 training loss: 0.6385071364350607, test loss: 0.6618496978846977\n",
      "Iteration 889 training loss: 0.637453885472265, test loss: 0.6593678147795377\n",
      "Iteration 890 training loss: 0.638015647186305, test loss: 0.6613791420440595\n",
      "Iteration 891 training loss: 0.6369679132647746, test loss: 0.6589105443044353\n",
      "Iteration 892 training loss: 0.6375258458708573, test loss: 0.6609103443607465\n",
      "Iteration 893 training loss: 0.6364835130512236, test loss: 0.6584548463000932\n",
      "Iteration 894 training loss: 0.6370376550746356, test loss: 0.6604432094940138\n",
      "Iteration 895 training loss: 0.6360003156899591, test loss: 0.6580003176564125\n",
      "Iteration 896 training loss: 0.636551254835222, test loss: 0.6599778907187275\n",
      "Iteration 897 training loss: 0.6355187351377345, test loss: 0.6575473822783557\n",
      "Iteration 898 training loss: 0.6360663248366908, test loss: 0.6595140959145223\n",
      "Iteration 899 training loss: 0.6350386296104759, test loss: 0.6570959464851515\n",
      "Iteration 900 training loss: 0.6355819033098258, test loss: 0.6590508519385412\n",
      "Iteration 901 training loss: 0.6345594712707029, test loss: 0.6566454804540228\n",
      "Iteration 902 training loss: 0.6350997661596421, test loss: 0.6585899995593644\n",
      "Iteration 903 training loss: 0.6340824453291937, test loss: 0.6561972311913565\n",
      "Iteration 904 training loss: 0.6346191467714543, test loss: 0.658130719469913\n",
      "Iteration 905 training loss: 0.6336071014636488, test loss: 0.6557506750388202\n",
      "Iteration 906 training loss: 0.6341396806751431, test loss: 0.6576725515882645\n",
      "Iteration 907 training loss: 0.6331334350714316, test loss: 0.6553058897956825\n",
      "Iteration 908 training loss: 0.6336622744683403, test loss: 0.6572165882154857\n",
      "Iteration 909 training loss: 0.632661008162625, test loss: 0.6548622798814455\n",
      "Iteration 910 training loss: 0.6331865653912019, test loss: 0.656762364766175\n",
      "Iteration 911 training loss: 0.6321900467078042, test loss: 0.6544201343334367\n",
      "Iteration 912 training loss: 0.6327119915948303, test loss: 0.6563092903846698\n",
      "Iteration 913 training loss: 0.6317201913585385, test loss: 0.6539791208522286\n",
      "Iteration 914 training loss: 0.6322385257759795, test loss: 0.6558573296592797\n",
      "Iteration 915 training loss: 0.631251988327243, test loss: 0.6535398498980274\n",
      "Iteration 916 training loss: 0.6317667182053985, test loss: 0.6554071461199905\n",
      "Iteration 917 training loss: 0.6307858791904074, test loss: 0.653102847682982\n",
      "Iteration 918 training loss: 0.6312971725366505, test loss: 0.6549593745327092\n",
      "Iteration 919 training loss: 0.6303209155524994, test loss: 0.6526669503486972\n",
      "Iteration 920 training loss: 0.6308291778470605, test loss: 0.6545131363169601\n",
      "Iteration 921 training loss: 0.629857016422638, test loss: 0.6522320796309735\n",
      "Iteration 922 training loss: 0.6303616154944928, test loss: 0.6540673047536174\n",
      "Iteration 923 training loss: 0.6293941474999903, test loss: 0.6517981535721065\n",
      "Iteration 924 training loss: 0.6298960356787016, test loss: 0.6536235000727093\n",
      "Iteration 925 training loss: 0.628933286312561, test loss: 0.6513662516472006\n",
      "Iteration 926 training loss: 0.6294316530171865, test loss: 0.6531809985065319\n",
      "Iteration 927 training loss: 0.6284740888298361, test loss: 0.6509360361639331\n",
      "Iteration 928 training loss: 0.6289690734012601, test loss: 0.6527403545732096\n",
      "Iteration 929 training loss: 0.6280155874056046, test loss: 0.6505065660901559\n",
      "Iteration 930 training loss: 0.6285068992920594, test loss: 0.652300142861028\n",
      "Iteration 931 training loss: 0.6275579005930836, test loss: 0.6500779409375468\n",
      "Iteration 932 training loss: 0.6280464596462183, test loss: 0.6518617718014353\n",
      "Iteration 933 training loss: 0.6271018935549376, test loss: 0.649651013287393\n",
      "Iteration 934 training loss: 0.6275868597419865, test loss: 0.6514242576235176\n",
      "Iteration 935 training loss: 0.6266464694431476, test loss: 0.6492246281519982\n",
      "Iteration 936 training loss: 0.6271278985322839, test loss: 0.6509874325311201\n",
      "Iteration 937 training loss: 0.6261919829905656, test loss: 0.6487990741817026\n",
      "Iteration 938 training loss: 0.6266700213862018, test loss: 0.6505516586415457\n",
      "Iteration 939 training loss: 0.6257384119631951, test loss: 0.6483744669767739\n",
      "Iteration 940 training loss: 0.626212981268529, test loss: 0.6501167666737139\n",
      "Iteration 941 training loss: 0.62528629567547, test loss: 0.647951382171033\n",
      "Iteration 942 training loss: 0.6257570511868709, test loss: 0.649683039370655\n",
      "Iteration 943 training loss: 0.6248349413207823, test loss: 0.647529112878745\n",
      "Iteration 944 training loss: 0.6253024528342531, test loss: 0.6492506866700409\n",
      "Iteration 945 training loss: 0.6243851619190313, test loss: 0.6471084074031371\n",
      "Iteration 946 training loss: 0.6248493273788589, test loss: 0.6488198242135202\n",
      "Iteration 947 training loss: 0.6239369872445129, test loss: 0.6466892815416418\n",
      "Iteration 948 training loss: 0.624398017133911, test loss: 0.6483907927193839\n",
      "Iteration 949 training loss: 0.6234900434087035, test loss: 0.6462713970443554\n",
      "Iteration 950 training loss: 0.6239472926166273, test loss: 0.6479623935526543\n",
      "Iteration 951 training loss: 0.6230438105524871, test loss: 0.6458542009262109\n",
      "Iteration 952 training loss: 0.6234974217700305, test loss: 0.6475347700204874\n",
      "Iteration 953 training loss: 0.6225987061130902, test loss: 0.6454380985757626\n",
      "Iteration 954 training loss: 0.6230491977915643, test loss: 0.6471089359814743\n",
      "Iteration 955 training loss: 0.622155229109681, test loss: 0.6450236722621702\n",
      "Iteration 956 training loss: 0.6226027971207982, test loss: 0.6466850029003678\n",
      "Iteration 957 training loss: 0.6217123440016488, test loss: 0.6446098246899752\n",
      "Iteration 958 training loss: 0.6221562125569471, test loss: 0.6462608485909733\n",
      "Iteration 959 training loss: 0.6212705784303718, test loss: 0.6441970586544554\n",
      "Iteration 960 training loss: 0.6217108886433581, test loss: 0.6458379553977929\n",
      "Iteration 961 training loss: 0.6208294971788877, test loss: 0.6437850110170088\n",
      "Iteration 962 training loss: 0.621266023026594, test loss: 0.6454156698949862\n",
      "Iteration 963 training loss: 0.6203890742296558, test loss: 0.6433737030608818\n",
      "Iteration 964 training loss: 0.6208220518373527, test loss: 0.6449943710227833\n",
      "Iteration 965 training loss: 0.6199493093996585, test loss: 0.6429631063177953\n",
      "Iteration 966 training loss: 0.620379194654672, test loss: 0.6445741302386581\n",
      "Iteration 967 training loss: 0.6195109213680172, test loss: 0.6425538372733923\n",
      "Iteration 968 training loss: 0.6199373201485254, test loss: 0.6441548980721234\n",
      "Iteration 969 training loss: 0.6190727345367051, test loss: 0.6421448182312198\n",
      "Iteration 970 training loss: 0.6194953207334438, test loss: 0.6437355461949755\n",
      "Iteration 971 training loss: 0.6186351014649288, test loss: 0.6417363593089016\n",
      "Iteration 972 training loss: 0.6190539357730585, test loss: 0.6433168039590559\n",
      "Iteration 973 training loss: 0.6181984082296186, test loss: 0.6413288597992041\n",
      "Iteration 974 training loss: 0.6186133837203808, test loss: 0.6428989720455269\n",
      "Iteration 975 training loss: 0.6177624977585362, test loss: 0.6409222394867707\n",
      "Iteration 976 training loss: 0.6181741529347607, test loss: 0.6424824361856778\n",
      "Iteration 977 training loss: 0.6173280352692693, test loss: 0.640517000807611\n",
      "Iteration 978 training loss: 0.6177362784127994, test loss: 0.6420672720605908\n",
      "Iteration 979 training loss: 0.6168944823392924, test loss: 0.6401126072921013\n",
      "Iteration 980 training loss: 0.6172988443995298, test loss: 0.6416526160511975\n",
      "Iteration 981 training loss: 0.616461804840271, test loss: 0.6397091471939444\n",
      "Iteration 982 training loss: 0.6168627339912063, test loss: 0.6412392980627337\n",
      "Iteration 983 training loss: 0.616030373853223, test loss: 0.6393068990607619\n",
      "Iteration 984 training loss: 0.616428119478855, test loss: 0.6408275174968447\n",
      "Iteration 985 training loss: 0.6155996235111704, test loss: 0.6389052652123608\n",
      "Iteration 986 training loss: 0.6159941389389461, test loss: 0.6404163657353107\n",
      "Iteration 987 training loss: 0.6151698209283322, test loss: 0.6385044986472838\n",
      "Iteration 988 training loss: 0.6155613132216905, test loss: 0.6400064553455874\n",
      "Iteration 989 training loss: 0.6147416491302652, test loss: 0.6381054497662431\n",
      "Iteration 990 training loss: 0.6151299170907402, test loss: 0.6395980711959666\n",
      "Iteration 991 training loss: 0.6143154048988472, test loss: 0.6377082867426047\n",
      "Iteration 992 training loss: 0.6146998089553255, test loss: 0.6391910131685862\n",
      "Iteration 993 training loss: 0.6138896931634451, test loss: 0.6373116542495362\n",
      "Iteration 994 training loss: 0.6142701788157103, test loss: 0.6387844794875462\n",
      "Iteration 995 training loss: 0.6134644214000139, test loss: 0.6369154369969509\n",
      "Iteration 996 training loss: 0.613841513752309, test loss: 0.6383789452071051\n",
      "Iteration 997 training loss: 0.6130398780313048, test loss: 0.6365199055624946\n",
      "Iteration 998 training loss: 0.6134138801385679, test loss: 0.6379744502312956\n",
      "Iteration 999 training loss: 0.6126157664987668, test loss: 0.6361248185834273\n",
      "Iteration 1000 training loss: 0.612986816451013, test loss: 0.6375705251198159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb993b2edd0>,\n",
       " <matplotlib.lines.Line2D at 0x7fb993b2eec0>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer_sizes = [feature_train.shape[1], 128, 10]\n",
    "params = init_params(layer_sizes)\n",
    "num_iters = 1000\n",
    "learning_rate = 1e-6\n",
    "losses_train, losses_test = [], []\n",
    "# Optimization loop\n",
    "for i in range(num_iters):\n",
    "    pred_train, cache = forward(feature_train, params)\n",
    "    pred_test, _ = forward(feature_test, params)\n",
    "    grads = grad(pred_train, target_train, params, cache)\n",
    "    loss_train = cross_entropy_loss_fn(pred_train, target_train)\n",
    "    loss_test = cross_entropy_loss_fn(pred_test, target_test)\n",
    "    for j in range(len(layer_sizes) -1):\n",
    "        params['W'+str(j+1)] = params['W'+str(j+1)] - learning_rate * grads['dW'+str(j+1)]\n",
    "        params['b'+str(j+1)] = params['b'+str(j+1)] - learning_rate * grads['db'+str(j+1)]\n",
    "    print(f\"Iteration {i+1} training loss: {loss_train}, test loss: {loss_test}\")\n",
    "    losses_train.append(loss_train)\n",
    "    losses_test.append(loss_test)\n",
    "\n",
    "plt.plot(range(num_iters), losses_train, 'b--', range(num_iters), losses_test, 'r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Congrats on finishing this assignment! "
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
