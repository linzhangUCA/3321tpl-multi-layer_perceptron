{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron Neural Network\n",
    "\n",
    "Welcome to your third assignment. You will build a Multi-Layer Perceptron (MLP) neural network in this assignment. The goal of building such a model is to classify images of wearings.\n",
    "\n",
    "## Exercises:\n",
    "1. $\\color{violet}{\\textbf{(10\\%) Data Preprocessing}}$\n",
    "2. $\\color{violet}{\\textbf{(5\\%) Parameter Initialization}}$\n",
    "3. $\\color{violet}{\\textbf{(15\\%) Linear Model and Activations}}$\n",
    "4. $\\color{violet}{\\textbf{(5\\%) Cross Entropy Loss}}$\n",
    "5. $\\color{violet}{\\textbf{(25\\%) Gradient Computation}}$\n",
    "6. $\\color{violet}{\\textbf{(30\\%) Gradient Descent Optimization}}$\n",
    "7. $\\color{violet}{\\textbf{(10\\%) Accuracy Evaluation}}$\n",
    "\n",
    "## Instructions:\n",
    "- Write your code only between the $\\color{green}{\\textbf{\\small \\#\\#\\# START CODE HERE \\#\\#\\#}}$ and $\\color{green}{\\textbf{\\small \\#\\#\\# END CODE HERE \\#\\#\\#}}$ commented lines. \n",
    "- $\\color{red}{\\textbf{Change code out of the designated area at your own risk.}}$\n",
    "- Reference answers are provided after a certain coding blocks. Be aware if your answer is different from the reference..\n",
    "- **Need to install [Torchvision](https://pytorch.org/vision/stable/index.html)**\n",
    "    ```console\n",
    "    pip install torchvision\n",
    "    ```\n",
    "**You will learn:**\n",
    "- One-hot encoding for multi-class targets.\n",
    "- Rectified Linear Unit (ReLU) activation function.\n",
    "- Softmax activation.\n",
    "- Forward and backward propagation of a generic MLP model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset\n",
    "Torchvision provides a descent pool of datasets. We'll load one of the built-in dataset, [FashionMNIST](https://pytorch.org/vision/stable/datasets.html) to investigate multi-class classification using a generic Multi-Layer Perceptron (MLP) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "training features array shape: (60000, 28, 28), test features array shape: (10000, 28, 28)\n",
      "training labels array shape: (60000,), test labels array shape: (10000,)\n",
      "{'T-shirt/top': 0, 'Trouser': 1, 'Pullover': 2, 'Dress': 3, 'Coat': 4, 'Sandal': 5, 'Shirt': 6, 'Sneaker': 7, 'Bag': 8, 'Ankle boot': 9}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "train_set = datasets.FashionMNIST('./data', train=True, download=True)\n",
    "test_set = datasets.FashionMNIST('./data', train=False, download=True)\n",
    "\n",
    "# Extract important arrays and information\n",
    "raw_features_train = train_set.data.numpy()\n",
    "raw_labels_train = train_set.targets.numpy()\n",
    "raw_features_test = test_set.data.numpy()\n",
    "raw_labels_test = test_set.targets.numpy()\n",
    "category_keys = train_set.classes\n",
    "category_vals = range(len(category_keys))\n",
    "category_dict = dict((map(lambda i,j : (i,j), category_keys, category_vals)))\n",
    "\n",
    "print(f\"training features array shape: {raw_features_train.shape}, test features array shape: {raw_features_test.shape}\")\n",
    "print(f\"training labels array shape: {raw_labels_train.shape}, test labels array shape: {raw_labels_test.shape}\")\n",
    "print(category_dict)\n",
    "\n",
    "# Visualize random image and target pair from training dataset\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 4, 4\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = np.random.randint(0, len(train_set))\n",
    "    img, label = train_set[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(category_keys[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img, cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess the Data\n",
    "A $\\mathcal{C}$-calss dataset is made up with a feature matrix: $\\mathbf{X} = [^{(1)}\\mathbf{x}, ^{(2)}\\mathbf{x}, ..., ^{(M)}\\mathbf{x}]^T$. and a one-hot encoded target matrix $\\mathbf{Y} = [^{(1)}\\mathbf{y}, ^{(2)}\\mathbf{y}, ..., ^{(M)}\\mathbf{y}]^T$. Where $M$ is the total number of instances in the dataset, $^{(m)}\\mathbf{x} = [^{(m)}x_1, ..., ^{(m)}x_n]$ is a normalized and flattened row vector bears $n$ feature values, and $^{(m)}\\mathbf{y} = [0, ..., 0, 1, 0, ..., 0]$ is a one-hot encoded row vector.\n",
    "\n",
    "- A grey-scale image can be represented by a **2-dimensional array with shape $(width, height)$**. Where, $width$ indicates number of pixels on horizontal direction, $height$ indicates number of pixels on vertical direction.\n",
    "- We can use an **integer ranged 0~255** to describe a pixel's color intensity. However, it is easier for your computer to handlle float values.\n",
    "- We would like to convert an image array into a row vector, or a **2d array with shape $(1, width*height)$**. So that, we can stack these row vectors vertically to form a feature matrix.\n",
    "- We also would like to encode target array into one-hot format.\n",
    "\n",
    "\n",
    "We will access the raw data by extracting it from the dataloaders. Then, process and prepare the raw data so that it can be used in later steps.\n",
    "\n",
    "### $\\color{violet}{\\textbf{(10\\%) Exercise 1: Data Preprocessing}}$\n",
    "1. Reshape feature array.\n",
    "2. One-hot encode target array\n",
    "3. Rescale feature arrary, represent each pixel with a float numbers in range 0~1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training features shape: (60000, 784), test feature shape: (10000, 784), training target shape: (60000, 10), test target shape: (10000, 10)\n",
      "sample features slice: \n",
      "[[0.80392157 0.81176471 0.8        0.80784314 0.83529412 0.83921569\n",
      "  0.84313725 0.84705882 0.85490196 0.90980392 0.77254902 0.\n",
      "  0.         0.00784314 0.01960784 0.02745098 0.00784314 0.00392157\n",
      "  0.         0.        ]\n",
      " [0.68235294 0.67843137 0.70196078 0.68627451 0.70980392 0.79607843\n",
      "  0.65490196 0.68235294 0.76470588 0.20784314 0.         0.\n",
      "  0.         0.         0.21568627 0.7372549  0.6745098  0.68627451\n",
      "  0.78039216 0.70196078]\n",
      " [0.         0.45882353 0.79607843 0.71372549 0.10980392 0.\n",
      "  0.38039216 0.45882353 0.38823529 0.38039216 0.45098039 0.08627451\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.37254902 0.4627451  0.25098039 0.17647059 0.2745098  0.21176471\n",
      "  0.24705882 0.76862745 0.6627451  0.41568627 0.43921569 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n",
      "sample labels slice: \n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (≈ 8 lines of code)\n",
    "# Reshape feature and target arrays\n",
    "reshaped_features_train = raw_features_train.reshape(raw_labels_train.shape[0], -1)  # (60000, 28, 28) -> (60000, 784)\n",
    "reshaped_features_test = raw_features_test.reshape(raw_features_test.shape[0], -1)  # (10000, 28, 28) -> (10000, 784)\n",
    "# One hot encode targets\n",
    "onehot_labels_train = np.zeros((raw_labels_train.shape[0], len(category_dict)))  # (60000,) -> (60000, 10)\n",
    "onehot_labels_train[np.arange(raw_labels_train.shape[0]), raw_labels_train] = 1\n",
    "onehot_labels_test = np.zeros((raw_labels_test.shape[0], len(category_dict)))  # (10000,) -> (10000, 10)\n",
    "onehot_labels_test[np.arange(raw_labels_test.shape[0]), raw_labels_test] = 1\n",
    "# Rescale features\n",
    "rescale_feature_train = reshaped_features_train / 255\n",
    "rescale_feature_test = reshaped_features_test / 255\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Rename for later usage\n",
    "features_train = rescale_feature_train\n",
    "features_test = rescale_feature_test\n",
    "labels_train = onehot_labels_train\n",
    "labels_test = onehot_labels_test\n",
    "print(f\"training features shape: {features_train.shape}, test feature shape: {features_test.shape}, training target shape: {labels_train.shape}, test target shape: {labels_test.shape}\")\n",
    "print(f\"sample features slice: \\n{features_train[3321:3325, 380:400]}\")\n",
    "print(f\"sample labels slice: \\n{labels_train[3321:3325]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "training features shape: (60000, 784), test feature shape: (10000, 784), training target shape: (60000, 10), test target shape: (10000, 10)\n",
    "sample features slice: \n",
    "[[0.80392157 0.81176471 0.8        0.80784314 0.83529412 0.83921569\n",
    "  0.84313725 0.84705882 0.85490196 0.90980392 0.77254902 0.\n",
    "  0.         0.00784314 0.01960784 0.02745098 0.00784314 0.00392157\n",
    "  0.         0.        ]\n",
    " [0.68235294 0.67843137 0.70196078 0.68627451 0.70980392 0.79607843\n",
    "  0.65490196 0.68235294 0.76470588 0.20784314 0.         0.\n",
    "  0.         0.         0.21568627 0.7372549  0.6745098  0.68627451\n",
    "  0.78039216 0.70196078]\n",
    " [0.         0.45882353 0.79607843 0.71372549 0.10980392 0.\n",
    "  0.38039216 0.45882353 0.38823529 0.38039216 0.45098039 0.08627451\n",
    "  0.         0.         0.         0.         0.         0.\n",
    "  0.         0.        ]\n",
    " [0.37254902 0.4627451  0.25098039 0.17647059 0.2745098  0.21176471\n",
    "  0.24705882 0.76862745 0.6627451  0.41568627 0.43921569 0.\n",
    "  0.         0.         0.         0.         0.         0.\n",
    "  0.         0.        ]]\n",
    "sample labels slice: \n",
    "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
    " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forward Pass\n",
    "A Multi-Layer Perceptron (MLP) model is featured with multiple layers of transformed features. Any two adjacent layer are connected by a linear model and an activation function. The linear model is governed by a set of weight parameters and a set of bias parameters. The general structure of an MLP model is shown below.\n",
    "\n",
    "![](./model.png)\n",
    "\n",
    "### 2.1. Initialize Parameters\n",
    "A linear model governed by weights $\\mathbf{W}^{[l]}$ and $\\mathbf{b}^{[l]}$ transforms features, $\\mathbf{X}^{[l-1]}$ in the $l-1$-th layer to $\\mathbf{Z}^{[l]}$ in the $l$-th layer.\n",
    "\n",
    "$\\mathbf{Z}^{[l]} = \\mathbf{X}^{[l-1]} \\cdot \\mathbf{W}^{[l]T} + \\mathbf{b}^{[l]}$\n",
    "\n",
    "Assume $\\mathbf{X}^{[l-1]}$ has $N_{l-1}$ features and $\\mathbf{X}^{[l]}$ has $N_{l}$ features, then $\\mathbf{W}^{[l]}$ is with shape $(N_l, N_{l-1})$, $\\mathbf{b}^{[l]}$ is with shape $(1, N_l)$\n",
    "\n",
    "\n",
    "In order to get the MLP model prepared, we need to initialize weight matrix and bias vector in every layer.\n",
    "\n",
    "### $\\color{violet}{\\textbf{(5\\%) Exercise 2: Parameter Initialization}}$\n",
    "Define a function to initialize weights and biases parameters and save these parameters in a **dictionary**. \n",
    "- Input sizes of all the layers (**include the input layer**) using a **list**.\n",
    "- Use a **`for` loop** to randomly initialize $\\mathbf{W}^{[l]}$ and $\\mathbf{b}^{[l]}$ for the $l$-th layer.\n",
    "- You may find `np.random.normal()` is a useful function when you were trying to create random values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['W1', 'b1', 'W2', 'b2', 'W3', 'b3'])\n",
      "{'W1': array([[ 3.68455981e-05, -1.38782645e-04, -7.23142013e-05,\n",
      "         9.20641119e-05, -1.95355264e-05,  3.87255258e-05],\n",
      "       [-6.00629078e-05,  3.25435259e-06,  6.46665063e-05,\n",
      "        -7.84748031e-05,  1.41884652e-04, -6.15479349e-05],\n",
      "       [-1.10127440e-04, -6.82385574e-05,  7.96622349e-05,\n",
      "         5.25934678e-05, -1.78411966e-04,  1.28629057e-04],\n",
      "       [ 5.66879246e-05,  8.95999019e-05, -1.62874713e-05,\n",
      "         3.51109174e-05, -1.03338181e-04, -8.88623865e-05],\n",
      "       [ 5.10860443e-05,  1.19661547e-04,  8.64757553e-05,\n",
      "        -1.41101724e-04,  1.38003186e-04, -1.06812125e-04]]), 'b1': array([[ 6.38004843e-05, -6.19666755e-05,  1.12931166e-04,\n",
      "        -3.12290798e-05,  5.59708416e-05]]), 'W2': array([[ 1.02685210e-04, -1.28307992e-04, -4.03018811e-05,\n",
      "        -4.09270011e-05, -1.84834135e-05],\n",
      "       [ 1.71466862e-04, -9.69611346e-05,  4.40873689e-05,\n",
      "        -1.10516782e-04, -5.17959570e-05],\n",
      "       [-1.87055902e-04, -6.31341925e-05, -5.47992268e-05,\n",
      "         3.67804566e-05, -1.05795654e-04],\n",
      "       [ 1.24964681e-04,  3.60854003e-05,  1.30113815e-04,\n",
      "         1.01177937e-04,  2.22286230e-05]]), 'b2': array([[ 1.16709537e-04, -1.12496462e-04, -9.47925077e-05,\n",
      "        -4.49789001e-05]]), 'W3': array([[ 1.01049508e-04, -3.52965408e-05,  2.06672986e-04,\n",
      "        -1.67359842e-05],\n",
      "       [-7.34788781e-05,  1.28189925e-05,  3.67334763e-05,\n",
      "        -3.00482492e-05],\n",
      "       [ 9.36343566e-05, -3.80351405e-05,  6.59823461e-06,\n",
      "         5.61206851e-05]]), 'b3': array([[ 2.57409859e-04,  1.69968799e-05, -7.22321090e-05]])}\n"
     ]
    }
   ],
   "source": [
    "def init_params(layer_sizes):\n",
    "    \"\"\" Parameter initialization function\n",
    "    Args:\n",
    "        layer_sizes -- list/tuple, (input size, ..., hidden layer size, ..., output size)\n",
    "    Returns:\n",
    "        parameters -- dictionary, contains parameters: Wi and bi, i is the i-th layer\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        parameters['W'+str(i+1)] = np.random.normal(0, 0.0001, size=(layer_sizes[i+1], layer_sizes[i]))\n",
    "        parameters['b'+str(i+1)] = np.random.normal(0, 0.0001, size=(1, layer_sizes[i+1]))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return parameters\n",
    "\n",
    "# Sanity check\n",
    "np.random.seed(3321)\n",
    "dummy_layer_sizes = (6, 5, 4, 3)  # (input size, layer1 size, layer2 size, output size)\n",
    "dummy_params = init_params(dummy_layer_sizes)\n",
    "print(dummy_params.keys())\n",
    "print(dummy_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "dict_keys(['W1', 'b1', 'W2', 'b2', 'W3', 'b3'])\n",
    "{'W1': array([[ 3.68455981e-05, -1.38782645e-04, -7.23142013e-05,\n",
    "         9.20641119e-05, -1.95355264e-05,  3.87255258e-05],\n",
    "       [-6.00629078e-05,  3.25435259e-06,  6.46665063e-05,\n",
    "        -7.84748031e-05,  1.41884652e-04, -6.15479349e-05],\n",
    "       [-1.10127440e-04, -6.82385574e-05,  7.96622349e-05,\n",
    "         5.25934678e-05, -1.78411966e-04,  1.28629057e-04],\n",
    "       [ 5.66879246e-05,  8.95999019e-05, -1.62874713e-05,\n",
    "         3.51109174e-05, -1.03338181e-04, -8.88623865e-05],\n",
    "       [ 5.10860443e-05,  1.19661547e-04,  8.64757553e-05,\n",
    "        -1.41101724e-04,  1.38003186e-04, -1.06812125e-04]]), 'b1': array([[ 6.38004843e-05, -6.19666755e-05,  1.12931166e-04,\n",
    "        -3.12290798e-05,  5.59708416e-05]]), 'W2': array([[ 1.02685210e-04, -1.28307992e-04, -4.03018811e-05,\n",
    "        -4.09270011e-05, -1.84834135e-05],\n",
    "       [ 1.71466862e-04, -9.69611346e-05,  4.40873689e-05,\n",
    "        -1.10516782e-04, -5.17959570e-05],\n",
    "       [-1.87055902e-04, -6.31341925e-05, -5.47992268e-05,\n",
    "         3.67804566e-05, -1.05795654e-04],\n",
    "       [ 1.24964681e-04,  3.60854003e-05,  1.30113815e-04,\n",
    "         1.01177937e-04,  2.22286230e-05]]), 'b2': array([[ 1.16709537e-04, -1.12496462e-04, -9.47925077e-05,\n",
    "        -4.49789001e-05]]), 'W3': array([[ 1.01049508e-04, -3.52965408e-05,  2.06672986e-04,\n",
    "        -1.67359842e-05],\n",
    "       [-7.34788781e-05,  1.28189925e-05,  3.67334763e-05,\n",
    "        -3.00482492e-05],\n",
    "       [ 9.36343566e-05, -3.80351405e-05,  6.59823461e-06,\n",
    "         5.61206851e-05]]), 'b3': array([[ 2.57409859e-04,  1.69968799e-05, -7.22321090e-05]])}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Forward Propagation\n",
    "A linear model transforms features, $\\mathbf{X}^{[l-1]}$ in the $l-1$-th layer to $\\mathbf{Z}^{[l]}$ in the $l$-th layer. Then we apply a Linear Rectified Unit (ReLU) function on $\\mathbf{Z}^{[l]}$ to form new features $\\mathbf{Z}^{[l]}$ in the $l$-th layer.\n",
    "\n",
    "$$\\mathbf{Z}^{[l]} = \\mathbf{X}^{[l-1]} \\cdot \\mathbf{W}^{[l]T} + \\mathbf{b}^{[l]}$$\n",
    "\n",
    "$$\\mathbf{X}^{[l]} = ReLU(\\mathbf{Z}^{[l]}) = \n",
    "    \\begin{cases}\n",
    "        0   & z \\leq 0 \\\\\n",
    "        z   & z > 0\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "The last layer needs to be activcated by a softmax function.\n",
    "\n",
    "#### $$\\hat{y}_i = \\frac{e^{z^{[L]}_i}}{\\sum^C_{i=1} e^{z^{[L]}_i}}$$\n",
    "\n",
    "The maxtrix $\\mathbf{Z}^{[L]}$ has shape: $(M, C)$, where $M$ is the number of samples, $C$ is the number of classes. When applying softmax activation, we only want to apply it on the 2nd dimension (1st axis in numpy). So that each row in $\\mathbf{Z}^{[L]}$ will be converted to probabilities.\n",
    "\n",
    "### $\\color{violet}{\\textbf{(15\\%) Exercise 3: Linear Model and Activations}}$\n",
    "- Define ReLU activation function and softmax activation function.\n",
    "- Define linear model.\n",
    "- Define forward propagation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]\n",
      " [0.33339668 0.33331653 0.33328679]]\n",
      "cache dictionary keys: dict_keys(['X0', 'Z1', 'X1', 'Z2', 'X2', 'Z3'])\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (≈ 7 lines of code)\n",
    "def sigmoid(x):\n",
    "    \"\"\" Rectified linear unit function\n",
    "    Args:\n",
    "        x -- scalar/array\n",
    "    Returns:\n",
    "        y -- scalar/array, y = 1 / (1 + e^{-x})\n",
    "    \"\"\"\n",
    "    y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "    return y\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\" Rectified linear unit function\n",
    "    Args:\n",
    "        x -- scalar/array\n",
    "    Returns:\n",
    "        y -- scalar/array, 0 if x <= 0, x if x >0\n",
    "    \"\"\"\n",
    "    y = np.maximum(0, x)\n",
    "\n",
    "    return y\n",
    "\n",
    "def softmax(out_features):\n",
    "    \"\"\" Softmax function\n",
    "    Args:\n",
    "        out_features: array with shape (M, C)\n",
    "    Returns:\n",
    "        probs: array with same shape as arr\n",
    "    \"\"\"\n",
    "    probs = np.exp(out_features) / np.sum(np.exp(out_features), axis=-1, keepdims=True)\n",
    "    \n",
    "    return probs\n",
    "\n",
    "def linear(in_features, weight, bias):\n",
    "    \"\"\" Linear model\n",
    "    Args:\n",
    "        in_features (matrix): 2d array with shape (M, N^[l-1])\n",
    "        weight (matrix): 2d array with shape (N^[l], N^[l-1])\n",
    "        bias (row vector): 2d array with shape (1, N^[l])\n",
    "    Returns:\n",
    "        out_features (matrix): 2d array with shape (M, , N^[l])\n",
    "    \"\"\"\n",
    "    out_features = in_features @ weight.T + bias\n",
    "        \n",
    "    return out_features\n",
    "\n",
    "def forward(in_features, params, activation='relu'):\n",
    "    \"\"\" Forward propagation process\n",
    "    Args:\n",
    "        in_features (matrix): 2d array with shape (M, N^[0])\n",
    "        params: dictionary, stores W's and b's\n",
    "    Returns:\n",
    "        prediction: 2d array with shape (M, C)\n",
    "        cache: dictionary, stores intemediate X's and Z's.\n",
    "    \"\"\"\n",
    "    num_layers = int(len(params) / 2)\n",
    "    cache = {'X0': in_features}\n",
    "    for i in range(num_layers - 1):\n",
    "        cache['Z' + str(i+1)] = linear(cache['X' + str(i)], params['W' + str(i+1)], params['b' + str(i+1)])\n",
    "        if activation == 'relu':\n",
    "            cache['X' + str(i+1)] = relu(cache['Z' + str(i+1)])\n",
    "        elif activation == 'sigmoid':\n",
    "            cache['X' + str(i+1)] = sigmoid(cache['Z' + str(i+1)])\n",
    "    cache['Z' + str(i+2)] = linear(cache['X' + str(i+1)], params['W' + str(i+2)], params['b' + str(i+2)])\n",
    "    prediction = softmax(cache['Z' + str(i+2)])\n",
    "\n",
    "    return prediction, cache\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Sanity check\n",
    "np.random.seed(3321)\n",
    "dummy_features = np.random.randn(8,6)\n",
    "dummy_preds, dummy_cache = forward(dummy_features, dummy_params, activation='relu')\n",
    "print(dummy_preds)\n",
    "print(f\"cache dictionary keys: {dummy_cache.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "[[0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]\n",
    " [0.33339668 0.33331653 0.33328679]]\n",
    "cache dictionary keys: dict_keys(['X0', 'Z1', 'X1', 'Z2', 'X2', 'Z3'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Class Cross Entropy Loss\n",
    "For multi-class classification problem, it is quite standard to use a general form of cross entropy function to evaluate model prediction vs. target. \n",
    "#### $\\mathcal{L}(\\mathbf{\\hat{Y}}, \\mathbf{Y}) = -\\frac{1}{M} \\sum_{m=1}^M \\sum_{c=1}^C {^{(m)} y_c} \\log {^{(m)} \\hat{y}_c}$\n",
    "\n",
    "### $\\color{violet}{\\textbf{(5\\%) Exercise 4: Cross Entropy Loss}}$\n",
    "Define a cross entropy function to compute the average loss between prediction matrix and target matrix. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE loss of dummy prediction: 1.0986472024186367\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (≈ 1 line of code)\n",
    "def ce_loss(predictions, labels):\n",
    "    \"\"\" Cross entropy loss function\n",
    "    Args:\n",
    "        predictions (matrix): 2d array with shape (M, C)\n",
    "        labels (matrix): 2d array with shape (M, C)\n",
    "    Returns:\n",
    "        loss: scalar, averaged ce loss\n",
    "    \"\"\"\n",
    "    errors = -np.sum(labels * np.log(predictions + 1e-10), axis=1)\n",
    "\n",
    "    return errors.mean()\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Sanity check\n",
    "dummy_labels = np.zeros((8, 3))\n",
    "dummy_labels[np.arange(8), np.random.randint(0, 3, (8,))] = 1\n",
    "dummy_loss = ce_loss(dummy_preds, dummy_labels)\n",
    "print(f\"CE loss of dummy prediction: {dummy_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "CE loss of dummy prediction: 1.098647202419345\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Back-Propagation\n",
    "In order to know how to update weights and biases, we need to compute the gradient of the loss. This requires compute gradient of loss w.r.t. the variables in the last layer first. Then compute gradient of loss w.r.t. the variables in the previous layer next. And so on, until the gradient of loss w.r.t. the first layer is computed. \n",
    "\n",
    "Due to the fact that the last layer is softmax activated, $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}^{[L]}}$ can be computed differently without explicitly solve for derivative of softmax function.\n",
    "$$d\\mathbf{Z}^{[L]} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}^{[L]}} = \\mathbf{\\hat{Y}} - \\mathbf{Y} $$\n",
    "\n",
    "Then, from last layer $L$ to first layer, we need to repeatedly computing the gradient of loss according to the chain rule. The computation of a general layer $[l]$ is as follows.\n",
    "$$d\\mathbf{W}^{[l]} = d\\mathbf{Z}^{[l]} \\frac{\\partial \\mathbf{Z}^{[l]}}{\\partial{\\mathbf{W}^{[l]}}} = d\\mathbf{Z}^{[l]T} \\cdot \\mathbf{X}^{[l-1]}$$\n",
    "$$d\\mathbf{b}^{[l]} = d\\mathbf{Z}^{[l]} \\frac{\\partial \\mathbf{Z}^{[l]}}{\\partial{\\mathbf{b}^{[l]}}} = mean(d\\mathbf{Z}^{[l]}, axis=0, keepdims=True)$$\n",
    "$$d\\mathbf{X}^{[l-1]} = d\\mathbf{Z}^{[l]} \\frac{\\partial \\mathbf{Z}^{[l]}}{\\partial{\\mathbf{X}^{[l-1]}}} = d\\mathbf{Z}^{[l]} \\cdot \\mathbf{W}^{[l]}$$\n",
    "$$d\\mathbf{Z}^{[l-1]} = d\\mathbf{X}^{[l-1]} * relu'(\\mathbf{Z}^{[l-1]})$$\n",
    "\n",
    "\n",
    "### $\\color{violet}{\\textbf{(25\\%) Exercise 5: Gradient Computation}}$\n",
    "- Define derivative of ReLU function.\n",
    "- Define a function to perform backward propagation to compute gradient of the (cross entropy) loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['dZ3', 'dW3', 'db3', 'dX2', 'dZ2', 'dW2', 'db2', 'dX1', 'dZ1', 'dW1', 'db1'])\n",
      "{'dZ3': array([[-0.66660332,  0.33331653,  0.33328679],\n",
      "       [ 0.33339668,  0.33331653, -0.66671321],\n",
      "       [-0.66660332,  0.33331653,  0.33328679],\n",
      "       [ 0.33339668,  0.33331653, -0.66671321],\n",
      "       [ 0.33339668, -0.66668347,  0.33328679],\n",
      "       [ 0.33339668,  0.33331653, -0.66671321],\n",
      "       [ 0.33339668,  0.33331653, -0.66671321],\n",
      "       [ 0.33339668, -0.66668347,  0.33328679]]), 'dW3': array([[ 7.77983020e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [ 7.77962299e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [-1.55594532e-04,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00]]), 'db3': array([[ 0.08339668,  0.08331653, -0.16671321]]), 'dX2': array([[-6.06445677e-05,  1.51249634e-05, -1.23325920e-04,\n",
      "         1.98449678e-05],\n",
      "       [-5.32294163e-05,  1.78635631e-05,  7.67488316e-05,\n",
      "        -5.30117015e-05],\n",
      "       [-6.06445677e-05,  1.51249634e-05, -1.23325920e-04,\n",
      "         1.98449678e-05],\n",
      "       [-5.32294163e-05,  1.78635631e-05,  7.67488316e-05,\n",
      "        -5.30117015e-05],\n",
      "       [ 1.13883818e-04, -3.29905699e-05,  4.66135899e-05,\n",
      "         3.31572328e-05],\n",
      "       [-5.32294163e-05,  1.78635631e-05,  7.67488316e-05,\n",
      "        -5.30117015e-05],\n",
      "       [-5.32294163e-05,  1.78635631e-05,  7.67488316e-05,\n",
      "        -5.30117015e-05],\n",
      "       [ 1.13883818e-04, -3.29905699e-05,  4.66135899e-05,\n",
      "         3.31572328e-05]]), 'dZ2': array([[-6.06445677e-05,  0.00000000e+00, -0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [-5.32294163e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "        -0.00000000e+00],\n",
      "       [-6.06445677e-05,  0.00000000e+00, -0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [-5.32294163e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "        -0.00000000e+00],\n",
      "       [ 1.13883818e-04, -0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [-5.32294163e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "        -0.00000000e+00],\n",
      "       [-5.32294163e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "        -0.00000000e+00],\n",
      "       [ 1.13883818e-04, -0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00]]), 'dW2': array([[-3.29568730e-08,  4.96018508e-09, -7.53891708e-08,\n",
      "        -1.50064992e-08,  5.73359920e-08],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00]]), 'db2': array([[-1.33048955e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00]]), 'dX1': array([[-6.22730015e-09,  7.78118273e-09,  2.44409016e-09,\n",
      "         2.48200029e-09,  1.12091862e-09],\n",
      "       [-5.46587377e-09,  6.82975954e-09,  2.14524561e-09,\n",
      "         2.17852038e-09,  9.83861313e-10],\n",
      "       [-6.22730015e-09,  7.78118273e-09,  2.44409016e-09,\n",
      "         2.48200029e-09,  1.12091862e-09],\n",
      "       [-5.46587377e-09,  6.82975954e-09,  2.14524561e-09,\n",
      "         2.17852038e-09,  9.83861313e-10],\n",
      "       [ 1.16941838e-08, -1.46122041e-08, -4.58973210e-09,\n",
      "        -4.66092316e-09, -2.10496171e-09],\n",
      "       [-5.46587377e-09,  6.82975954e-09,  2.14524561e-09,\n",
      "         2.17852038e-09,  9.83861313e-10],\n",
      "       [-5.46587377e-09,  6.82975954e-09,  2.14524561e-09,\n",
      "         2.17852038e-09,  9.83861313e-10],\n",
      "       [ 1.16941838e-08, -1.46122041e-08, -4.58973210e-09,\n",
      "        -4.66092316e-09, -2.10496171e-09]]), 'dZ1': array([[-6.22730015e-09,  0.00000000e+00,  2.44409016e-09,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [-0.00000000e+00,  6.82975954e-09,  0.00000000e+00,\n",
      "         0.00000000e+00,  9.83861313e-10],\n",
      "       [-6.22730015e-09,  0.00000000e+00,  2.44409016e-09,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [-0.00000000e+00,  0.00000000e+00,  2.14524561e-09,\n",
      "         2.17852038e-09,  9.83861313e-10],\n",
      "       [ 0.00000000e+00, -1.46122041e-08, -0.00000000e+00,\n",
      "        -0.00000000e+00, -2.10496171e-09],\n",
      "       [-5.46587377e-09,  6.82975954e-09,  2.14524561e-09,\n",
      "         0.00000000e+00,  9.83861313e-10],\n",
      "       [-5.46587377e-09,  6.82975954e-09,  0.00000000e+00,\n",
      "         0.00000000e+00,  9.83861313e-10],\n",
      "       [ 1.16941838e-08, -0.00000000e+00, -4.58973210e-09,\n",
      "        -0.00000000e+00, -2.10496171e-09]]), 'dW1': array([[ 1.32450372e-08,  5.55766626e-09, -1.04503552e-08,\n",
      "        -2.81657005e-08, -7.48770180e-09, -1.71428652e-08],\n",
      "       [-1.59726569e-08, -2.42476215e-08, -3.30173568e-09,\n",
      "         1.18631879e-08,  5.05852160e-09,  1.17949703e-08],\n",
      "       [-1.22979413e-09,  6.05439978e-10,  4.63013284e-09,\n",
      "         1.22042014e-08, -2.95647063e-09,  6.90196809e-09],\n",
      "       [ 1.23495799e-09,  1.95195212e-09, -3.54825882e-10,\n",
      "         7.64898491e-10, -2.25124333e-09, -1.93588520e-09],\n",
      "       [-2.67123348e-09, -2.85117481e-10,  4.54407252e-10,\n",
      "         5.99184990e-09,  1.04095175e-09,  1.97834506e-09]]), 'db1': array([[-1.46152051e-09,  7.34634315e-10,  5.73617427e-10,\n",
      "         2.72315047e-10, -3.43097706e-11]])}\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (≈ 8 lines of code)\n",
    "def d_sigmoid(x):\n",
    "    \"\"\" Derivative of sigmoid function\n",
    "    Args:\n",
    "        x: scalar/array\n",
    "    Returns:\n",
    "        dydx: scalar/array, 0 if x < 0, 1 if x >= 0\n",
    "    \"\"\"\n",
    "    dydx = sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "    return dydx\n",
    "\n",
    "def d_relu(x):\n",
    "    \"\"\" Derivative of ReLU function\n",
    "    Args:\n",
    "        x: scalar/array\n",
    "    Returns:\n",
    "        dydx: scalar/array, 0 if x < 0, 1 if x >= 0\n",
    "    \"\"\"\n",
    "    dydx = np.ones_like(x)\n",
    "    dydx[x < 0] = 0\n",
    "\n",
    "    return dydx\n",
    "\n",
    "def grad(predictions, labels, params, cache, activation='relu'):\n",
    "    \"\"\" Backward propogating gradient computation\n",
    "    Args:\n",
    "        predictions (matrix): 2d array with shape (M, C)\n",
    "        labels (matrix): 2d array with shape (M, C)\n",
    "        params: dictionary, stores W's and b's.\n",
    "        cache: dictionary, stores intemediate X's and Z's.\n",
    "    Returns:\n",
    "        grads -- dictionary, stores dW's and db's\n",
    "    \"\"\"\n",
    "    num_layers = int(len(params) / 2)\n",
    "    grads = {'dZ' + str(num_layers): predictions - labels}\n",
    "    for i in reversed(range(num_layers)):\n",
    "        grads['dW'+ str(i+1)] = grads['dZ' + str(i+1)].T @ cache['X' + str(i)]\n",
    "        grads['db' + str(i+1)] = np.mean(grads['dZ' + str(i+1)], axis=0, keepdims=True)\n",
    "        if i==0:\n",
    "            break  \n",
    "        grads['dX' + str(i)] = grads['dZ' + str(i+1)] @ params['W' + str(i+1)]\n",
    "        if activation == 'relu':\n",
    "            grads['dZ' + str(i)] = grads['dX' + str(i)] * d_relu(cache['Z' + str(i)])\n",
    "        elif activation == 'sigmoid':\n",
    "            grads['dZ' + str(i)] = grads['dX' + str(i)] * d_sigmoid(cache['Z' + str(i)])\n",
    "        else:\n",
    "            grads['dZ' + str(i)] = grads['dX' + str(i)]\n",
    "\n",
    "    return grads\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Sanity check\n",
    "dummy_grads = grad(dummy_preds, dummy_labels, dummy_params, dummy_cache)\n",
    "print(dummy_grads.keys())\n",
    "print(dummy_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    ">\n",
    "```console\n",
    "dict_keys(['dZ3', 'dW3', 'db3', 'dX2', 'dZ2', 'dW2', 'db2', 'dX1', 'dZ1', 'dW1', 'db1'])\n",
    "{'dZ3': array([[ 0.33339668, -0.66668347,  0.33328679],\n",
    "       [-0.66660332,  0.33331653,  0.33328679],\n",
    "       [ 0.33339668,  0.33331653, -0.66671321],\n",
    "       [ 0.33339668,  0.33331653, -0.66671321],\n",
    "       [ 0.33339668, -0.66668347,  0.33328679],\n",
    "       [ 0.33339668,  0.33331653, -0.66671321],\n",
    "       [-0.66660332,  0.33331653,  0.33328679],\n",
    "       [ 0.33339668,  0.33331653, -0.66671321]]), 'dW3': array([[ 7.79094514e-05,  0.00000000e+00,  0.00000000e+00,\n",
    "         0.00000000e+00],\n",
    "       [ 7.77672619e-05,  0.00000000e+00,  0.00000000e+00,\n",
    "         0.00000000e+00],\n",
    "       [-1.55676713e-04,  0.00000000e+00,  0.00000000e+00,\n",
    "         0.00000000e+00]]), 'db3': array([[ 0.08339668,  0.08331653, -0.16671321]]), 'dX2': array([[ 1.13883818e-04, -3.29905699e-05,  4.66135899e-05,\n",
    "         3.31572328e-05],\n",
    "       [-6.06445677e-05,  1.51249634e-05, -1.23325920e-04,\n",
    "         1.98449678e-05],\n",
    "       [-5.32294163e-05,  1.78635631e-05,  7.67488316e-05,\n",
    "        -5.30117015e-05],\n",
    "       [-5.32294163e-05,  1.78635631e-05,  7.67488316e-05,\n",
    "        -5.30117015e-05],\n",
    "       [ 1.13883818e-04, -3.29905699e-05,  4.66135899e-05,\n",
    "         3.31572328e-05],\n",
    "       [-5.32294163e-05,  1.78635631e-05,  7.67488316e-05,\n",
    "        -5.30117015e-05],\n",
    "...\n",
    "         7.64898491e-10, -2.25124333e-09, -1.93588520e-09],\n",
    "       [-1.56762876e-09, -3.74956163e-09, -1.11294146e-09,\n",
    "         8.11358070e-11, -4.79680573e-10,  6.84456641e-11]]), 'db1': array([[-1.36634221e-09,  9.72490113e-10,  4.98906290e-10,\n",
    "         2.72315047e-10,  3.86057435e-10]])}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimization\n",
    "We have been able to compute the gradient of the cross entropy loss. Now, it's time to perform gradient descent optimization to bring the loss down.\n",
    "\n",
    "![](./gradient_dscent.png)\n",
    "\n",
    "\n",
    "### $\\color{violet}{\\textbf{(30\\%) Exercise 6: Gradient Descent Optimization}}$\n",
    "Train your model. Bring both training loss and test loss down. **Note: you may need to spend more time on this task. So, get started as early as possible.**\n",
    "1. Customize layer sizes.\n",
    "2. Use previously used function to complete the training process.\n",
    "3. Set appropriate iterations and learning rate\n",
    "4. You can use a `for` loop to update weights and biases embedded in the `params` dictionary.\n",
    "\n",
    "> Try to at least bring loss down below 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 training loss: 2.302584993249448, test loss: 2.302584992252783\n",
      "Iteration 2 training loss: 2.3025846297377077, test loss: 2.302584629947198\n",
      "Iteration 3 training loss: 2.302584254640614, test loss: 2.302584255840694\n",
      "Iteration 4 training loss: 2.302583848218244, test loss: 2.3025838504501897\n",
      "Iteration 5 training loss: 2.3025833869081094, test loss: 2.302583390476002\n",
      "Iteration 6 training loss: 2.302582844768504, test loss: 2.302582849616331\n",
      "Iteration 7 training loss: 2.3025821927453722, test loss: 2.3025821987687314\n",
      "Iteration 8 training loss: 2.3025813967093423, test loss: 2.302581404068802\n",
      "Iteration 9 training loss: 2.3025804166074355, test loss: 2.3025804252527813\n",
      "Iteration 10 training loss: 2.3025792038885218, test loss: 2.302579213680517\n",
      "Iteration 11 training loss: 2.302577698376526, test loss: 2.302577709332904\n",
      "Iteration 12 training loss: 2.302575821874552, test loss: 2.302575833736603\n",
      "Iteration 13 training loss: 2.302573472385539, test loss: 2.3025734848460613\n",
      "Iteration 14 training loss: 2.302570519974808, test loss: 2.3025705328947237\n",
      "Iteration 15 training loss: 2.3025668010184566, test loss: 2.3025668140768096\n",
      "Iteration 16 training loss: 2.302562108659806, test loss: 2.3025621217381893\n",
      "Iteration 17 training loss: 2.302556179570847, test loss: 2.3025561922995506\n",
      "Iteration 18 training loss: 2.3025486770111194, test loss: 2.3025486886774127\n",
      "Iteration 19 training loss: 2.302539169627011, test loss: 2.3025391789036367\n",
      "Iteration 20 training loss: 2.302527104916528, test loss: 2.3025271096936977\n",
      "Iteration 21 training loss: 2.302511775325799, test loss: 2.3025117732376335\n",
      "Iteration 22 training loss: 2.3024922760015007, test loss: 2.302492263699104\n",
      "Iteration 23 training loss: 2.3024674503737685, test loss: 2.3024674228726765\n",
      "Iteration 24 training loss: 2.3024358169105956, test loss: 2.3024357673157305\n",
      "Iteration 25 training loss: 2.3023954747743507, test loss: 2.3023953939381916\n",
      "Iteration 26 training loss: 2.3023439888528148, test loss: 2.302343864641287\n",
      "Iteration 27 training loss: 2.3022782391794587, test loss: 2.30227805521824\n",
      "Iteration 28 training loss: 2.3021942294218243, test loss: 2.3021939641667295\n",
      "Iteration 29 training loss: 2.302086842198714, test loss: 2.302086466834294\n",
      "Iteration 30 training loss: 2.3019495279810434, test loss: 2.301949005441659\n",
      "Iteration 31 training loss: 2.301773910988049, test loss: 2.3017731927872744\n",
      "Iteration 32 training loss: 2.301549294612284, test loss: 2.3015483165752144\n",
      "Iteration 33 training loss: 2.3012620401758395, test loss: 2.301260719022538\n",
      "Iteration 34 training loss: 2.300894797443072, test loss: 2.300893024366925\n",
      "Iteration 35 training loss: 2.3004255624590373, test loss: 2.30042319569576\n",
      "Iteration 36 training loss: 2.299826543851486, test loss: 2.2998234021181854\n",
      "Iteration 37 training loss: 2.299062824395, test loss: 2.2990586749707207\n",
      "Iteration 38 training loss: 2.2980908464157137, test loss: 2.298085387883339\n",
      "Iteration 39 training loss: 2.296856771592224, test loss: 2.2968496249496035\n",
      "Iteration 40 training loss: 2.2952948610985704, test loss: 2.2952855488423016\n",
      "Iteration 41 training loss: 2.293326120729734, test loss: 2.2933140482964744\n",
      "Iteration 42 training loss: 2.2908576473750504, test loss: 2.2908420710676185\n",
      "Iteration 43 training loss: 2.2877832851012543, test loss: 2.2877633186248096\n",
      "Iteration 44 training loss: 2.2839864161824956, test loss: 2.2839609887646906\n",
      "Iteration 45 training loss: 2.279345890263062, test loss: 2.279313781755743\n",
      "Iteration 46 training loss: 2.2737460102495373, test loss: 2.2737058566905626\n",
      "Iteration 47 training loss: 2.267090792788256, test loss: 2.267041142984404\n",
      "Iteration 48 training loss: 2.259321286999218, test loss: 2.259260627724904\n",
      "Iteration 49 training loss: 2.2504339383012577, test loss: 2.250361035777072\n",
      "Iteration 50 training loss: 2.2404928604757286, test loss: 2.2404067872049347\n",
      "Iteration 51 training loss: 2.2296311428383904, test loss: 2.2295316831475347\n",
      "Iteration 52 training loss: 2.218036512038741, test loss: 2.217925142803421\n",
      "Iteration 53 training loss: 2.205919531779673, test loss: 2.205798835488501\n",
      "Iteration 54 training loss: 2.1934764453451234, test loss: 2.1933496881136225\n",
      "Iteration 55 training loss: 2.180851172181816, test loss: 2.180723067965039\n",
      "Iteration 56 training loss: 2.168100057193193, test loss: 2.167975030260138\n",
      "Iteration 57 training loss: 2.1551829461800183, test loss: 2.1550644716410607\n",
      "Iteration 58 training loss: 2.14196123193282, test loss: 2.141858418215234\n",
      "Iteration 59 training loss: 2.1282125637933436, test loss: 2.1281366018371863\n",
      "Iteration 60 training loss: 2.1136472738607783, test loss: 2.113611908823854\n",
      "Iteration 61 training loss: 2.097924710265396, test loss: 2.097945673539329\n",
      "Iteration 62 training loss: 2.0806797361512577, test loss: 2.080773029568157\n",
      "Iteration 63 training loss: 2.061559368493831, test loss: 2.061733711197929\n",
      "Iteration 64 training loss: 2.040267116988525, test loss: 2.0405383279847733\n",
      "Iteration 65 training loss: 2.01663352107714, test loss: 2.017014484259803\n",
      "Iteration 66 training loss: 1.99068487718421, test loss: 1.9911874588666099\n",
      "Iteration 67 training loss: 1.9626804858415179, test loss: 1.9633160236080047\n",
      "Iteration 68 training loss: 1.9331159413555146, test loss: 1.933891626553272\n",
      "Iteration 69 training loss: 1.9026510328957549, test loss: 1.9035682626697674\n",
      "Iteration 70 training loss: 1.8719904193721415, test loss: 1.8730460442508066\n",
      "Iteration 71 training loss: 1.8417332818641827, test loss: 1.8429231030102073\n",
      "Iteration 72 training loss: 1.8123009934453487, test loss: 1.813619709203605\n",
      "Iteration 73 training loss: 1.7839357929054178, test loss: 1.7853792704435172\n",
      "Iteration 74 training loss: 1.7567208178686236, test loss: 1.758288281112467\n",
      "Iteration 75 training loss: 1.7306605574417304, test loss: 1.732353268833582\n",
      "Iteration 76 training loss: 1.7057301037089156, test loss: 1.7075498108608451\n",
      "Iteration 77 training loss: 1.6818948787213688, test loss: 1.6838427775876996\n",
      "Iteration 78 training loss: 1.659123090686131, test loss: 1.6612016621295487\n",
      "Iteration 79 training loss: 1.6373837195033782, test loss: 1.6395956756934276\n",
      "Iteration 80 training loss: 1.6166463090392382, test loss: 1.6189942723019894\n",
      "Iteration 81 training loss: 1.5968760361120546, test loss: 1.5993641027859948\n",
      "Iteration 82 training loss: 1.5780360662240092, test loss: 1.5806672129240014\n",
      "Iteration 83 training loss: 1.5600866786589054, test loss: 1.5628641978730091\n",
      "Iteration 84 training loss: 1.542984226957829, test loss: 1.5459114349663625\n",
      "Iteration 85 training loss: 1.5266789631527844, test loss: 1.529757702328643\n",
      "Iteration 86 training loss: 1.5111198145576221, test loss: 1.5143515389669497\n",
      "Iteration 87 training loss: 1.4962559061209988, test loss: 1.4996418758218855\n",
      "Iteration 88 training loss: 1.482037607373319, test loss: 1.4855785621684086\n",
      "Iteration 89 training loss: 1.468416864489967, test loss: 1.4721129807212434\n",
      "Iteration 90 training loss: 1.4553477144550215, test loss: 1.4591992673914598\n",
      "Iteration 91 training loss: 1.4427863628233417, test loss: 1.446793001481996\n",
      "Iteration 92 training loss: 1.430691759083579, test loss: 1.4348531404799403\n",
      "Iteration 93 training loss: 1.4190255602128132, test loss: 1.4233412639655132\n",
      "Iteration 94 training loss: 1.4077517159731994, test loss: 1.412221018450363\n",
      "Iteration 95 training loss: 1.3968367182526271, test loss: 1.4014588253476394\n",
      "Iteration 96 training loss: 1.3862493442180677, test loss: 1.3910236333404093\n",
      "Iteration 97 training loss: 1.3759607383561872, test loss: 1.3808861767273901\n",
      "Iteration 98 training loss: 1.3659440113880343, test loss: 1.3710200467204499\n",
      "Iteration 99 training loss: 1.3561747376070086, test loss: 1.3614004208676762\n",
      "Iteration 100 training loss: 1.3466299898600342, test loss: 1.3520046344644034\n",
      "Iteration 101 training loss: 1.3372886886922797, test loss: 1.342811067848777\n",
      "Iteration 102 training loss: 1.3281313591459278, test loss: 1.3338005153749413\n",
      "Iteration 103 training loss: 1.3191398312558393, test loss: 1.324954214170661\n",
      "Iteration 104 training loss: 1.3102971880586836, test loss: 1.316255885339727\n",
      "Iteration 105 training loss: 1.3015881976588692, test loss: 1.3076897254298547\n",
      "Iteration 106 training loss: 1.2929987138243606, test loss: 1.2992416647084348\n",
      "Iteration 107 training loss: 1.284515928294775, test loss: 1.2908988274442337\n",
      "Iteration 108 training loss: 1.2761283969264, test loss: 1.2826496330814932\n",
      "Iteration 109 training loss: 1.2678259045023388, test loss: 1.2744844276890073\n",
      "Iteration 110 training loss: 1.2595998736263745, test loss: 1.2663937186213072\n",
      "Iteration 111 training loss: 1.2514434511160983, test loss: 1.2583709356137731\n",
      "Iteration 112 training loss: 1.2433523091229826, test loss: 1.2504121470868912\n",
      "Iteration 113 training loss: 1.235325268409639, test loss: 1.2425160793733139\n",
      "Iteration 114 training loss: 1.2273642435442143, test loss: 1.234686003883051\n",
      "Iteration 115 training loss: 1.2194738422213889, test loss: 1.2269258644369039\n",
      "Iteration 116 training loss: 1.2116555559446291, test loss: 1.219237787481942\n",
      "Iteration 117 training loss: 1.2039116719055154, test loss: 1.2116227413906187\n",
      "Iteration 118 training loss: 1.1962440090398494, test loss: 1.2040814239041886\n",
      "Iteration 119 training loss: 1.1886515079271203, test loss: 1.1966129657026947\n",
      "Iteration 120 training loss: 1.1811351249214734, test loss: 1.1892200735176677\n",
      "Iteration 121 training loss: 1.1736960560125338, test loss: 1.1819035591585862\n",
      "Iteration 122 training loss: 1.1663363253411192, test loss: 1.1746667409906164\n",
      "Iteration 123 training loss: 1.159058571891837, test loss: 1.1675108109858094\n",
      "Iteration 124 training loss: 1.15186630881823, test loss: 1.1604417328509653\n",
      "Iteration 125 training loss: 1.1447633328676838, test loss: 1.1534593509513416\n",
      "Iteration 126 training loss: 1.1377530904405304, test loss: 1.146574438001095\n",
      "Iteration 127 training loss: 1.1308386415225178, test loss: 1.1397750617609055\n",
      "Iteration 128 training loss: 1.1240240322456627, test loss: 1.1330940011841617\n",
      "Iteration 129 training loss: 1.117314073436682, test loss: 1.1264798588216032\n",
      "Iteration 130 training loss: 1.110718669613526, test loss: 1.1200527261327176\n",
      "Iteration 131 training loss: 1.1042672316962625, test loss: 1.1136245356238654\n",
      "Iteration 132 training loss: 1.0980799355377484, test loss: 1.1077546811072057\n",
      "Iteration 133 training loss: 1.0926645542252131, test loss: 1.102058233670593\n",
      "Iteration 134 training loss: 1.0903499536039176, test loss: 1.1007483588494218\n",
      "Iteration 135 training loss: 1.1003843439552738, test loss: 1.1092375931427505\n",
      "Iteration 136 training loss: 1.1537240378003328, test loss: 1.1668252401981891\n",
      "Iteration 137 training loss: 1.2262860035463896, test loss: 1.233386488427365\n",
      "Iteration 138 training loss: 1.258382524369782, test loss: 1.272772157343864\n",
      "Iteration 139 training loss: 1.1212084869740957, test loss: 1.1285381067798976\n",
      "Iteration 140 training loss: 1.1411561627426272, test loss: 1.1550026225938568\n",
      "Iteration 141 training loss: 1.205816312230383, test loss: 1.213634722599211\n",
      "Iteration 142 training loss: 1.1912900338613128, test loss: 1.2050220806279557\n",
      "Iteration 143 training loss: 1.1240266395331833, test loss: 1.1324092144337687\n",
      "Iteration 144 training loss: 1.139264855548972, test loss: 1.1533787292322457\n",
      "Iteration 145 training loss: 1.1673481993909431, test loss: 1.1756304199593912\n",
      "Iteration 146 training loss: 1.136744739620366, test loss: 1.150381921239932\n",
      "Iteration 147 training loss: 1.116367889196323, test loss: 1.1252612786331901\n",
      "Iteration 148 training loss: 1.1193066776758978, test loss: 1.133531617592126\n",
      "Iteration 149 training loss: 1.122805360288582, test loss: 1.131573730325075\n",
      "Iteration 150 training loss: 1.1040631565920278, test loss: 1.1180842985346318\n",
      "Iteration 151 training loss: 1.0974751449404692, test loss: 1.1066828917794136\n",
      "Iteration 152 training loss: 1.0946314001085036, test loss: 1.1090230551816251\n",
      "Iteration 153 training loss: 1.0899707530406029, test loss: 1.0992522312598016\n",
      "Iteration 154 training loss: 1.0826545917866914, test loss: 1.097126339345245\n",
      "Iteration 155 training loss: 1.0760017412407097, test loss: 1.0855492066377985\n",
      "Iteration 156 training loss: 1.0736837251833535, test loss: 1.088385699331422\n",
      "Iteration 157 training loss: 1.0662060696715079, test loss: 1.075928797076373\n",
      "Iteration 158 training loss: 1.0645238939757597, test loss: 1.0794000369603767\n",
      "Iteration 159 training loss: 1.0560740834338296, test loss: 1.0660064145670687\n",
      "Iteration 160 training loss: 1.0562064677409704, test loss: 1.0712741099939347\n",
      "Iteration 161 training loss: 1.0470501584773393, test loss: 1.0571773850390778\n",
      "Iteration 162 training loss: 1.0482577504960855, test loss: 1.063507005529519\n",
      "Iteration 163 training loss: 1.0384814128220148, test loss: 1.0488049707377023\n",
      "Iteration 164 training loss: 1.0406935558700423, test loss: 1.0561207507891939\n",
      "Iteration 165 training loss: 1.0304150301490158, test loss: 1.0409316492798004\n",
      "Iteration 166 training loss: 1.0334364187235763, test loss: 1.0490383310799958\n",
      "Iteration 167 training loss: 1.0227407009948568, test loss: 1.033447680356773\n",
      "Iteration 168 training loss: 1.0264682650280947, test loss: 1.0422403765242643\n",
      "Iteration 169 training loss: 1.0154070316866146, test loss: 1.026301520382807\n",
      "Iteration 170 training loss: 1.0197579040522025, test loss: 1.0356950198420285\n",
      "Iteration 171 training loss: 1.0083843559099082, test loss: 1.0194637058906764\n",
      "Iteration 172 training loss: 1.0133031517120386, test loss: 1.0294014661649724\n",
      "Iteration 173 training loss: 1.001624408790444, test loss: 1.0128847324343246\n",
      "Iteration 174 training loss: 1.0070613633612346, test loss: 1.0233180720591704\n",
      "Iteration 175 training loss: 0.9951104517147829, test loss: 1.0065486721691572\n",
      "Iteration 176 training loss: 1.0010305405443587, test loss: 1.01744394123281\n",
      "Iteration 177 training loss: 0.988818697108776, test loss: 1.0004305913294547\n",
      "Iteration 178 training loss: 0.9951894758936465, test loss: 1.0117564658640246\n",
      "Iteration 179 training loss: 0.9827348548467006, test loss: 0.9945172849515195\n",
      "Iteration 180 training loss: 0.989540996531845, test loss: 1.00625849965874\n",
      "Iteration 181 training loss: 0.9768401935727101, test loss: 0.9887869346435489\n",
      "Iteration 182 training loss: 0.9840504544349498, test loss: 1.0009140995523946\n",
      "Iteration 183 training loss: 0.9711231855733842, test loss: 0.9832314834439078\n",
      "Iteration 184 training loss: 0.9787474105811648, test loss: 0.9957539556585627\n",
      "Iteration 185 training loss: 0.9655781454339429, test loss: 0.9778440695545951\n",
      "Iteration 186 training loss: 0.9735899755977295, test loss: 0.9907365306551829\n",
      "Iteration 187 training loss: 0.9601783042362195, test loss: 0.97259675821937\n",
      "Iteration 188 training loss: 0.9685544718119548, test loss: 0.9858373873194188\n",
      "Iteration 189 training loss: 0.954912570326469, test loss: 0.9674783867871384\n",
      "Iteration 190 training loss: 0.9636369671249039, test loss: 0.9810551209442303\n",
      "Iteration 191 training loss: 0.9497722101051004, test loss: 0.9624813542018535\n",
      "Iteration 192 training loss: 0.9588467673414167, test loss: 0.9763965726811837\n",
      "Iteration 193 training loss: 0.9447596103862715, test loss: 0.9576086096045284\n",
      "Iteration 194 training loss: 0.9541752735531024, test loss: 0.9718538717247586\n",
      "Iteration 195 training loss: 0.939851437527869, test loss: 0.9528372728276093\n",
      "Iteration 196 training loss: 0.9495914828927717, test loss: 0.9673964200140829\n",
      "Iteration 197 training loss: 0.9350514185129785, test loss: 0.9481715422576524\n",
      "Iteration 198 training loss: 0.9451006647128115, test loss: 0.9630298792583278\n",
      "Iteration 199 training loss: 0.9303554958446507, test loss: 0.9436063224105011\n",
      "Iteration 200 training loss: 0.9407024088041814, test loss: 0.9587534155649299\n",
      "Iteration 201 training loss: 0.9257572516604717, test loss: 0.9391359812413137\n",
      "Iteration 202 training loss: 0.9363874736076007, test loss: 0.9545580992268758\n",
      "Iteration 203 training loss: 0.9212502384426627, test loss: 0.9347547175295476\n",
      "Iteration 204 training loss: 0.9321563565899305, test loss: 0.9504428427525341\n",
      "Iteration 205 training loss: 0.9168337295819747, test loss: 0.9304606955712039\n",
      "Iteration 206 training loss: 0.928003992170806, test loss: 0.9464031814539976\n",
      "Iteration 207 training loss: 0.9125088234882673, test loss: 0.9262543162396598\n",
      "Iteration 208 training loss: 0.9239347110113848, test loss: 0.9424434014631257\n",
      "Iteration 209 training loss: 0.9082639057329642, test loss: 0.9221238551676626\n",
      "Iteration 210 training loss: 0.9199253018441564, test loss: 0.9385416371223128\n",
      "Iteration 211 training loss: 0.904093861744722, test loss: 0.9180655723327622\n",
      "Iteration 212 training loss: 0.9159721344497418, test loss: 0.9346937357806179\n",
      "Iteration 213 training loss: 0.9000031018807645, test loss: 0.9140837840023909\n",
      "Iteration 214 training loss: 0.9120816836892691, test loss: 0.9309052493705169\n",
      "Iteration 215 training loss: 0.8959870399272296, test loss: 0.9101743023762132\n",
      "Iteration 216 training loss: 0.9082554590794346, test loss: 0.9271783979966093\n",
      "Iteration 217 training loss: 0.8920429005005005, test loss: 0.9063339680651028\n",
      "Iteration 218 training loss: 0.9044822633804819, test loss: 0.923501242253981\n",
      "Iteration 219 training loss: 0.8881782286353391, test loss: 0.9025697172360766\n",
      "Iteration 220 training loss: 0.9007820203054991, test loss: 0.9198947758390604\n",
      "Iteration 221 training loss: 0.884397463241132, test loss: 0.8988867540996249\n",
      "Iteration 222 training loss: 0.8971367576897256, test loss: 0.9163422377637033\n",
      "Iteration 223 training loss: 0.8806877783954418, test loss: 0.8952724320291694\n",
      "Iteration 224 training loss: 0.8935420955001765, test loss: 0.9128378111839156\n",
      "Iteration 225 training loss: 0.8770326362152673, test loss: 0.891709818722829\n",
      "Iteration 226 training loss: 0.889989047572417, test loss: 0.9093739357950317\n",
      "Iteration 227 training loss: 0.8734570306468142, test loss: 0.8882245817331226\n",
      "Iteration 228 training loss: 0.886494021564771, test loss: 0.9059662923202905\n",
      "Iteration 229 training loss: 0.8699424748964106, test loss: 0.8847976544427293\n",
      "Iteration 230 training loss: 0.8830334043022311, test loss: 0.9025907008023835\n",
      "Iteration 231 training loss: 0.8664710871823976, test loss: 0.8814122732271078\n",
      "Iteration 232 training loss: 0.87959822151405, test loss: 0.8992372786446674\n",
      "Iteration 233 training loss: 0.863067374023547, test loss: 0.8780933787938022\n",
      "Iteration 234 training loss: 0.8762063709098206, test loss: 0.8959253047285205\n",
      "Iteration 235 training loss: 0.8597253259864757, test loss: 0.8748345830691158\n",
      "Iteration 236 training loss: 0.8728673337153074, test loss: 0.8926645264571009\n",
      "Iteration 237 training loss: 0.8564473016093598, test loss: 0.8716383199476976\n",
      "Iteration 238 training loss: 0.8695720306934915, test loss: 0.8894460508634798\n",
      "Iteration 239 training loss: 0.8532269213224722, test loss: 0.8684984916721483\n",
      "Iteration 240 training loss: 0.8663235673166132, test loss: 0.8862729368763462\n",
      "Iteration 241 training loss: 0.8500626122784921, test loss: 0.8654131377109286\n",
      "Iteration 242 training loss: 0.8631154524299335, test loss: 0.8831375365520471\n",
      "Iteration 243 training loss: 0.8469545608861111, test loss: 0.8623828134107534\n",
      "Iteration 244 training loss: 0.8599470381049824, test loss: 0.8800395357787522\n",
      "Iteration 245 training loss: 0.8438971751833225, test loss: 0.8594020438617385\n",
      "Iteration 246 training loss: 0.8568198437846792, test loss: 0.8769814528603554\n",
      "Iteration 247 training loss: 0.8408893532130136, test loss: 0.8564689824087666\n",
      "Iteration 248 training loss: 0.8537330574551913, test loss: 0.8739611262351684\n",
      "Iteration 249 training loss: 0.8379301177979289, test loss: 0.8535828534032509\n",
      "Iteration 250 training loss: 0.8506899537178262, test loss: 0.8709833708531027\n",
      "Iteration 251 training loss: 0.8350120740570713, test loss: 0.8507367187826829\n",
      "Iteration 252 training loss: 0.8476723551838306, test loss: 0.8680297269281022\n",
      "Iteration 253 training loss: 0.8321389389692951, test loss: 0.8479336451143097\n",
      "Iteration 254 training loss: 0.8446929978545172, test loss: 0.8651134389036337\n",
      "Iteration 255 training loss: 0.8293141027450459, test loss: 0.8451766462452639\n",
      "Iteration 256 training loss: 0.8417525578690866, test loss: 0.8622330325857536\n",
      "Iteration 257 training loss: 0.8265219248180208, test loss: 0.8424518858992976\n",
      "Iteration 258 training loss: 0.8388379146839205, test loss: 0.8593773000132474\n",
      "Iteration 259 training loss: 0.8237774854467611, test loss: 0.8397741251811606\n",
      "Iteration 260 training loss: 0.8359724160353241, test loss: 0.8565700282340775\n",
      "Iteration 261 training loss: 0.8210726763196753, test loss: 0.8371348275924758\n",
      "Iteration 262 training loss: 0.8331412848793933, test loss: 0.8537960104890319\n",
      "Iteration 263 training loss: 0.8183945767023488, test loss: 0.8345207343288774\n",
      "Iteration 264 training loss: 0.8303280520927504, test loss: 0.8510378396576461\n",
      "Iteration 265 training loss: 0.815757092431318, test loss: 0.8319461821389484\n",
      "Iteration 266 training loss: 0.8275535093293438, test loss: 0.8483177293354535\n",
      "Iteration 267 training loss: 0.8131526038237002, test loss: 0.8294039354572366\n",
      "Iteration 268 training loss: 0.8248154808389009, test loss: 0.8456327690863243\n",
      "Iteration 269 training loss: 0.8105833690920388, test loss: 0.8268968059700824\n",
      "Iteration 270 training loss: 0.8221120182457564, test loss: 0.8429814804621139\n",
      "Iteration 271 training loss: 0.8080452587902714, test loss: 0.8244197428180762\n",
      "Iteration 272 training loss: 0.8194317567451557, test loss: 0.840352821165215\n",
      "Iteration 273 training loss: 0.8055265714280564, test loss: 0.8219606073101144\n",
      "Iteration 274 training loss: 0.8167716287669008, test loss: 0.837743139970783\n",
      "Iteration 275 training loss: 0.803041103269501, test loss: 0.8195341723522784\n",
      "Iteration 276 training loss: 0.8141445073978382, test loss: 0.8351663511518247\n",
      "Iteration 277 training loss: 0.8005866440523061, test loss: 0.8171383302892016\n",
      "Iteration 278 training loss: 0.8115554562511464, test loss: 0.8326265972415494\n",
      "Iteration 279 training loss: 0.79816686300605, test loss: 0.8147765646075115\n",
      "Iteration 280 training loss: 0.8089975891682508, test loss: 0.8301170793878666\n",
      "Iteration 281 training loss: 0.7957697529124396, test loss: 0.8124359126352878\n",
      "Iteration 282 training loss: 0.8064651184663978, test loss: 0.8276319428368304\n",
      "Iteration 283 training loss: 0.7933979778881528, test loss: 0.8101201827973067\n",
      "Iteration 284 training loss: 0.8039555518077313, test loss: 0.8251683521584324\n",
      "Iteration 285 training loss: 0.7910563958269867, test loss: 0.8078353439464696\n",
      "Iteration 286 training loss: 0.8014799845399441, test loss: 0.8227385878816658\n",
      "Iteration 287 training loss: 0.7887369689054043, test loss: 0.8055725876517625\n",
      "Iteration 288 training loss: 0.7990261522520546, test loss: 0.8203292718721814\n",
      "Iteration 289 training loss: 0.7864471044111461, test loss: 0.803339204000501\n",
      "Iteration 290 training loss: 0.7966079630594077, test loss: 0.8179549679176876\n",
      "Iteration 291 training loss: 0.7841783528633887, test loss: 0.8011264857812912\n",
      "Iteration 292 training loss: 0.7942090107518234, test loss: 0.8155991040359327\n",
      "Iteration 293 training loss: 0.781924650009976, test loss: 0.7989284638625054\n",
      "Iteration 294 training loss: 0.7918293738523765, test loss: 0.8132607638711271\n",
      "Iteration 295 training loss: 0.7796919050767004, test loss: 0.796750319279074\n",
      "Iteration 296 training loss: 0.7894681932880407, test loss: 0.8109403813083291\n",
      "Iteration 297 training loss: 0.7774874277185208, test loss: 0.7946006206057601\n",
      "Iteration 298 training loss: 0.7871444457188151, test loss: 0.8086567758371828\n",
      "Iteration 299 training loss: 0.7753066842093572, test loss: 0.792474131506174\n",
      "Iteration 300 training loss: 0.7848410074733603, test loss: 0.8063926676051114\n",
      "Iteration 301 training loss: 0.7731474354048807, test loss: 0.7903689406094524\n",
      "Iteration 302 training loss: 0.7825645052971427, test loss: 0.8041556073297353\n",
      "Iteration 303 training loss: 0.7710082580374373, test loss: 0.7882834413980303\n",
      "Iteration 304 training loss: 0.7803059458790798, test loss: 0.8019352407976231\n",
      "Iteration 305 training loss: 0.768894335240422, test loss: 0.7862239529356637\n",
      "Iteration 306 training loss: 0.7780839054479791, test loss: 0.7997517037516243\n",
      "Iteration 307 training loss: 0.7667947364122987, test loss: 0.7841786121590756\n",
      "Iteration 308 training loss: 0.7758761868503312, test loss: 0.7975814452794643\n",
      "Iteration 309 training loss: 0.764712092179436, test loss: 0.7821502804180169\n",
      "Iteration 310 training loss: 0.7736797916348159, test loss: 0.7954210448616006\n",
      "Iteration 311 training loss: 0.7626474350223243, test loss: 0.7801396878536936\n",
      "Iteration 312 training loss: 0.7715054482155188, test loss: 0.7932823978723695\n",
      "Iteration 313 training loss: 0.7606027265561575, test loss: 0.7781488137163779\n",
      "Iteration 314 training loss: 0.7693561010826124, test loss: 0.7911677345213\n",
      "Iteration 315 training loss: 0.7585777161315147, test loss: 0.7761772712340016\n",
      "Iteration 316 training loss: 0.7672289001348729, test loss: 0.7890748553994101\n",
      "Iteration 317 training loss: 0.7565742392750466, test loss: 0.7742274866038257\n",
      "Iteration 318 training loss: 0.765124745654329, test loss: 0.7870040011779617\n",
      "Iteration 319 training loss: 0.7545747700932227, test loss: 0.7722801345324937\n",
      "Iteration 320 training loss: 0.7630222796069619, test loss: 0.7849338847550246\n",
      "Iteration 321 training loss: 0.7526016223464641, test loss: 0.7703590081673138\n",
      "Iteration 322 training loss: 0.7609590693855772, test loss: 0.782903146274242\n",
      "Iteration 323 training loss: 0.7506513342335802, test loss: 0.7684610202685304\n",
      "Iteration 324 training loss: 0.7589163882638459, test loss: 0.780892311758102\n",
      "Iteration 325 training loss: 0.7487204361721428, test loss: 0.7665817553700671\n",
      "Iteration 326 training loss: 0.7568856627659886, test loss: 0.7788929838677902\n",
      "Iteration 327 training loss: 0.7467992174339616, test loss: 0.7647118616779255\n",
      "Iteration 328 training loss: 0.754862927262145, test loss: 0.7769008562321958\n",
      "Iteration 329 training loss: 0.7448850579542997, test loss: 0.7628484943641298\n",
      "Iteration 330 training loss: 0.7528454098228513, test loss: 0.774913771591492\n",
      "Iteration 331 training loss: 0.7429834334955574, test loss: 0.7609979734637168\n",
      "Iteration 332 training loss: 0.7508481171557722, test loss: 0.7729469451469395\n",
      "Iteration 333 training loss: 0.7411061196837995, test loss: 0.7591715446311779\n",
      "Iteration 334 training loss: 0.748878941358741, test loss: 0.771007813656053\n",
      "Iteration 335 training loss: 0.7392408587125842, test loss: 0.7573569163760748\n",
      "Iteration 336 training loss: 0.7469188622880247, test loss: 0.7690776347681212\n",
      "Iteration 337 training loss: 0.7373934551629718, test loss: 0.7555601801597479\n",
      "Iteration 338 training loss: 0.7449864607500996, test loss: 0.7671744451561976\n",
      "Iteration 339 training loss: 0.7355635111905438, test loss: 0.7537810187874949\n",
      "Iteration 340 training loss: 0.7430643619422894, test loss: 0.7652813597183108\n",
      "Iteration 341 training loss: 0.7337463028722322, test loss: 0.7520147663079412\n",
      "Iteration 342 training loss: 0.7411556935693403, test loss: 0.763401013306958\n",
      "Iteration 343 training loss: 0.7319395248231888, test loss: 0.7502588327563143\n",
      "Iteration 344 training loss: 0.7392634724456674, test loss: 0.7615369739473099\n",
      "Iteration 345 training loss: 0.73014926094737, test loss: 0.7485195019592311\n",
      "Iteration 346 training loss: 0.7373924057575497, test loss: 0.7596936854380075\n",
      "Iteration 347 training loss: 0.7283795707713987, test loss: 0.746800546653879\n",
      "Iteration 348 training loss: 0.735537670366615, test loss: 0.7578668561848314\n",
      "Iteration 349 training loss: 0.7266304638391756, test loss: 0.7451026724530688\n",
      "Iteration 350 training loss: 0.7337079052682732, test loss: 0.7560647357382931\n",
      "Iteration 351 training loss: 0.7248969640109448, test loss: 0.7434195649129394\n",
      "Iteration 352 training loss: 0.7318950885325767, test loss: 0.7542791516280903\n",
      "Iteration 353 training loss: 0.723174498148712, test loss: 0.7417475292944491\n",
      "Iteration 354 training loss: 0.7300913477855412, test loss: 0.7525025497464964\n",
      "Iteration 355 training loss: 0.7214642519518482, test loss: 0.7400874791269826\n",
      "Iteration 356 training loss: 0.728299371765903, test loss: 0.7507371333221047\n",
      "Iteration 357 training loss: 0.7197670037353074, test loss: 0.7384406081121\n",
      "Iteration 358 training loss: 0.7265192495195537, test loss: 0.7489834877405792\n",
      "Iteration 359 training loss: 0.7180801335406125, test loss: 0.7368044531151039\n",
      "Iteration 360 training loss: 0.7247515384036398, test loss: 0.7472423418883883\n",
      "Iteration 361 training loss: 0.7164038032458785, test loss: 0.735178226502034\n",
      "Iteration 362 training loss: 0.7229925243782076, test loss: 0.7455098159748017\n",
      "Iteration 363 training loss: 0.7147457237325295, test loss: 0.733570455382537\n",
      "Iteration 364 training loss: 0.7212570424479551, test loss: 0.7438011843870548\n",
      "Iteration 365 training loss: 0.7131027608276963, test loss: 0.7319780548610545\n",
      "Iteration 366 training loss: 0.7195344406598696, test loss: 0.7421056999415963\n",
      "Iteration 367 training loss: 0.7114705467195833, test loss: 0.7303955337103417\n",
      "Iteration 368 training loss: 0.7178296979549295, test loss: 0.7404283868449556\n",
      "Iteration 369 training loss: 0.7098695638682601, test loss: 0.7288452530476698\n",
      "Iteration 370 training loss: 0.716153664672767, test loss: 0.7387799621902482\n",
      "Iteration 371 training loss: 0.7082746740391682, test loss: 0.7273011493065477\n",
      "Iteration 372 training loss: 0.7144848389005118, test loss: 0.7371385899200769\n",
      "Iteration 373 training loss: 0.7066968089402141, test loss: 0.725774312024245\n",
      "Iteration 374 training loss: 0.7128323985269316, test loss: 0.7355141818230071\n",
      "Iteration 375 training loss: 0.705126027474286, test loss: 0.7242548330863304\n",
      "Iteration 376 training loss: 0.711187058213209, test loss: 0.7338964830823902\n",
      "Iteration 377 training loss: 0.7035585137780284, test loss: 0.722738548981623\n",
      "Iteration 378 training loss: 0.7095454522561274, test loss: 0.732281717548783\n",
      "Iteration 379 training loss: 0.7020029558173273, test loss: 0.7212351377706158\n",
      "Iteration 380 training loss: 0.707912509510599, test loss: 0.7306756720332864\n",
      "Iteration 381 training loss: 0.7004613887223685, test loss: 0.7197462826917354\n",
      "Iteration 382 training loss: 0.7063089598533661, test loss: 0.7290995791745203\n",
      "Iteration 383 training loss: 0.6989401283202749, test loss: 0.7182773330399059\n",
      "Iteration 384 training loss: 0.7047119195130501, test loss: 0.7275301760685119\n",
      "Iteration 385 training loss: 0.6974282007240518, test loss: 0.7168177569864121\n",
      "Iteration 386 training loss: 0.7031315289932007, test loss: 0.725977310630217\n",
      "Iteration 387 training loss: 0.6959337063728687, test loss: 0.7153757312445806\n",
      "Iteration 388 training loss: 0.7015725652752505, test loss: 0.7244470859409352\n",
      "Iteration 389 training loss: 0.6944449771173657, test loss: 0.7139398207857663\n",
      "Iteration 390 training loss: 0.7000201083263685, test loss: 0.7229229249083513\n",
      "Iteration 391 training loss: 0.69297356175091, test loss: 0.7125212004256235\n",
      "Iteration 392 training loss: 0.6984834577524468, test loss: 0.7214154194399598\n",
      "Iteration 393 training loss: 0.6915142770977943, test loss: 0.7111144963919279\n",
      "Iteration 394 training loss: 0.6969597188630894, test loss: 0.7199206534538718\n",
      "Iteration 395 training loss: 0.690060581525098, test loss: 0.7097133921412772\n",
      "Iteration 396 training loss: 0.6954416597689064, test loss: 0.7184308096782227\n",
      "Iteration 397 training loss: 0.6886329098375653, test loss: 0.7083396154148781\n",
      "Iteration 398 training loss: 0.6939488433827099, test loss: 0.7169672347805482\n",
      "Iteration 399 training loss: 0.6872166928621073, test loss: 0.706975875990611\n",
      "Iteration 400 training loss: 0.6924730167920021, test loss: 0.715520265522138\n",
      "Iteration 401 training loss: 0.6858147686206624, test loss: 0.7056269418435636\n",
      "Iteration 402 training loss: 0.6910106783413912, test loss: 0.7140876589638818\n",
      "Iteration 403 training loss: 0.6844200619225247, test loss: 0.7042851945023474\n",
      "Iteration 404 training loss: 0.6895517426618073, test loss: 0.7126586066931357\n",
      "Iteration 405 training loss: 0.6830293308693242, test loss: 0.7029481327771406\n",
      "Iteration 406 training loss: 0.6880993422095953, test loss: 0.7112366527771707\n",
      "Iteration 407 training loss: 0.6816490678056062, test loss: 0.7016215108767183\n",
      "Iteration 408 training loss: 0.6866532996014029, test loss: 0.7098206959608815\n",
      "Iteration 409 training loss: 0.6802747845809376, test loss: 0.7003006670568922\n",
      "Iteration 410 training loss: 0.685222301985007, test loss: 0.7084203646670958\n",
      "Iteration 411 training loss: 0.678914208505963, test loss: 0.6989941869627867\n",
      "Iteration 412 training loss: 0.6838016912211812, test loss: 0.7070310699311755\n",
      "Iteration 413 training loss: 0.6775588897470479, test loss: 0.6976934848980413\n",
      "Iteration 414 training loss: 0.6823898444589139, test loss: 0.7056517978097094\n",
      "Iteration 415 training loss: 0.6762145238941684, test loss: 0.6964048773626635\n",
      "Iteration 416 training loss: 0.6809796211562213, test loss: 0.7042738269821194\n",
      "Iteration 417 training loss: 0.6748816851783286, test loss: 0.6951279462092023\n",
      "Iteration 418 training loss: 0.6795843590587141, test loss: 0.7029113792746912\n",
      "Iteration 419 training loss: 0.673562560388132, test loss: 0.6938651311122797\n",
      "Iteration 420 training loss: 0.6782072708267456, test loss: 0.7015688230427869\n",
      "Iteration 421 training loss: 0.6722497024543657, test loss: 0.6926086148380383\n",
      "Iteration 422 training loss: 0.6768320522504978, test loss: 0.7002278474656434\n",
      "Iteration 423 training loss: 0.6709612862577696, test loss: 0.6913761647185465\n",
      "Iteration 424 training loss: 0.6754845926242526, test loss: 0.6989154993663345\n",
      "Iteration 425 training loss: 0.6696775753900768, test loss: 0.6901475983630642\n",
      "Iteration 426 training loss: 0.674142421655167, test loss: 0.6976089738457426\n",
      "Iteration 427 training loss: 0.6683903731421438, test loss: 0.6889155429291355\n",
      "Iteration 428 training loss: 0.6727974054408832, test loss: 0.6963001008895995\n",
      "Iteration 429 training loss: 0.6671107067084254, test loss: 0.6876915711459184\n",
      "Iteration 430 training loss: 0.6714481331049519, test loss: 0.6949859788332894\n",
      "Iteration 431 training loss: 0.6658411093931601, test loss: 0.6864779048846557\n",
      "Iteration 432 training loss: 0.6701224327589126, test loss: 0.6936961650023473\n",
      "Iteration 433 training loss: 0.6645865283642767, test loss: 0.685279516225379\n",
      "Iteration 434 training loss: 0.668802054987217, test loss: 0.6924119422953718\n",
      "Iteration 435 training loss: 0.663337379767337, test loss: 0.6840861823890663\n",
      "Iteration 436 training loss: 0.6675050292870993, test loss: 0.6911518416388627\n",
      "Iteration 437 training loss: 0.6621112781093982, test loss: 0.6829162037009744\n",
      "Iteration 438 training loss: 0.6662235157110943, test loss: 0.6899083615277309\n",
      "Iteration 439 training loss: 0.660893740885034, test loss: 0.6817543873835271\n",
      "Iteration 440 training loss: 0.6649589832507175, test loss: 0.6886815089351034\n",
      "Iteration 441 training loss: 0.6596892473452204, test loss: 0.6806061649849561\n",
      "Iteration 442 training loss: 0.6637052420160908, test loss: 0.6874657519923074\n",
      "Iteration 443 training loss: 0.6584927609719285, test loss: 0.6794656861313444\n",
      "Iteration 444 training loss: 0.662451763428964, test loss: 0.6862507917991301\n",
      "Iteration 445 training loss: 0.657307059525841, test loss: 0.6783358050555651\n",
      "Iteration 446 training loss: 0.6612067732032582, test loss: 0.6850441194051684\n",
      "Iteration 447 training loss: 0.6561178628982163, test loss: 0.6772029812862175\n",
      "Iteration 448 training loss: 0.6599518209965339, test loss: 0.6838272495524723\n",
      "Iteration 449 training loss: 0.6549309765576767, test loss: 0.6760727548333272\n",
      "Iteration 450 training loss: 0.6587142215924091, test loss: 0.6826281787302884\n",
      "Iteration 451 training loss: 0.6537517591251613, test loss: 0.6749501437980696\n",
      "Iteration 452 training loss: 0.6574771272438618, test loss: 0.681429481034788\n",
      "Iteration 453 training loss: 0.652557896973717, test loss: 0.6738123222646878\n",
      "Iteration 454 training loss: 0.6562250424949321, test loss: 0.6802156785545032\n",
      "Iteration 455 training loss: 0.6513787416443592, test loss: 0.6726897216682768\n",
      "Iteration 456 training loss: 0.6549848654637509, test loss: 0.6790136349189692\n",
      "Iteration 457 training loss: 0.6501954991527319, test loss: 0.671563069772215\n",
      "Iteration 458 training loss: 0.6537532359211246, test loss: 0.6778198826856784\n",
      "Iteration 459 training loss: 0.6490358670380734, test loss: 0.6704605251820851\n",
      "Iteration 460 training loss: 0.652536143422185, test loss: 0.6766410019128284\n",
      "Iteration 461 training loss: 0.6478910012997576, test loss: 0.6693721669168128\n",
      "Iteration 462 training loss: 0.6513553949902826, test loss: 0.6754993241146365\n",
      "Iteration 463 training loss: 0.6467683622865882, test loss: 0.668306937493754\n",
      "Iteration 464 training loss: 0.6501950591633335, test loss: 0.6743784851248539\n",
      "Iteration 465 training loss: 0.6456569670583142, test loss: 0.6672528782106895\n",
      "Iteration 466 training loss: 0.6490499585231255, test loss: 0.6732734848529224\n",
      "Iteration 467 training loss: 0.6445576331757732, test loss: 0.6662114485628057\n",
      "Iteration 468 training loss: 0.6478984495576937, test loss: 0.6721610839883697\n",
      "Iteration 469 training loss: 0.6434607280347879, test loss: 0.6651723951751486\n",
      "Iteration 470 training loss: 0.6467651828791466, test loss: 0.6710666654766897\n",
      "Iteration 471 training loss: 0.6423635102201775, test loss: 0.6641337493819263\n",
      "Iteration 472 training loss: 0.6456179034438374, test loss: 0.6699571836629705\n",
      "Iteration 473 training loss: 0.6412682595022946, test loss: 0.663096377836016\n",
      "Iteration 474 training loss: 0.6444725061069025, test loss: 0.6688495452862295\n",
      "Iteration 475 training loss: 0.6401724929451188, test loss: 0.6620586958724108\n",
      "Iteration 476 training loss: 0.6433293249938079, test loss: 0.667744426702302\n",
      "Iteration 477 training loss: 0.6390869362749366, test loss: 0.661031878541357\n",
      "Iteration 478 training loss: 0.6422030359164604, test loss: 0.6666569752306034\n",
      "Iteration 479 training loss: 0.637998704064039, test loss: 0.6600037399305018\n",
      "Iteration 480 training loss: 0.6410777306176204, test loss: 0.6655718089125093\n",
      "Iteration 481 training loss: 0.6369225796281707, test loss: 0.6589874554551131\n",
      "Iteration 482 training loss: 0.6399584187184825, test loss: 0.6644919424531146\n",
      "Iteration 483 training loss: 0.6358621579303051, test loss: 0.6579861169246042\n",
      "Iteration 484 training loss: 0.6388535374598882, test loss: 0.6634261224207704\n",
      "Iteration 485 training loss: 0.6348001588705958, test loss: 0.6569827289896255\n",
      "Iteration 486 training loss: 0.6377419652628236, test loss: 0.6623533422567268\n",
      "Iteration 487 training loss: 0.6337426695895721, test loss: 0.6559841565582386\n",
      "Iteration 488 training loss: 0.6366299322370419, test loss: 0.6612791829297437\n",
      "Iteration 489 training loss: 0.632670186917888, test loss: 0.6549714744503605\n",
      "Iteration 490 training loss: 0.6355110660074674, test loss: 0.6601987919375918\n",
      "Iteration 491 training loss: 0.6315888176886583, test loss: 0.6539497726297312\n",
      "Iteration 492 training loss: 0.6343836899924638, test loss: 0.6591098374539137\n",
      "Iteration 493 training loss: 0.6305243451700402, test loss: 0.6529457175326545\n",
      "Iteration 494 training loss: 0.6332901969824789, test loss: 0.6580553290563926\n",
      "Iteration 495 training loss: 0.6294865267267038, test loss: 0.6519678256298748\n",
      "Iteration 496 training loss: 0.6322104609715729, test loss: 0.657014219608985\n",
      "Iteration 497 training loss: 0.6284648245112675, test loss: 0.651006478107577\n",
      "Iteration 498 training loss: 0.6311531642501359, test loss: 0.6559955657141551\n",
      "Iteration 499 training loss: 0.6274359184611747, test loss: 0.6500378153970687\n",
      "Iteration 500 training loss: 0.6300840995935516, test loss: 0.6549648983026461\n",
      "Iteration 501 training loss: 0.6264082470752363, test loss: 0.6490703815097497\n",
      "Iteration 502 training loss: 0.6290283338993476, test loss: 0.6539489142514353\n",
      "Iteration 503 training loss: 0.6253976086677643, test loss: 0.6481203730711526\n",
      "Iteration 504 training loss: 0.627986789033274, test loss: 0.6529464989971264\n",
      "Iteration 505 training loss: 0.6243851794320743, test loss: 0.6471696432049404\n",
      "Iteration 506 training loss: 0.6269514740559613, test loss: 0.6519517946912249\n",
      "Iteration 507 training loss: 0.6233881533832671, test loss: 0.6462342187897331\n",
      "Iteration 508 training loss: 0.6259312385814576, test loss: 0.6509730165523352\n",
      "Iteration 509 training loss: 0.6224063090427332, test loss: 0.6453149005329384\n",
      "Iteration 510 training loss: 0.6249203364622824, test loss: 0.6500038263915336\n",
      "Iteration 511 training loss: 0.621422914005835, test loss: 0.6443937695609218\n",
      "Iteration 512 training loss: 0.6239110740745597, test loss: 0.6490355676482676\n",
      "Iteration 513 training loss: 0.6204506044152822, test loss: 0.6434836545605637\n",
      "Iteration 514 training loss: 0.6229278248400211, test loss: 0.6480946844031705\n",
      "Iteration 515 training loss: 0.6195019784594927, test loss: 0.6425969580029105\n",
      "Iteration 516 training loss: 0.6219638428019537, test loss: 0.6471731960468323\n",
      "Iteration 517 training loss: 0.618559315446273, test loss: 0.6417150786660806\n",
      "Iteration 518 training loss: 0.6209929840206444, test loss: 0.6462450180063328\n",
      "Iteration 519 training loss: 0.6176171531394254, test loss: 0.6408336504694336\n",
      "Iteration 520 training loss: 0.62002012861698, test loss: 0.6453132974599317\n",
      "Iteration 521 training loss: 0.6166714506218612, test loss: 0.639948688542411\n",
      "Iteration 522 training loss: 0.6190400202848866, test loss: 0.644374276314911\n",
      "Iteration 523 training loss: 0.6156996895227261, test loss: 0.6390373503898561\n",
      "Iteration 524 training loss: 0.6180443241117003, test loss: 0.6434189425471515\n",
      "Iteration 525 training loss: 0.6147372698515735, test loss: 0.6381360568264738\n",
      "Iteration 526 training loss: 0.6170570889007326, test loss: 0.6424729692931481\n",
      "Iteration 527 training loss: 0.6137873477765857, test loss: 0.6372467756064935\n",
      "Iteration 528 training loss: 0.6160834465723134, test loss: 0.6415402947532723\n",
      "Iteration 529 training loss: 0.6128547460508484, test loss: 0.6363742597111299\n",
      "Iteration 530 training loss: 0.6151275052274728, test loss: 0.6406263491053281\n",
      "Iteration 531 training loss: 0.6119232160584613, test loss: 0.6355023115027668\n",
      "Iteration 532 training loss: 0.614183864198832, test loss: 0.639724421306934\n",
      "Iteration 533 training loss: 0.6109921137233347, test loss: 0.6346302360235719\n",
      "Iteration 534 training loss: 0.6132315434010481, test loss: 0.6388140815324698\n",
      "Iteration 535 training loss: 0.6100678281716209, test loss: 0.6337661254709538\n",
      "Iteration 536 training loss: 0.6122835334540611, test loss: 0.6379084295097532\n",
      "Iteration 537 training loss: 0.6091575719149582, test loss: 0.6329153981326298\n",
      "Iteration 538 training loss: 0.6113456184529227, test loss: 0.6370125168813744\n",
      "Iteration 539 training loss: 0.6082486934860909, test loss: 0.6320655452910154\n",
      "Iteration 540 training loss: 0.6104131378542368, test loss: 0.6361221240088064\n",
      "Iteration 541 training loss: 0.6073333526956537, test loss: 0.6312084704820489\n",
      "Iteration 542 training loss: 0.6094661382721382, test loss: 0.6352159109504837\n",
      "Iteration 543 training loss: 0.6063952846583223, test loss: 0.6303291167578895\n",
      "Iteration 544 training loss: 0.6085016512485797, test loss: 0.6342926505003267\n",
      "Iteration 545 training loss: 0.6054673670545403, test loss: 0.6294598952585055\n",
      "Iteration 546 training loss: 0.6075537085618489, test loss: 0.6333857898785432\n",
      "Iteration 547 training loss: 0.6045449009892467, test loss: 0.6285964497859928\n",
      "Iteration 548 training loss: 0.6066131247843068, test loss: 0.6324857961940106\n",
      "Iteration 549 training loss: 0.6036322707312927, test loss: 0.6277422932220535\n",
      "Iteration 550 training loss: 0.6056795424100967, test loss: 0.6315927673059937\n",
      "Iteration 551 training loss: 0.6027181826254687, test loss: 0.6268862962932716\n",
      "Iteration 552 training loss: 0.6047451577185011, test loss: 0.6307004102089604\n",
      "Iteration 553 training loss: 0.6018167277863651, test loss: 0.6260430437297179\n",
      "Iteration 554 training loss: 0.6038304682022595, test loss: 0.6298283002858334\n",
      "Iteration 555 training loss: 0.6009271402894113, test loss: 0.6252103198447279\n",
      "Iteration 556 training loss: 0.6029325628710318, test loss: 0.6289742742982034\n",
      "Iteration 557 training loss: 0.6000586188262687, test loss: 0.6243990532997772\n",
      "Iteration 558 training loss: 0.602060740119081, test loss: 0.6281477182038084\n",
      "Iteration 559 training loss: 0.5991974672080633, test loss: 0.6235959395785614\n",
      "Iteration 560 training loss: 0.6011788230359426, test loss: 0.6273119175560118\n",
      "Iteration 561 training loss: 0.5983326232955234, test loss: 0.6227894783208783\n",
      "Iteration 562 training loss: 0.6003005178667548, test loss: 0.6264794221569803\n",
      "Iteration 563 training loss: 0.5974712530489624, test loss: 0.6219859000699048\n",
      "Iteration 564 training loss: 0.5994223342573133, test loss: 0.6256472831099555\n",
      "Iteration 565 training loss: 0.596616106467946, test loss: 0.6211884512687625\n",
      "Iteration 566 training loss: 0.5985542876638383, test loss: 0.6248250938306428\n",
      "Iteration 567 training loss: 0.5957683655752508, test loss: 0.6203987714657027\n",
      "Iteration 568 training loss: 0.5976941497144941, test loss: 0.6240112112794306\n",
      "Iteration 569 training loss: 0.594931260248253, test loss: 0.6196187293815995\n",
      "Iteration 570 training loss: 0.5968373216669942, test loss: 0.6232009662337585\n",
      "Iteration 571 training loss: 0.59408114644712, test loss: 0.6188258114034757\n",
      "Iteration 572 training loss: 0.5959754187652564, test loss: 0.6223860235773314\n",
      "Iteration 573 training loss: 0.5932442053347208, test loss: 0.618045060581909\n",
      "Iteration 574 training loss: 0.5951184238424034, test loss: 0.621575271386294\n",
      "Iteration 575 training loss: 0.592406153607851, test loss: 0.6172611049864659\n",
      "Iteration 576 training loss: 0.5942591739093402, test loss: 0.6207617074211815\n",
      "Iteration 577 training loss: 0.5915659926496928, test loss: 0.616474429522003\n",
      "Iteration 578 training loss: 0.5934008993705955, test loss: 0.619949069149476\n",
      "Iteration 579 training loss: 0.5907227567628957, test loss: 0.6156837443414068\n",
      "Iteration 580 training loss: 0.5925435044706925, test loss: 0.6191380111318459\n",
      "Iteration 581 training loss: 0.5898898079978336, test loss: 0.6149028270007795\n",
      "Iteration 582 training loss: 0.5916878903985753, test loss: 0.6183289330572198\n",
      "Iteration 583 training loss: 0.5890510228658317, test loss: 0.6141155918319817\n",
      "Iteration 584 training loss: 0.5908340422459376, test loss: 0.6175210443260373\n",
      "Iteration 585 training loss: 0.5882092288970903, test loss: 0.613325071652374\n",
      "Iteration 586 training loss: 0.5899806761773261, test loss: 0.616715211036938\n",
      "Iteration 587 training loss: 0.5873881350126353, test loss: 0.6125540562024471\n",
      "Iteration 588 training loss: 0.5891395432109974, test loss: 0.6159215091776391\n",
      "Iteration 589 training loss: 0.5865712541297271, test loss: 0.6117869537606946\n",
      "Iteration 590 training loss: 0.5883073284703352, test loss: 0.615138503523743\n",
      "Iteration 591 training loss: 0.5857626635692555, test loss: 0.6110278580784446\n",
      "Iteration 592 training loss: 0.587472388911994, test loss: 0.6143531511539263\n",
      "Iteration 593 training loss: 0.584946462960714, test loss: 0.6102601568701506\n",
      "Iteration 594 training loss: 0.586632979799208, test loss: 0.6135632002644988\n",
      "Iteration 595 training loss: 0.5841318912326244, test loss: 0.6094936479816137\n",
      "Iteration 596 training loss: 0.5858009556423391, test loss: 0.6127816033034895\n",
      "Iteration 597 training loss: 0.5833343984610713, test loss: 0.608742839243335\n",
      "Iteration 598 training loss: 0.5849794218813649, test loss: 0.6120110828531918\n",
      "Iteration 599 training loss: 0.5825423900850922, test loss: 0.6079964300703099\n",
      "Iteration 600 training loss: 0.5841625946851997, test loss: 0.6112450138422757\n",
      "Iteration 601 training loss: 0.5817438462296026, test loss: 0.6072423718033025\n",
      "Iteration 602 training loss: 0.583343940148556, test loss: 0.6104782703290017\n",
      "Iteration 603 training loss: 0.5809438160635505, test loss: 0.6064857667626811\n",
      "Iteration 604 training loss: 0.582512515435515, test loss: 0.6096990657826767\n",
      "Iteration 605 training loss: 0.5801416265193892, test loss: 0.6057262890868413\n",
      "Iteration 606 training loss: 0.5816760606316882, test loss: 0.6089153680562822\n",
      "Iteration 607 training loss: 0.5793461017992613, test loss: 0.6049735052243772\n",
      "Iteration 608 training loss: 0.5808433463962722, test loss: 0.6081358073316363\n",
      "Iteration 609 training loss: 0.578550562831272, test loss: 0.6042194938948456\n",
      "Iteration 610 training loss: 0.5800121464195171, test loss: 0.607358898684012\n",
      "Iteration 611 training loss: 0.5777647192859122, test loss: 0.6034728637716394\n",
      "Iteration 612 training loss: 0.5791802775608194, test loss: 0.6065813902417176\n",
      "Iteration 613 training loss: 0.576988504996465, test loss: 0.6027347654292617\n",
      "Iteration 614 training loss: 0.5783509752154801, test loss: 0.605807963651392\n",
      "Iteration 615 training loss: 0.57621477230446, test loss: 0.6019972051677548\n",
      "Iteration 616 training loss: 0.5775194544940189, test loss: 0.6050335422255928\n",
      "Iteration 617 training loss: 0.5754583457117097, test loss: 0.6012744806656262\n",
      "Iteration 618 training loss: 0.5767032775882845, test loss: 0.6042755117198413\n",
      "Iteration 619 training loss: 0.5747156156017436, test loss: 0.6005633518756234\n",
      "Iteration 620 training loss: 0.575897068654969, test loss: 0.6035295870591574\n",
      "Iteration 621 training loss: 0.5739897449890297, test loss: 0.5998670052588619\n",
      "Iteration 622 training loss: 0.5750898266523488, test loss: 0.6027837278624646\n",
      "Iteration 623 training loss: 0.5732757809979698, test loss: 0.5991795475661349\n",
      "Iteration 624 training loss: 0.5742874656552431, test loss: 0.6020444400860876\n",
      "Iteration 625 training loss: 0.5725809017351283, test loss: 0.598507569138953\n",
      "Iteration 626 training loss: 0.5734894645489869, test loss: 0.6013090904748367\n",
      "Iteration 627 training loss: 0.5719087110093032, test loss: 0.59785475018407\n",
      "Iteration 628 training loss: 0.5726919875022499, test loss: 0.600576950896274\n",
      "Iteration 629 training loss: 0.5712524336087486, test loss: 0.5972138031664787\n",
      "Iteration 630 training loss: 0.571885028191305, test loss: 0.5998369600675253\n",
      "Iteration 631 training loss: 0.5706141476266773, test loss: 0.5965870743190032\n",
      "Iteration 632 training loss: 0.5710632906619656, test loss: 0.5990832156897418\n",
      "Iteration 633 training loss: 0.5699974951547773, test loss: 0.5959779809241124\n",
      "Iteration 634 training loss: 0.570229209804362, test loss: 0.598318638921388\n",
      "Iteration 635 training loss: 0.5694210192419438, test loss: 0.5954048399271262\n",
      "Iteration 636 training loss: 0.5694014424072629, test loss: 0.5975624281010345\n",
      "Iteration 637 training loss: 0.5688768840487699, test loss: 0.5948591750566952\n",
      "Iteration 638 training loss: 0.5685611182221681, test loss: 0.5967958327055769\n",
      "Iteration 639 training loss: 0.568383471583749, test loss: 0.5943594310750239\n",
      "Iteration 640 training loss: 0.5677051300669272, test loss: 0.5960152922798924\n",
      "Iteration 641 training loss: 0.5679405634560387, test loss: 0.5939048293092112\n",
      "Iteration 642 training loss: 0.5668260857058129, test loss: 0.5952135925035623\n",
      "Iteration 643 training loss: 0.5675558803321753, test loss: 0.59350252022731\n",
      "Iteration 644 training loss: 0.5659305587149255, test loss: 0.594396798050345\n",
      "Iteration 645 training loss: 0.567229273917867, test loss: 0.5931542289447587\n",
      "Iteration 646 training loss: 0.565026742466982, test loss: 0.5935724155758044\n",
      "Iteration 647 training loss: 0.5669778674984197, test loss: 0.5928781103737979\n",
      "Iteration 648 training loss: 0.5641094360576571, test loss: 0.5927360894270615\n",
      "Iteration 649 training loss: 0.5668143425502915, test loss: 0.5926861551189009\n",
      "Iteration 650 training loss: 0.563214041430056, test loss: 0.5919236350810836\n",
      "Iteration 651 training loss: 0.5667595556725817, test loss: 0.5926027278887854\n",
      "Iteration 652 training loss: 0.5623696976487459, test loss: 0.5911649303631006\n",
      "Iteration 653 training loss: 0.5668119950445727, test loss: 0.5926269166036695\n",
      "Iteration 654 training loss: 0.5616252862754587, test loss: 0.5905069080184949\n",
      "Iteration 655 training loss: 0.5669924945496567, test loss: 0.5927833050502819\n",
      "Iteration 656 training loss: 0.5610868555412686, test loss: 0.5900581862982576\n",
      "Iteration 657 training loss: 0.5673248916667682, test loss: 0.5930978896147977\n",
      "Iteration 658 training loss: 0.5608643397172123, test loss: 0.5899274134031831\n",
      "Iteration 659 training loss: 0.5677629394790453, test loss: 0.5935334795529676\n",
      "Iteration 660 training loss: 0.561059398659461, test loss: 0.5902172602421016\n",
      "Iteration 661 training loss: 0.5682855936292011, test loss: 0.5940740958076643\n",
      "Iteration 662 training loss: 0.561829392229618, test loss: 0.591083855347074\n",
      "Iteration 663 training loss: 0.5688322188300736, test loss: 0.594663404604485\n",
      "Iteration 664 training loss: 0.5632432263483398, test loss: 0.5925938672648058\n",
      "Iteration 665 training loss: 0.569324829451875, test loss: 0.5952299223613076\n",
      "Iteration 666 training loss: 0.5652238499087946, test loss: 0.5946679803573046\n",
      "Iteration 667 training loss: 0.5695978209273701, test loss: 0.5956034698155973\n",
      "Iteration 668 training loss: 0.5675025264152874, test loss: 0.5970292707385221\n",
      "Iteration 669 training loss: 0.5695597699686528, test loss: 0.5956850665847818\n",
      "Iteration 670 training loss: 0.5696552294596415, test loss: 0.5992485556616226\n",
      "Iteration 671 training loss: 0.5691176586400343, test loss: 0.5953727871147411\n",
      "Iteration 672 training loss: 0.5711857573038405, test loss: 0.600827501615959\n",
      "Iteration 673 training loss: 0.5682508872479561, test loss: 0.5946386635923431\n",
      "Iteration 674 training loss: 0.5718305280690404, test loss: 0.6015017431596414\n",
      "Iteration 675 training loss: 0.5670025822828247, test loss: 0.5935163282682221\n",
      "Iteration 676 training loss: 0.5715300171941927, test loss: 0.6012130013751482\n",
      "Iteration 677 training loss: 0.5654781772541496, test loss: 0.5921056555553196\n",
      "Iteration 678 training loss: 0.5704437281048498, test loss: 0.6001286316847798\n",
      "Iteration 679 training loss: 0.5638005188842127, test loss: 0.5905297165992847\n",
      "Iteration 680 training loss: 0.5688484975910763, test loss: 0.5985316945572107\n",
      "Iteration 681 training loss: 0.5620398554527128, test loss: 0.5888567878159442\n",
      "Iteration 682 training loss: 0.5669433163391573, test loss: 0.596624903133573\n",
      "Iteration 683 training loss: 0.5602737476821826, test loss: 0.5871661500122068\n",
      "Iteration 684 training loss: 0.5649157372063092, test loss: 0.5946004523323142\n",
      "Iteration 685 training loss: 0.5585836835187634, test loss: 0.5855408816399318\n",
      "Iteration 686 training loss: 0.5629144465996949, test loss: 0.5926077646194142\n",
      "Iteration 687 training loss: 0.556990767435072, test loss: 0.584006402576068\n",
      "Iteration 688 training loss: 0.5610356615814669, test loss: 0.5907451182654844\n",
      "Iteration 689 training loss: 0.5555310411013183, test loss: 0.5825973525953501\n",
      "Iteration 690 training loss: 0.5592971153066032, test loss: 0.5890301620937746\n",
      "Iteration 691 training loss: 0.5542047551467891, test loss: 0.5813152419347194\n",
      "Iteration 692 training loss: 0.5577120801826092, test loss: 0.5874778626324073\n",
      "Iteration 693 training loss: 0.5530213475478385, test loss: 0.580169556221505\n",
      "Iteration 694 training loss: 0.5562845107780692, test loss: 0.5860913213259971\n",
      "Iteration 695 training loss: 0.551973561949849, test loss: 0.5791524182412916\n",
      "Iteration 696 training loss: 0.5550154809334605, test loss: 0.5848690107014135\n",
      "Iteration 697 training loss: 0.5510498176597065, test loss: 0.5782537085064967\n",
      "Iteration 698 training loss: 0.5538758024938677, test loss: 0.5837808162680285\n",
      "Iteration 699 training loss: 0.5502519579108169, test loss: 0.5774752192637811\n",
      "Iteration 700 training loss: 0.5528492381918289, test loss: 0.5828106322493306\n",
      "Iteration 701 training loss: 0.5495562203579064, test loss: 0.5767944449273131\n",
      "Iteration 702 training loss: 0.5519057480108316, test loss: 0.5819272589598622\n",
      "Iteration 703 training loss: 0.5489512879264419, test loss: 0.5762012741541905\n",
      "Iteration 704 training loss: 0.5510378023893583, test loss: 0.5811217375249375\n",
      "Iteration 705 training loss: 0.5484267358993968, test loss: 0.5756846103105953\n",
      "Iteration 706 training loss: 0.5502111224341547, test loss: 0.5803585462400953\n",
      "Iteration 707 training loss: 0.547964012462719, test loss: 0.5752280203039621\n",
      "Iteration 708 training loss: 0.5494101597898472, test loss: 0.5796224523094826\n",
      "Iteration 709 training loss: 0.5475569130298149, test loss: 0.5748259659093984\n",
      "Iteration 710 training loss: 0.5486269079567796, test loss: 0.5789036467861414\n",
      "Iteration 711 training loss: 0.547186473994352, test loss: 0.5744590177120563\n",
      "Iteration 712 training loss: 0.5478481248449592, test loss: 0.5781870506373445\n",
      "Iteration 713 training loss: 0.5468365519971767, test loss: 0.5741122341698253\n",
      "Iteration 714 training loss: 0.547053528647541, test loss: 0.5774524234693502\n",
      "Iteration 715 training loss: 0.5465079016023533, test loss: 0.5737882510158806\n",
      "Iteration 716 training loss: 0.5462563553083796, test loss: 0.576713455718717\n",
      "Iteration 717 training loss: 0.5461800220046096, test loss: 0.573466165981909\n",
      "Iteration 718 training loss: 0.5454491848772378, test loss: 0.5759617183608572\n",
      "Iteration 719 training loss: 0.5458516051612011, test loss: 0.5731441707347327\n",
      "Iteration 720 training loss: 0.5446265160465054, test loss: 0.5751902236379224\n",
      "Iteration 721 training loss: 0.5455235455922157, test loss: 0.572824390700283\n",
      "Iteration 722 training loss: 0.5438018076547831, test loss: 0.5744139846788394\n",
      "Iteration 723 training loss: 0.5451897338826497, test loss: 0.5725012604096539\n",
      "Iteration 724 training loss: 0.5429800522992198, test loss: 0.573636341621004\n",
      "Iteration 725 training loss: 0.5448415126827212, test loss: 0.5721664267987846\n",
      "Iteration 726 training loss: 0.5421562850229704, test loss: 0.5728540079920603\n",
      "Iteration 727 training loss: 0.5444749790053324, test loss: 0.5718156443034397\n",
      "Iteration 728 training loss: 0.5413223984711064, test loss: 0.5720580222066812\n",
      "Iteration 729 training loss: 0.5440858711811023, test loss: 0.571446637318405\n",
      "Iteration 730 training loss: 0.5405118717513306, test loss: 0.571284612870689\n",
      "Iteration 731 training loss: 0.5436985129281606, test loss: 0.5710810169711312\n",
      "Iteration 732 training loss: 0.5397261570790999, test loss: 0.5705352581353672\n",
      "Iteration 733 training loss: 0.5433203463880384, test loss: 0.5707269265995486\n",
      "Iteration 734 training loss: 0.5389752575761403, test loss: 0.5698189963878134\n",
      "Iteration 735 training loss: 0.5429474454360099, test loss: 0.5703802284793559\n",
      "Iteration 736 training loss: 0.5382377126125917, test loss: 0.5691151447840341\n",
      "Iteration 737 training loss: 0.5425562318252443, test loss: 0.5700182466785726\n",
      "Iteration 738 training loss: 0.5375258702306891, test loss: 0.5684356868046734\n",
      "Iteration 739 training loss: 0.5421830197616848, test loss: 0.5696755658666615\n",
      "Iteration 740 training loss: 0.536861263438927, test loss: 0.5678020531380351\n",
      "Iteration 741 training loss: 0.5418255437629295, test loss: 0.5693495396626764\n",
      "Iteration 742 training loss: 0.5362411602311911, test loss: 0.5672113692470195\n",
      "Iteration 743 training loss: 0.5414968118383263, test loss: 0.5690534865530541\n",
      "Iteration 744 training loss: 0.5356642821170653, test loss: 0.5666623220042448\n",
      "Iteration 745 training loss: 0.5411660087411093, test loss: 0.5687582453721609\n",
      "Iteration 746 training loss: 0.5351429481068668, test loss: 0.5661670737558623\n",
      "Iteration 747 training loss: 0.5408638922573188, test loss: 0.5684943758567792\n",
      "Iteration 748 training loss: 0.5346813960252648, test loss: 0.5657311996141716\n",
      "Iteration 749 training loss: 0.5405736686193295, test loss: 0.5682459873576057\n",
      "Iteration 750 training loss: 0.534281146541451, test loss: 0.5653542529185722\n",
      "Iteration 751 training loss: 0.5402840746176948, test loss: 0.5680012016080224\n",
      "Iteration 752 training loss: 0.5339609919227861, test loss: 0.5650536552338834\n",
      "Iteration 753 training loss: 0.5400130125155128, test loss: 0.5677773840103341\n",
      "Iteration 754 training loss: 0.533724121187292, test loss: 0.5648340411699424\n",
      "Iteration 755 training loss: 0.5397556513865716, test loss: 0.5675726109808535\n",
      "Iteration 756 training loss: 0.5336051402028309, test loss: 0.5647299396148748\n",
      "Iteration 757 training loss: 0.5395361925968748, test loss: 0.5674097325900128\n",
      "Iteration 758 training loss: 0.5336143447023476, test loss: 0.5647502260233127\n",
      "Iteration 759 training loss: 0.5393357054112216, test loss: 0.5672718899331408\n",
      "Iteration 760 training loss: 0.5337609686400117, test loss: 0.5649035266043839\n",
      "Iteration 761 training loss: 0.5391714641942996, test loss: 0.567177721679979\n",
      "Iteration 762 training loss: 0.5340674597480652, test loss: 0.5652121414260012\n",
      "Iteration 763 training loss: 0.5390394898157229, test loss: 0.5671236236100768\n",
      "Iteration 764 training loss: 0.5345613205913199, test loss: 0.5657042821452618\n",
      "Iteration 765 training loss: 0.5389583869252412, test loss: 0.5671311287236509\n",
      "Iteration 766 training loss: 0.5352088520679136, test loss: 0.5663422812790907\n",
      "Iteration 767 training loss: 0.5388707889355286, test loss: 0.5671439525537613\n",
      "Iteration 768 training loss: 0.5359642635116676, test loss: 0.567079545028848\n",
      "Iteration 769 training loss: 0.538758268898191, test loss: 0.5671470270934267\n",
      "Iteration 770 training loss: 0.5367743995895772, test loss: 0.5678662684194654\n",
      "Iteration 771 training loss: 0.538606380084768, test loss: 0.5671239871337639\n",
      "Iteration 772 training loss: 0.5375738894295946, test loss: 0.5686357261402222\n",
      "Iteration 773 training loss: 0.5383966091750706, test loss: 0.5670583538826656\n",
      "Iteration 774 training loss: 0.5382740675725325, test loss: 0.5693000601738553\n",
      "Iteration 775 training loss: 0.538096258168184, test loss: 0.5669132520594518\n",
      "Iteration 776 training loss: 0.5387477622581311, test loss: 0.569733389229283\n",
      "Iteration 777 training loss: 0.5376589852119757, test loss: 0.5666414040683126\n",
      "Iteration 778 training loss: 0.5389043229834732, test loss: 0.5698423271789191\n",
      "Iteration 779 training loss: 0.5370823024421899, test loss: 0.5662355584944768\n",
      "Iteration 780 training loss: 0.5387383362221585, test loss: 0.5696258350202219\n",
      "Iteration 781 training loss: 0.5363847777705845, test loss: 0.5657138776627408\n",
      "Iteration 782 training loss: 0.5382211252817117, test loss: 0.5690514491128903\n",
      "Iteration 783 training loss: 0.5355566978773063, test loss: 0.5650621886361746\n",
      "Iteration 784 training loss: 0.537385177667973, test loss: 0.568151562367532\n",
      "Iteration 785 training loss: 0.5346233000295078, test loss: 0.5643074855243816\n",
      "Iteration 786 training loss: 0.5363054387954774, test loss: 0.56700372291938\n",
      "Iteration 787 training loss: 0.5336828069101451, test loss: 0.5635498787476843\n",
      "Iteration 788 training loss: 0.5350867813701565, test loss: 0.5657102194261971\n",
      "Iteration 789 training loss: 0.5327739290965128, test loss: 0.562832532837681\n",
      "Iteration 790 training loss: 0.5337646816883354, test loss: 0.5643051565898635\n",
      "Iteration 791 training loss: 0.53200669065534, test loss: 0.5622730038952057\n",
      "Iteration 792 training loss: 0.5324810872683887, test loss: 0.5629219892553556\n",
      "Iteration 793 training loss: 0.5315120930795595, test loss: 0.5620103473262074\n",
      "Iteration 794 training loss: 0.5313412741659133, test loss: 0.5616573110323065\n",
      "Iteration 795 training loss: 0.5314884332745438, test loss: 0.5622530070313136\n",
      "Iteration 796 training loss: 0.5304953225547581, test loss: 0.560655023242976\n",
      "Iteration 797 training loss: 0.5321979102936352, test loss: 0.5632724817963609\n",
      "Iteration 798 training loss: 0.5301567861343309, test loss: 0.5601177111908079\n",
      "Iteration 799 training loss: 0.5339384445358039, test loss: 0.5653740630574784\n",
      "Iteration 800 training loss: 0.5306321242011616, test loss: 0.5603516735741069\n",
      "Iteration 801 training loss: 0.5369672756702207, test loss: 0.5688073098170852\n",
      "Iteration 802 training loss: 0.5323192205735547, test loss: 0.5617617570094874\n",
      "Iteration 803 training loss: 0.5410560840445724, test loss: 0.5733071489079553\n",
      "Iteration 804 training loss: 0.5353338085582665, test loss: 0.564497075757605\n",
      "Iteration 805 training loss: 0.544827582296444, test loss: 0.5774125974430588\n",
      "Iteration 806 training loss: 0.5386576088566212, test loss: 0.567574442105883\n",
      "Iteration 807 training loss: 0.5460494613863172, test loss: 0.5788091492645082\n",
      "Iteration 808 training loss: 0.5404676999978745, test loss: 0.5692079478039413\n",
      "Iteration 809 training loss: 0.54398731553112, test loss: 0.5767645685291249\n",
      "Iteration 810 training loss: 0.5398197753605611, test loss: 0.5684638753203963\n",
      "Iteration 811 training loss: 0.540003003935509, test loss: 0.5727058777008764\n",
      "Iteration 812 training loss: 0.5375088392941811, test loss: 0.5661231542393698\n",
      "Iteration 813 training loss: 0.5357009193807079, test loss: 0.5683073234533212\n",
      "Iteration 814 training loss: 0.5346056587741556, test loss: 0.5632310966919738\n",
      "Iteration 815 training loss: 0.5318630620671316, test loss: 0.5643896344310461\n",
      "Iteration 816 training loss: 0.5318584239258545, test loss: 0.5605195895688644\n",
      "Iteration 817 training loss: 0.528742315598981, test loss: 0.5612167530197433\n",
      "Iteration 818 training loss: 0.5295283383539141, test loss: 0.5582398060438225\n",
      "Iteration 819 training loss: 0.5262536549037149, test loss: 0.5586985042149288\n",
      "Iteration 820 training loss: 0.5276038944657444, test loss: 0.5563721134200585\n",
      "Iteration 821 training loss: 0.5242704240052762, test loss: 0.5567095917928203\n",
      "Iteration 822 training loss: 0.52605702727791, test loss: 0.5548823760059081\n",
      "Iteration 823 training loss: 0.5226840596474375, test loss: 0.5551335562108669\n",
      "Iteration 824 training loss: 0.5248343930559591, test loss: 0.5537139652518039\n",
      "Iteration 825 training loss: 0.5214188557601834, test loss: 0.5538890872975242\n",
      "Iteration 826 training loss: 0.5238799427721043, test loss: 0.5528104437928932\n",
      "Iteration 827 training loss: 0.5203975859724121, test loss: 0.5528969099538853\n",
      "Iteration 828 training loss: 0.523132740752365, test loss: 0.552109757275716\n",
      "Iteration 829 training loss: 0.5195529859230574, test loss: 0.552086445934863\n",
      "Iteration 830 training loss: 0.5225358019312812, test loss: 0.551555752695499\n",
      "Iteration 831 training loss: 0.5188373113571559, test loss: 0.5514077206767111\n",
      "Iteration 832 training loss: 0.522021136238282, test loss: 0.5510820559268594\n",
      "Iteration 833 training loss: 0.5182096590941367, test loss: 0.5508188092549495\n",
      "Iteration 834 training loss: 0.5215684169161584, test loss: 0.5506686211345084\n",
      "Iteration 835 training loss: 0.5176403403093217, test loss: 0.5502895218148567\n",
      "Iteration 836 training loss: 0.5211469798771146, test loss: 0.5502860024564281\n",
      "Iteration 837 training loss: 0.5171081939144547, test loss: 0.5497977632402875\n",
      "Iteration 838 training loss: 0.5207300320865337, test loss: 0.5499081426869616\n",
      "Iteration 839 training loss: 0.5166079608583519, test loss: 0.5493366711131804\n",
      "Iteration 840 training loss: 0.5203235430201815, test loss: 0.5495403202658378\n",
      "Iteration 841 training loss: 0.5161180151695464, test loss: 0.5488852930179196\n",
      "Iteration 842 training loss: 0.519901228530714, test loss: 0.5491574422446833\n",
      "Iteration 843 training loss: 0.5156368128689273, test loss: 0.5484415411566119\n",
      "Iteration 844 training loss: 0.5194825968051642, test loss: 0.5487788034231015\n",
      "Iteration 845 training loss: 0.5151758001173153, test loss: 0.5480179278104245\n",
      "Iteration 846 training loss: 0.5190665002627095, test loss: 0.5484035978324089\n",
      "Iteration 847 training loss: 0.5147147197562629, test loss: 0.5475949659473323\n",
      "Iteration 848 training loss: 0.518636030223081, test loss: 0.548014982275571\n",
      "Iteration 849 training loss: 0.5142743341917234, test loss: 0.5471926933983652\n",
      "Iteration 850 training loss: 0.5182176319252376, test loss: 0.5476384251281958\n",
      "Iteration 851 training loss: 0.513847447033704, test loss: 0.5468034229190617\n",
      "Iteration 852 training loss: 0.5177964057341868, test loss: 0.5472592658704273\n",
      "Iteration 853 training loss: 0.5134244455123124, test loss: 0.5464171369536162\n",
      "Iteration 854 training loss: 0.517373773770737, test loss: 0.5468788739290592\n",
      "Iteration 855 training loss: 0.5130101465640967, test loss: 0.5460391021266487\n",
      "Iteration 856 training loss: 0.5169507066360517, test loss: 0.5464980270468672\n",
      "Iteration 857 training loss: 0.5126010924525664, test loss: 0.5456662441306346\n",
      "Iteration 858 training loss: 0.5165246400046378, test loss: 0.5461145279391592\n",
      "Iteration 859 training loss: 0.5122003772151725, test loss: 0.5453009129660167\n",
      "Iteration 860 training loss: 0.5161048522734134, test loss: 0.545737286751926\n",
      "Iteration 861 training loss: 0.5118112754311613, test loss: 0.5449473504783862\n",
      "Iteration 862 training loss: 0.515684799333513, test loss: 0.5453592042868226\n",
      "Iteration 863 training loss: 0.5114235782963491, test loss: 0.5445951065634752\n",
      "Iteration 864 training loss: 0.5152620167873235, test loss: 0.5449783392668897\n",
      "Iteration 865 training loss: 0.5110311739473804, test loss: 0.5442369865864443\n",
      "Iteration 866 training loss: 0.5148287987041559, test loss: 0.5445883461668822\n",
      "Iteration 867 training loss: 0.5106473686457644, test loss: 0.5438881991711584\n",
      "Iteration 868 training loss: 0.5143900488466868, test loss: 0.5441919298784492\n",
      "Iteration 869 training loss: 0.5102699412641514, test loss: 0.5435447325461986\n",
      "Iteration 870 training loss: 0.5139629968810031, test loss: 0.5438076066657582\n",
      "Iteration 871 training loss: 0.5099071073484347, test loss: 0.5432163472297431\n",
      "Iteration 872 training loss: 0.5135579655262462, test loss: 0.5434451269996902\n",
      "Iteration 873 training loss: 0.5095670990677013, test loss: 0.5429120051575781\n",
      "Iteration 874 training loss: 0.5131754248120716, test loss: 0.5431040547589391\n",
      "Iteration 875 training loss: 0.5092400354827972, test loss: 0.5426206173411441\n",
      "Iteration 876 training loss: 0.5127924388277914, test loss: 0.542762747023872\n",
      "Iteration 877 training loss: 0.5089134934777247, test loss: 0.5423296724378337\n",
      "Iteration 878 training loss: 0.5124095017387276, test loss: 0.5424215099674586\n",
      "Iteration 879 training loss: 0.5085998413042793, test loss: 0.5420524098759602\n",
      "Iteration 880 training loss: 0.5120399833023984, test loss: 0.5420938769537799\n",
      "Iteration 881 training loss: 0.5082907839312264, test loss: 0.5417785265875859\n",
      "Iteration 882 training loss: 0.5116709655501864, test loss: 0.5417669053066926\n",
      "Iteration 883 training loss: 0.5079952070227297, test loss: 0.5415184267363556\n",
      "Iteration 884 training loss: 0.5113128827007379, test loss: 0.5414507478134889\n",
      "Iteration 885 training loss: 0.5077006786950641, test loss: 0.541258395020791\n",
      "Iteration 886 training loss: 0.5109346660308428, test loss: 0.5411150512836497\n",
      "Iteration 887 training loss: 0.5073919837657461, test loss: 0.5409820206550353\n",
      "Iteration 888 training loss: 0.5105370723367189, test loss: 0.5407606631943217\n",
      "Iteration 889 training loss: 0.507069552308223, test loss: 0.5406905078203321\n",
      "Iteration 890 training loss: 0.5101183437733408, test loss: 0.5403857141523023\n",
      "Iteration 891 training loss: 0.506740934999518, test loss: 0.5403917269742761\n",
      "Iteration 892 training loss: 0.5097043684581655, test loss: 0.5400156118087729\n",
      "Iteration 893 training loss: 0.5064176748019523, test loss: 0.5400991123661167\n",
      "Iteration 894 training loss: 0.5092988820989593, test loss: 0.5396543678542527\n",
      "Iteration 895 training loss: 0.506103755069912, test loss: 0.5398171983935043\n",
      "Iteration 896 training loss: 0.5089028456569851, test loss: 0.5393023161133191\n",
      "Iteration 897 training loss: 0.5057917219744985, test loss: 0.5395363938780816\n",
      "Iteration 898 training loss: 0.5085092522167602, test loss: 0.5389528809085435\n",
      "Iteration 899 training loss: 0.5054720364836609, test loss: 0.5392478455026448\n",
      "Iteration 900 training loss: 0.5081156340643209, test loss: 0.5386042383084171\n",
      "Iteration 901 training loss: 0.5051567163406404, test loss: 0.5389638955889959\n",
      "Iteration 902 training loss: 0.5077220856783572, test loss: 0.538255541204846\n",
      "Iteration 903 training loss: 0.5048396020593914, test loss: 0.5386773461543457\n",
      "Iteration 904 training loss: 0.507319376928664, test loss: 0.5378973982739224\n",
      "Iteration 905 training loss: 0.5045003792489694, test loss: 0.5383678975589512\n",
      "Iteration 906 training loss: 0.5069078024902567, test loss: 0.5375293105738932\n",
      "Iteration 907 training loss: 0.5041491940863319, test loss: 0.5380452809966653\n",
      "Iteration 908 training loss: 0.5064912355405206, test loss: 0.5371556781456136\n",
      "Iteration 909 training loss: 0.5037905774675104, test loss: 0.5377149539549945\n",
      "Iteration 910 training loss: 0.5060706069472608, test loss: 0.5367777962057942\n",
      "Iteration 911 training loss: 0.5034285058786298, test loss: 0.5373812737302637\n",
      "Iteration 912 training loss: 0.5056673536414011, test loss: 0.5364154682815394\n",
      "Iteration 913 training loss: 0.5030691569158193, test loss: 0.537050538364313\n",
      "Iteration 914 training loss: 0.5052671021038053, test loss: 0.536054931609275\n",
      "Iteration 915 training loss: 0.5026957443178953, test loss: 0.5367049000745127\n",
      "Iteration 916 training loss: 0.5048680237264506, test loss: 0.5356953635178026\n",
      "Iteration 917 training loss: 0.5023087615841128, test loss: 0.5363459681350647\n",
      "Iteration 918 training loss: 0.50447496048543, test loss: 0.5353413205026875\n",
      "Iteration 919 training loss: 0.5019242756483434, test loss: 0.5359907053006595\n",
      "Iteration 920 training loss: 0.5040969068552639, test loss: 0.5350009859974558\n",
      "Iteration 921 training loss: 0.5015380625138879, test loss: 0.5356338420308746\n",
      "Iteration 922 training loss: 0.5037216554004322, test loss: 0.5346615304740783\n",
      "Iteration 923 training loss: 0.5011449755191665, test loss: 0.5352699800117882\n",
      "Iteration 924 training loss: 0.503363563920535, test loss: 0.5343379487501049\n",
      "Iteration 925 training loss: 0.5007394308135871, test loss: 0.5348941456713896\n",
      "Iteration 926 training loss: 0.5030134038923634, test loss: 0.5340215426143851\n",
      "Iteration 927 training loss: 0.5003268333776736, test loss: 0.5345114703527808\n",
      "Iteration 928 training loss: 0.5026575075829626, test loss: 0.5336985089097838\n",
      "Iteration 929 training loss: 0.49989481407423664, test loss: 0.5341092864885086\n",
      "Iteration 930 training loss: 0.5023081222514035, test loss: 0.5333809002928901\n",
      "Iteration 931 training loss: 0.4994425967965404, test loss: 0.5336878496692485\n",
      "Iteration 932 training loss: 0.5019636481264163, test loss: 0.5330668967599053\n",
      "Iteration 933 training loss: 0.49897485422329474, test loss: 0.5332511230639713\n",
      "Iteration 934 training loss: 0.5016271174921842, test loss: 0.5327606923985087\n",
      "Iteration 935 training loss: 0.49849043031366147, test loss: 0.5327982585909745\n",
      "Iteration 936 training loss: 0.5012949327271221, test loss: 0.5324582020017332\n",
      "Iteration 937 training loss: 0.4980014387093036, test loss: 0.5323413544695957\n",
      "Iteration 938 training loss: 0.5009931764561951, test loss: 0.5321838702968655\n",
      "Iteration 939 training loss: 0.497508425350366, test loss: 0.53188147644099\n",
      "Iteration 940 training loss: 0.5007217736511238, test loss: 0.5319390267089289\n",
      "Iteration 941 training loss: 0.4970221123478077, test loss: 0.5314286061366437\n",
      "Iteration 942 training loss: 0.5004716596668004, test loss: 0.5317132185273986\n",
      "Iteration 943 training loss: 0.4965329676448106, test loss: 0.5309721102375431\n",
      "Iteration 944 training loss: 0.5002427106008162, test loss: 0.5315061908224542\n",
      "Iteration 945 training loss: 0.49604915574642244, test loss: 0.5305192078885037\n",
      "Iteration 946 training loss: 0.5000426781776396, test loss: 0.5313259560811264\n",
      "Iteration 947 training loss: 0.49557437045850616, test loss: 0.5300747251785365\n",
      "Iteration 948 training loss: 0.4998594657430807, test loss: 0.5311628719514252\n",
      "Iteration 949 training loss: 0.49511862014879293, test loss: 0.5296481358760291\n",
      "Iteration 950 training loss: 0.4997146359899577, test loss: 0.5310377628744923\n",
      "Iteration 951 training loss: 0.49471036538153135, test loss: 0.5292689150712171\n",
      "Iteration 952 training loss: 0.49962333151035443, test loss: 0.5309660365421122\n",
      "Iteration 953 training loss: 0.49439264065629446, test loss: 0.5289779586655998\n",
      "Iteration 954 training loss: 0.49961396717180423, test loss: 0.5309721679616649\n",
      "Iteration 955 training loss: 0.4941816832036635, test loss: 0.5287892527918794\n",
      "Iteration 956 training loss: 0.49969357290761135, test loss: 0.5310679579589013\n",
      "Iteration 957 training loss: 0.49412939174265935, test loss: 0.5287564898730589\n",
      "Iteration 958 training loss: 0.4999072702864607, test loss: 0.5312999710729005\n",
      "Iteration 959 training loss: 0.49433654589786663, test loss: 0.5289765926442568\n",
      "Iteration 960 training loss: 0.5003194403187261, test loss: 0.5317331630740512\n",
      "Iteration 961 training loss: 0.4948795519061166, test loss: 0.5295206340373405\n",
      "Iteration 962 training loss: 0.5009369367372786, test loss: 0.5323779736644365\n",
      "Iteration 963 training loss: 0.49584222322007504, test loss: 0.5304685890822246\n",
      "Iteration 964 training loss: 0.5017858002035417, test loss: 0.5332663729445342\n",
      "Iteration 965 training loss: 0.49731588321561737, test loss: 0.5319076500041927\n",
      "Iteration 966 training loss: 0.502840067415595, test loss: 0.5343777880045416\n",
      "Iteration 967 training loss: 0.49930033154931136, test loss: 0.5338420765202645\n",
      "Iteration 968 training loss: 0.5040700676385339, test loss: 0.5356904084764273\n",
      "Iteration 969 training loss: 0.5017253747883285, test loss: 0.5362018454436955\n",
      "Iteration 970 training loss: 0.505345471075285, test loss: 0.537070842056971\n",
      "Iteration 971 training loss: 0.5042986140650982, test loss: 0.5386903468616213\n",
      "Iteration 972 training loss: 0.5064577438986808, test loss: 0.5383097804555238\n",
      "Iteration 973 training loss: 0.5066069080643104, test loss: 0.540903211113527\n",
      "Iteration 974 training loss: 0.5071759249385439, test loss: 0.5391689577465739\n",
      "Iteration 975 training loss: 0.508163411372743, test loss: 0.5423535337617101\n",
      "Iteration 976 training loss: 0.5072078040150965, test loss: 0.5393474858134714\n",
      "Iteration 977 training loss: 0.5085575470088044, test loss: 0.5426352912131159\n",
      "Iteration 978 training loss: 0.5064261162893633, test loss: 0.5387047761730178\n",
      "Iteration 979 training loss: 0.5077697790632837, test loss: 0.5417416468718419\n",
      "Iteration 980 training loss: 0.5049319216401145, test loss: 0.5373463139424814\n",
      "Iteration 981 training loss: 0.50595458149146, test loss: 0.539829506039582\n",
      "Iteration 982 training loss: 0.5029325885219462, test loss: 0.5354714921376529\n",
      "Iteration 983 training loss: 0.5035781832955442, test loss: 0.5373804464303839\n",
      "Iteration 984 training loss: 0.5007121996126158, test loss: 0.5333673672484686\n",
      "Iteration 985 training loss: 0.5010224972276286, test loss: 0.5347696741935041\n",
      "Iteration 986 training loss: 0.4984589486098674, test loss: 0.5312251840791862\n",
      "Iteration 987 training loss: 0.4984982372347889, test loss: 0.532205622103807\n",
      "Iteration 988 training loss: 0.4963231989613618, test loss: 0.5291996103045921\n",
      "Iteration 989 training loss: 0.4961566128352406, test loss: 0.5298348600766767\n",
      "Iteration 990 training loss: 0.4943866858595758, test loss: 0.5273708028980014\n",
      "Iteration 991 training loss: 0.4940756942860355, test loss: 0.527730427724181\n",
      "Iteration 992 training loss: 0.49269664794946155, test loss: 0.5257899021336089\n",
      "Iteration 993 training loss: 0.49227462203247385, test loss: 0.5259080651318601\n",
      "Iteration 994 training loss: 0.4912597714203972, test loss: 0.5244666297844929\n",
      "Iteration 995 training loss: 0.4907355339994389, test loss: 0.5243501382673454\n",
      "Iteration 996 training loss: 0.4900811906566721, test loss: 0.5234109429959258\n",
      "Iteration 997 training loss: 0.4894517026943953, test loss: 0.5230394800400218\n",
      "Iteration 998 training loss: 0.48917950232873386, test loss: 0.5226467492488545\n",
      "Iteration 999 training loss: 0.48839830654179345, test loss: 0.5219467156907683\n",
      "Iteration 1000 training loss: 0.48856525263445233, test loss: 0.5221914635455479\n",
      "Iteration 1001 training loss: 0.4876106180897754, test loss: 0.5211001368441869\n",
      "Iteration 1002 training loss: 0.4883537899108906, test loss: 0.5221701372166242\n",
      "Iteration 1003 training loss: 0.48718807677105685, test loss: 0.5205937289480572\n",
      "Iteration 1004 training loss: 0.4887054407196499, test loss: 0.5227516156916517\n",
      "Iteration 1005 training loss: 0.48727145867355803, test loss: 0.5205524057620572\n",
      "Iteration 1006 training loss: 0.48987600061418884, test loss: 0.5242063409265394\n",
      "Iteration 1007 training loss: 0.48818535647945116, test loss: 0.5213016815228282\n",
      "Iteration 1008 training loss: 0.4922712197710762, test loss: 0.5269483226537067\n",
      "Iteration 1009 training loss: 0.49045451700068776, test loss: 0.5233782403220327\n",
      "Iteration 1010 training loss: 0.4961975424177499, test loss: 0.5312688654956322\n",
      "Iteration 1011 training loss: 0.4946060756440498, test loss: 0.5273143357769031\n",
      "Iteration 1012 training loss: 0.501225187342536, test loss: 0.5366851837512341\n",
      "Iteration 1013 training loss: 0.5003135154101253, test loss: 0.5328130739254228\n",
      "Iteration 1014 training loss: 0.5053971384767727, test loss: 0.5411417473989841\n",
      "Iteration 1015 training loss: 0.5054452985269525, test loss: 0.5377689163599995\n",
      "Iteration 1016 training loss: 0.5063574485612783, test loss: 0.5422019030004382\n",
      "Iteration 1017 training loss: 0.5070966404349471, test loss: 0.5392982341168266\n",
      "Iteration 1018 training loss: 0.503932136600725, test loss: 0.5397050162582995\n",
      "Iteration 1019 training loss: 0.5051612989375608, test loss: 0.5372918182380578\n",
      "Iteration 1020 training loss: 0.4999619223509354, test loss: 0.5355983656554741\n",
      "Iteration 1021 training loss: 0.5015797996596376, test loss: 0.5336909605409662\n",
      "Iteration 1022 training loss: 0.4959974864728569, test loss: 0.5315000922548971\n",
      "Iteration 1023 training loss: 0.49802653664375246, test loss: 0.5301512714600449\n",
      "Iteration 1024 training loss: 0.4926856051127118, test loss: 0.5280929710595492\n",
      "Iteration 1025 training loss: 0.49502734267990944, test loss: 0.5271861015961328\n",
      "Iteration 1026 training loss: 0.49009029297021184, test loss: 0.5254379008663002\n",
      "Iteration 1027 training loss: 0.4926945387762767, test loss: 0.5248999841642105\n",
      "Iteration 1028 training loss: 0.4881001414259685, test loss: 0.5234213370326334\n",
      "Iteration 1029 training loss: 0.4909734966441303, test loss: 0.5232285139089828\n",
      "Iteration 1030 training loss: 0.48662023610311134, test loss: 0.521938173548196\n",
      "Iteration 1031 training loss: 0.4897483490818542, test loss: 0.5220497539053258\n",
      "Iteration 1032 training loss: 0.48551296820580025, test loss: 0.5208447084620894\n",
      "Iteration 1033 training loss: 0.48888590500951723, test loss: 0.521230836763373\n",
      "Iteration 1034 training loss: 0.4846963816555206, test loss: 0.5200564794590401\n",
      "Iteration 1035 training loss: 0.4883052263725666, test loss: 0.5206881605357929\n",
      "Iteration 1036 training loss: 0.48407606250564655, test loss: 0.5194696827337757\n",
      "Iteration 1037 training loss: 0.4879013742029968, test loss: 0.5203187135872742\n",
      "Iteration 1038 training loss: 0.48359470326056325, test loss: 0.5190252942804306\n",
      "Iteration 1039 training loss: 0.48762422123595217, test loss: 0.5200723019363079\n",
      "Iteration 1040 training loss: 0.48320001352598324, test loss: 0.5186677995044858\n",
      "Iteration 1041 training loss: 0.4873936681434166, test loss: 0.519869868736448\n",
      "Iteration 1042 training loss: 0.4828500783370806, test loss: 0.5183532850222102\n",
      "Iteration 1043 training loss: 0.48719277207868067, test loss: 0.5196965912975375\n",
      "Iteration 1044 training loss: 0.4825324122688375, test loss: 0.518070131268185\n",
      "Iteration 1045 training loss: 0.48698760519147005, test loss: 0.5195185581722566\n",
      "Iteration 1046 training loss: 0.48222154148965396, test loss: 0.517791120810729\n",
      "Iteration 1047 training loss: 0.4867625018616431, test loss: 0.519320628352028\n",
      "Iteration 1048 training loss: 0.4819129170430635, test loss: 0.5175131636795847\n",
      "Iteration 1049 training loss: 0.48651267931669084, test loss: 0.5190990877674596\n",
      "Iteration 1050 training loss: 0.4815992334589007, test loss: 0.5172279996967917\n",
      "Iteration 1051 training loss: 0.4862266839301255, test loss: 0.5188413875493294\n",
      "Iteration 1052 training loss: 0.4812864421705677, test loss: 0.516942341465163\n",
      "Iteration 1053 training loss: 0.48593397473157424, test loss: 0.5185782505067842\n",
      "Iteration 1054 training loss: 0.48097034245717624, test loss: 0.5166526438233039\n",
      "Iteration 1055 training loss: 0.48562324523020123, test loss: 0.5182989938436923\n",
      "Iteration 1056 training loss: 0.48066470672361467, test loss: 0.516372700591576\n",
      "Iteration 1057 training loss: 0.4853088896247305, test loss: 0.5180163901474705\n",
      "Iteration 1058 training loss: 0.48035658155321875, test loss: 0.516089338554852\n",
      "Iteration 1059 training loss: 0.48498202524414835, test loss: 0.5177219582885516\n",
      "Iteration 1060 training loss: 0.480054135318461, test loss: 0.5158111857768284\n",
      "Iteration 1061 training loss: 0.4846551021793607, test loss: 0.5174271431033715\n",
      "Iteration 1062 training loss: 0.47975535215438675, test loss: 0.5155356951187314\n",
      "Iteration 1063 training loss: 0.4843213286165249, test loss: 0.5171251174111927\n",
      "Iteration 1064 training loss: 0.4794604882514598, test loss: 0.5152635456615333\n",
      "Iteration 1065 training loss: 0.48399083578466817, test loss: 0.5168265012477589\n",
      "Iteration 1066 training loss: 0.4791806881740777, test loss: 0.5150072061684065\n",
      "Iteration 1067 training loss: 0.48367573940352815, test loss: 0.5165431278535805\n",
      "Iteration 1068 training loss: 0.4789163957098868, test loss: 0.5147659006185314\n",
      "Iteration 1069 training loss: 0.48337209999478564, test loss: 0.5162713205380884\n",
      "Iteration 1070 training loss: 0.47866464486800586, test loss: 0.5145370071241192\n",
      "Iteration 1071 training loss: 0.4830825730017568, test loss: 0.516013797732243\n",
      "Iteration 1072 training loss: 0.47842195338722504, test loss: 0.5143175408447522\n",
      "Iteration 1073 training loss: 0.4827925618656641, test loss: 0.5157561951004374\n",
      "Iteration 1074 training loss: 0.47819057263501813, test loss: 0.5141092732253195\n",
      "Iteration 1075 training loss: 0.48251166797277006, test loss: 0.5155079504549397\n",
      "Iteration 1076 training loss: 0.47797073780267296, test loss: 0.5139123540616859\n",
      "Iteration 1077 training loss: 0.48222581830192346, test loss: 0.5152547572815738\n",
      "Iteration 1078 training loss: 0.47775410012478814, test loss: 0.5137180829667453\n",
      "Iteration 1079 training loss: 0.48194042705318274, test loss: 0.5150025009146033\n",
      "Iteration 1080 training loss: 0.4775411253073927, test loss: 0.5135271295101231\n",
      "Iteration 1081 training loss: 0.4816540280957978, test loss: 0.5147505130122851\n",
      "Iteration 1082 training loss: 0.4773369124301511, test loss: 0.5133459370194703\n",
      "Iteration 1083 training loss: 0.48137182825625197, test loss: 0.5145035831482428\n",
      "Iteration 1084 training loss: 0.47714366484221504, test loss: 0.5131759314213717\n",
      "Iteration 1085 training loss: 0.48110276899471593, test loss: 0.5142700477202745\n",
      "Iteration 1086 training loss: 0.47697296453343363, test loss: 0.5130288222003393\n",
      "Iteration 1087 training loss: 0.48085266253850806, test loss: 0.5140553633093504\n",
      "Iteration 1088 training loss: 0.4768215164593054, test loss: 0.5129012948371401\n",
      "Iteration 1089 training loss: 0.4806120784579844, test loss: 0.513850281407463\n",
      "Iteration 1090 training loss: 0.47668445160447187, test loss: 0.5127877814119319\n",
      "Iteration 1091 training loss: 0.4803699757020249, test loss: 0.5136438984980704\n",
      "Iteration 1092 training loss: 0.4765568866239374, test loss: 0.5126825706584821\n",
      "Iteration 1093 training loss: 0.4801384253557201, test loss: 0.5134494889958533\n",
      "Iteration 1094 training loss: 0.4764353603560133, test loss: 0.5125843767493976\n",
      "Iteration 1095 training loss: 0.47988518700151866, test loss: 0.5132339357978047\n",
      "Iteration 1096 training loss: 0.4763083628667693, test loss: 0.5124798166165807\n",
      "Iteration 1097 training loss: 0.4796132393885685, test loss: 0.5129992755341768\n",
      "Iteration 1098 training loss: 0.47618665976054547, test loss: 0.5123787043021539\n",
      "Iteration 1099 training loss: 0.4793506554527146, test loss: 0.5127734034884712\n",
      "Iteration 1100 training loss: 0.47606331371410704, test loss: 0.5122751015663313\n",
      "Iteration 1101 training loss: 0.479087357716267, test loss: 0.5125470440068821\n",
      "Iteration 1102 training loss: 0.475941019583296, test loss: 0.5121719945138319\n",
      "Iteration 1103 training loss: 0.4788084022062257, test loss: 0.5123057132998677\n",
      "Iteration 1104 training loss: 0.4758028165767133, test loss: 0.5120518985876732\n",
      "Iteration 1105 training loss: 0.47851056524918406, test loss: 0.5120459647052497\n",
      "Iteration 1106 training loss: 0.47566713318277243, test loss: 0.5119336843922385\n",
      "Iteration 1107 training loss: 0.4782089361121961, test loss: 0.5117825653145138\n",
      "Iteration 1108 training loss: 0.47552735194824847, test loss: 0.5118108567455116\n",
      "Iteration 1109 training loss: 0.4779102258668967, test loss: 0.5115218884686216\n",
      "Iteration 1110 training loss: 0.47537236552614015, test loss: 0.5116730486654938\n",
      "Iteration 1111 training loss: 0.47760685625773347, test loss: 0.5112571046899406\n",
      "Iteration 1112 training loss: 0.47521720341092466, test loss: 0.5115358482112501\n",
      "Iteration 1113 training loss: 0.47731384973583546, test loss: 0.5110024804939997\n",
      "Iteration 1114 training loss: 0.47505546027749695, test loss: 0.5113929736164576\n",
      "Iteration 1115 training loss: 0.47701506067554517, test loss: 0.510740646549245\n",
      "Iteration 1116 training loss: 0.47486875263963657, test loss: 0.5112241184404618\n",
      "Iteration 1117 training loss: 0.4767030505982444, test loss: 0.5104657021333397\n",
      "Iteration 1118 training loss: 0.4746648356923005, test loss: 0.5110382318650697\n",
      "Iteration 1119 training loss: 0.4763810952813413, test loss: 0.5101796550724526\n",
      "Iteration 1120 training loss: 0.4744211228344696, test loss: 0.5108125459904295\n",
      "Iteration 1121 training loss: 0.47604324462773984, test loss: 0.509877134314625\n",
      "Iteration 1122 training loss: 0.47414683474562336, test loss: 0.5105559322234141\n",
      "Iteration 1123 training loss: 0.4756911349644406, test loss: 0.5095600205990931\n",
      "Iteration 1124 training loss: 0.4738531738958318, test loss: 0.5102799891035129\n",
      "Iteration 1125 training loss: 0.4753416226884744, test loss: 0.5092443743699896\n",
      "Iteration 1126 training loss: 0.4735320257709816, test loss: 0.5099774224915039\n",
      "Iteration 1127 training loss: 0.47499839700042706, test loss: 0.5089342021421762\n",
      "Iteration 1128 training loss: 0.47318854402713406, test loss: 0.5096538351680517\n",
      "Iteration 1129 training loss: 0.4746592343733071, test loss: 0.5086279884110327\n",
      "Iteration 1130 training loss: 0.47283046413446084, test loss: 0.5093184030054548\n",
      "Iteration 1131 training loss: 0.47432868431985087, test loss: 0.5083287511762855\n",
      "Iteration 1132 training loss: 0.47245749417572125, test loss: 0.5089696744515166\n",
      "Iteration 1133 training loss: 0.4740131410915067, test loss: 0.5080430548822437\n",
      "Iteration 1134 training loss: 0.4720731249448019, test loss: 0.5086110127680324\n",
      "Iteration 1135 training loss: 0.4737206567593023, test loss: 0.5077781963669122\n",
      "Iteration 1136 training loss: 0.47169010371500664, test loss: 0.5082548626586665\n",
      "Iteration 1137 training loss: 0.47344907430654326, test loss: 0.5075318666275546\n",
      "Iteration 1138 training loss: 0.4713095203425167, test loss: 0.5079018625332982\n",
      "Iteration 1139 training loss: 0.47320698095825453, test loss: 0.5073112442215616\n",
      "Iteration 1140 training loss: 0.4709174081080994, test loss: 0.5075378055833957\n",
      "Iteration 1141 training loss: 0.47297761383851583, test loss: 0.5071010952840461\n",
      "Iteration 1142 training loss: 0.47052513515402666, test loss: 0.5071747634272424\n",
      "Iteration 1143 training loss: 0.47277366965916834, test loss: 0.506913736216171\n",
      "Iteration 1144 training loss: 0.4701341792438788, test loss: 0.5068143672050893\n",
      "Iteration 1145 training loss: 0.4725911811624896, test loss: 0.5067444927345944\n",
      "Iteration 1146 training loss: 0.4697359906538842, test loss: 0.5064456864044714\n",
      "Iteration 1147 training loss: 0.4724459437691793, test loss: 0.5066111603698551\n",
      "Iteration 1148 training loss: 0.4693490182698557, test loss: 0.5060905161089146\n",
      "Iteration 1149 training loss: 0.4723101157879241, test loss: 0.5064848547691717\n",
      "Iteration 1150 training loss: 0.46897162061180164, test loss: 0.5057443407849045\n",
      "Iteration 1151 training loss: 0.4721930436191883, test loss: 0.5063757226041562\n",
      "Iteration 1152 training loss: 0.46860198155301636, test loss: 0.5054040842899432\n",
      "Iteration 1153 training loss: 0.4720937319997931, test loss: 0.5062829982416103\n",
      "Iteration 1154 training loss: 0.4682521281113331, test loss: 0.5050822175625441\n",
      "Iteration 1155 training loss: 0.4720278475247709, test loss: 0.5062237792643718\n",
      "Iteration 1156 training loss: 0.4679576945944616, test loss: 0.5048150054284012\n",
      "Iteration 1157 training loss: 0.4720321413302337, test loss: 0.506235279312322\n",
      "Iteration 1158 training loss: 0.46773905068821264, test loss: 0.5046232803021352\n",
      "Iteration 1159 training loss: 0.47208760642419956, test loss: 0.5062936538514642\n",
      "Iteration 1160 training loss: 0.4676075749016476, test loss: 0.504513293514473\n",
      "Iteration 1161 training loss: 0.4722043010536071, test loss: 0.5064110922701579\n",
      "Iteration 1162 training loss: 0.46758343582604694, test loss: 0.504507826808613\n",
      "Iteration 1163 training loss: 0.47241172952442595, test loss: 0.5066183666506271\n",
      "Iteration 1164 training loss: 0.4677109999891012, test loss: 0.5046527314238146\n",
      "Iteration 1165 training loss: 0.4726974185461269, test loss: 0.5069046772239366\n",
      "Iteration 1166 training loss: 0.4679982384794856, test loss: 0.5049533264444975\n",
      "Iteration 1167 training loss: 0.473025533193124, test loss: 0.507238053740417\n",
      "Iteration 1168 training loss: 0.46845034046469114, test loss: 0.5054108429006842\n",
      "Iteration 1169 training loss: 0.4734141382700401, test loss: 0.5076373412647088\n",
      "Iteration 1170 training loss: 0.4691082906217463, test loss: 0.5060656328460957\n",
      "Iteration 1171 training loss: 0.47385994954757, test loss: 0.5081025643920144\n",
      "Iteration 1172 training loss: 0.4699711526972179, test loss: 0.5069106143347185\n",
      "Iteration 1173 training loss: 0.47435731479100174, test loss: 0.5086258654118474\n",
      "Iteration 1174 training loss: 0.47102725850981025, test loss: 0.5079371831155033\n",
      "Iteration 1175 training loss: 0.474895335319452, test loss: 0.5091982990932069\n",
      "Iteration 1176 training loss: 0.4721816600891122, test loss: 0.5090559408474418\n",
      "Iteration 1177 training loss: 0.475369720030932, test loss: 0.5097227797819304\n",
      "Iteration 1178 training loss: 0.47330402728758764, test loss: 0.5101333560193784\n",
      "Iteration 1179 training loss: 0.47570250321690516, test loss: 0.5101093269144634\n",
      "Iteration 1180 training loss: 0.47422063535666426, test loss: 0.5109986632955816\n",
      "Iteration 1181 training loss: 0.4757758881274615, test loss: 0.5102438355564369\n",
      "Iteration 1182 training loss: 0.47476840598723163, test loss: 0.5114896071183443\n",
      "Iteration 1183 training loss: 0.47552863921983674, test loss: 0.5100644325090155\n",
      "Iteration 1184 training loss: 0.47486112786752466, test loss: 0.511523871145785\n",
      "Iteration 1185 training loss: 0.474906265837129, test loss: 0.5095106754194032\n",
      "Iteration 1186 training loss: 0.4744036330034551, test loss: 0.5110089084540871\n",
      "Iteration 1187 training loss: 0.4738869652876396, test loss: 0.508560490402126\n",
      "Iteration 1188 training loss: 0.4734602232872999, test loss: 0.5100135294827902\n",
      "Iteration 1189 training loss: 0.4725809952810829, test loss: 0.5073219463816887\n",
      "Iteration 1190 training loss: 0.4721868567312229, test loss: 0.5086960738174793\n",
      "Iteration 1191 training loss: 0.4711166995124975, test loss: 0.5059260801751567\n",
      "Iteration 1192 training loss: 0.47072344088240997, test loss: 0.5072008442543454\n",
      "Iteration 1193 training loss: 0.46958409606147367, test loss: 0.5044601557383528\n",
      "Iteration 1194 training loss: 0.4691907260537806, test loss: 0.5056457122373684\n",
      "Iteration 1195 training loss: 0.46805553102326075, test loss: 0.5029939464224678\n",
      "Iteration 1196 training loss: 0.46767532361405056, test loss: 0.5041178830554415\n",
      "Iteration 1197 training loss: 0.46660183215305134, test loss: 0.5016015326181333\n",
      "Iteration 1198 training loss: 0.4662337551706237, test loss: 0.5026734055590522\n",
      "Iteration 1199 training loss: 0.46525754843978195, test loss: 0.5003172811184777\n",
      "Iteration 1200 training loss: 0.4649259740859279, test loss: 0.5013731893389921\n",
      "Iteration 1201 training loss: 0.4640733885448885, test loss: 0.49918826779399117\n",
      "Iteration 1202 training loss: 0.46377207490447325, test loss: 0.5002341502763731\n",
      "Iteration 1203 training loss: 0.46303303859134226, test loss: 0.4981970021892779\n",
      "Iteration 1204 training loss: 0.46275621181502125, test loss: 0.4992443068890594\n",
      "Iteration 1205 training loss: 0.462128996601517, test loss: 0.49733417643140193\n",
      "Iteration 1206 training loss: 0.46188600626087617, test loss: 0.4984106717599072\n",
      "Iteration 1207 training loss: 0.4613689520057178, test loss: 0.49660731937307245\n",
      "Iteration 1208 training loss: 0.4611495368695014, test loss: 0.4977185790174906\n",
      "Iteration 1209 training loss: 0.4607361514488658, test loss: 0.4959992906513544\n",
      "Iteration 1210 training loss: 0.46054388363599885, test loss: 0.49716792916780994\n",
      "Iteration 1211 training loss: 0.4602489135659223, test loss: 0.4955243988515096\n",
      "Iteration 1212 training loss: 0.46006336018239674, test loss: 0.49675272909766555\n",
      "Iteration 1213 training loss: 0.45989922899512575, test loss: 0.49517480796237995\n",
      "Iteration 1214 training loss: 0.4596981657770014, test loss: 0.4964653182305977\n",
      "Iteration 1215 training loss: 0.4597232567107613, test loss: 0.49498249501490926\n",
      "Iteration 1216 training loss: 0.4594670085268542, test loss: 0.49632739380363927\n",
      "Iteration 1217 training loss: 0.45977463408008257, test loss: 0.4950024088302871\n",
      "Iteration 1218 training loss: 0.459408411249489, test loss: 0.4963807600156114\n",
      "Iteration 1219 training loss: 0.46014095409777184, test loss: 0.4953214142169313\n",
      "Iteration 1220 training loss: 0.4595406216461704, test loss: 0.49664355641555485\n",
      "Iteration 1221 training loss: 0.4608914966382381, test loss: 0.4960094562388966\n",
      "Iteration 1222 training loss: 0.4598799841299369, test loss: 0.4971313197363843\n",
      "Iteration 1223 training loss: 0.46212639243006504, test loss: 0.4971712339709096\n",
      "Iteration 1224 training loss: 0.4604644500234577, test loss: 0.49787740345959575\n",
      "Iteration 1225 training loss: 0.46392282350971226, test loss: 0.49889667880949035\n",
      "Iteration 1226 training loss: 0.46129657926031525, test loss: 0.49886872182057634\n",
      "Iteration 1227 training loss: 0.4662564801286711, test loss: 0.5011687919896671\n",
      "Iteration 1228 training loss: 0.4623868882865799, test loss: 0.5001040176548593\n",
      "Iteration 1229 training loss: 0.46890775482674646, test loss: 0.5037772587350229\n",
      "Iteration 1230 training loss: 0.46377136982188216, test loss: 0.5015885373635263\n",
      "Iteration 1231 training loss: 0.47162783789782153, test loss: 0.5064745312352525\n",
      "Iteration 1232 training loss: 0.4656069875019711, test loss: 0.5034757672384406\n",
      "Iteration 1233 training loss: 0.4741218531312586, test loss: 0.5089785509236905\n",
      "Iteration 1234 training loss: 0.46803270690223364, test loss: 0.5059067577893244\n",
      "Iteration 1235 training loss: 0.47614681938304954, test loss: 0.5110323088372264\n",
      "Iteration 1236 training loss: 0.4709569633928823, test loss: 0.5087983822308448\n",
      "Iteration 1237 training loss: 0.4775528684477131, test loss: 0.512483747903295\n",
      "Iteration 1238 training loss: 0.4739357775328889, test loss: 0.5117264950708406\n",
      "Iteration 1239 training loss: 0.4781880006657167, test loss: 0.5131684297086746\n",
      "Iteration 1240 training loss: 0.4763077698724222, test loss: 0.5140332323544166\n",
      "Iteration 1241 training loss: 0.477803503934562, test loss: 0.5128231411815976\n",
      "Iteration 1242 training loss: 0.4772317911413727, test loss: 0.5148675542600704\n",
      "Iteration 1243 training loss: 0.47635069849845624, test loss: 0.5114115940980118\n",
      "Iteration 1244 training loss: 0.47646650405634483, test loss: 0.5139961988747002\n",
      "Iteration 1245 training loss: 0.47412390039813573, test loss: 0.5092217962240793\n",
      "Iteration 1246 training loss: 0.4745062740227728, test loss: 0.5119395214349396\n",
      "Iteration 1247 training loss: 0.4715895336322185, test loss: 0.506732790306616\n",
      "Iteration 1248 training loss: 0.47196341526503943, test loss: 0.5093229469129112\n",
      "Iteration 1249 training loss: 0.4690471486020342, test loss: 0.504241493558226\n",
      "Iteration 1250 training loss: 0.4692620973072525, test loss: 0.5065720689419626\n",
      "Iteration 1251 training loss: 0.4667106917930353, test loss: 0.5019615571014849\n",
      "Iteration 1252 training loss: 0.4667709900801224, test loss: 0.5040554136540067\n",
      "Iteration 1253 training loss: 0.46468110374170857, test loss: 0.4999906224709711\n",
      "Iteration 1254 training loss: 0.46460475066401014, test loss: 0.5018829860932564\n",
      "Iteration 1255 training loss: 0.4630752357867548, test loss: 0.49843996063622464\n",
      "Iteration 1256 training loss: 0.46283810765815375, test loss: 0.5001326952303956\n",
      "Iteration 1257 training loss: 0.46182118629438024, test loss: 0.4972362143306953\n",
      "Iteration 1258 training loss: 0.4614460040365455, test loss: 0.4987767747260676\n",
      "Iteration 1259 training loss: 0.46091142921107464, test loss: 0.4963725409789741\n",
      "Iteration 1260 training loss: 0.46039681258836146, test loss: 0.49777295920398673\n",
      "Iteration 1261 training loss: 0.46026868192332404, test loss: 0.49576885985691843\n",
      "Iteration 1262 training loss: 0.45960567673089076, test loss: 0.4970344287564191\n",
      "Iteration 1263 training loss: 0.45986485871205535, test loss: 0.49539796282398135\n",
      "Iteration 1264 training loss: 0.4590394920478919, test loss: 0.4965260858726895\n",
      "Iteration 1265 training loss: 0.45963414493586957, test loss: 0.49519555971862683\n",
      "Iteration 1266 training loss: 0.4586167799926814, test loss: 0.49616109721348256\n",
      "Iteration 1267 training loss: 0.4595257952514277, test loss: 0.4951119787033853\n",
      "Iteration 1268 training loss: 0.4583047614441978, test loss: 0.49590609021722093\n",
      "Iteration 1269 training loss: 0.4595049883118177, test loss: 0.49511212912748226\n",
      "Iteration 1270 training loss: 0.45807292348051987, test loss: 0.4957286259618314\n",
      "Iteration 1271 training loss: 0.4595368628410325, test loss: 0.49516140027164857\n",
      "Iteration 1272 training loss: 0.45787557857584255, test loss: 0.49558093557445465\n",
      "Iteration 1273 training loss: 0.4595870787692151, test loss: 0.4952270440153232\n",
      "Iteration 1274 training loss: 0.4576909026019639, test loss: 0.4954411684896829\n",
      "Iteration 1275 training loss: 0.45960517157076675, test loss: 0.4952596226955181\n",
      "Iteration 1276 training loss: 0.45747050585352167, test loss: 0.49525812142807774\n",
      "Iteration 1277 training loss: 0.45955754677669763, test loss: 0.49522635750313904\n",
      "Iteration 1278 training loss: 0.45720038473661734, test loss: 0.49502049541960524\n",
      "Iteration 1279 training loss: 0.45943190554832286, test loss: 0.49511668775314793\n",
      "Iteration 1280 training loss: 0.4568937180713607, test loss: 0.4947414299196279\n",
      "Iteration 1281 training loss: 0.45925191388398573, test loss: 0.49495499413445393\n",
      "Iteration 1282 training loss: 0.4565575415415608, test loss: 0.494431410421736\n",
      "Iteration 1283 training loss: 0.4590140998540531, test loss: 0.4947369334008343\n",
      "Iteration 1284 training loss: 0.45620040939899287, test loss: 0.49409873547967126\n",
      "Iteration 1285 training loss: 0.4587451401888994, test loss: 0.4944894084400208\n",
      "Iteration 1286 training loss: 0.4558376155993659, test loss: 0.49375982002658686\n",
      "Iteration 1287 training loss: 0.45845520218480096, test loss: 0.494221503014086\n",
      "Iteration 1288 training loss: 0.4554754996441277, test loss: 0.4934202960503718\n",
      "Iteration 1289 training loss: 0.45816439442448204, test loss: 0.4939553190803267\n",
      "Iteration 1290 training loss: 0.4551331731447812, test loss: 0.4931017532171365\n",
      "Iteration 1291 training loss: 0.45788967599386493, test loss: 0.49370518509320827\n",
      "Iteration 1292 training loss: 0.4548157444699194, test loss: 0.49280893024431527\n",
      "Iteration 1293 training loss: 0.4576212422429879, test loss: 0.4934618002610393\n",
      "Iteration 1294 training loss: 0.45452700066244767, test loss: 0.4925442286085356\n",
      "Iteration 1295 training loss: 0.45738014464157206, test loss: 0.49324479571784724\n",
      "Iteration 1296 training loss: 0.4542407251793544, test loss: 0.4922807792685365\n",
      "Iteration 1297 training loss: 0.4571450235104136, test loss: 0.49303473373765844\n",
      "Iteration 1298 training loss: 0.45396954063598693, test loss: 0.4920323957768639\n",
      "Iteration 1299 training loss: 0.4569187779459486, test loss: 0.49283249721457795\n",
      "Iteration 1300 training loss: 0.4537055314017907, test loss: 0.4917908188656445\n",
      "Iteration 1301 training loss: 0.4566994941746026, test loss: 0.4926373493965226\n",
      "Iteration 1302 training loss: 0.45345442229358596, test loss: 0.4915615127920203\n",
      "Iteration 1303 training loss: 0.45649162604963356, test loss: 0.4924529497822439\n",
      "Iteration 1304 training loss: 0.45320772306291274, test loss: 0.49133574082779824\n",
      "Iteration 1305 training loss: 0.45627924411763937, test loss: 0.49226465668018765\n",
      "Iteration 1306 training loss: 0.4529600540521545, test loss: 0.4911080945017084\n",
      "Iteration 1307 training loss: 0.4560662154414657, test loss: 0.492076112501839\n",
      "Iteration 1308 training loss: 0.45270953395143804, test loss: 0.49087806111753746\n",
      "Iteration 1309 training loss: 0.45584078026832353, test loss: 0.4918756264416459\n",
      "Iteration 1310 training loss: 0.4524704671610733, test loss: 0.4906601607789529\n",
      "Iteration 1311 training loss: 0.4556283757423071, test loss: 0.49168819612306713\n",
      "Iteration 1312 training loss: 0.4522304941023366, test loss: 0.49044119259832997\n",
      "Iteration 1313 training loss: 0.4554132694724762, test loss: 0.4914980066721566\n",
      "Iteration 1314 training loss: 0.4520026373993409, test loss: 0.4902341048354281\n",
      "Iteration 1315 training loss: 0.45521236801096454, test loss: 0.4913214280085267\n",
      "Iteration 1316 training loss: 0.4517831297029915, test loss: 0.49003507727125034\n",
      "Iteration 1317 training loss: 0.4550129137672309, test loss: 0.491145620326461\n",
      "Iteration 1318 training loss: 0.45157146061360337, test loss: 0.4898437183281415\n",
      "Iteration 1319 training loss: 0.45483078444333347, test loss: 0.49098700413812757\n",
      "Iteration 1320 training loss: 0.45136499301062366, test loss: 0.4896573399068252\n",
      "Iteration 1321 training loss: 0.45463706165691237, test loss: 0.49081660324442755\n",
      "Iteration 1322 training loss: 0.451157173181512, test loss: 0.4894689068318127\n",
      "Iteration 1323 training loss: 0.4544439977670881, test loss: 0.4906457157219651\n",
      "Iteration 1324 training loss: 0.45095350264897943, test loss: 0.4892836883605999\n",
      "Iteration 1325 training loss: 0.4542496020327741, test loss: 0.490474088580612\n",
      "Iteration 1326 training loss: 0.4507535416180783, test loss: 0.4891024739234636\n",
      "Iteration 1327 training loss: 0.45405340307353653, test loss: 0.49029968187674433\n",
      "Iteration 1328 training loss: 0.4505463932069288, test loss: 0.48891315169864785\n",
      "Iteration 1329 training loss: 0.45384065284661707, test loss: 0.49010819601268324\n",
      "Iteration 1330 training loss: 0.4503336529709877, test loss: 0.48871711844255206\n",
      "Iteration 1331 training loss: 0.45362150509367755, test loss: 0.48991080716922\n",
      "Iteration 1332 training loss: 0.45011940292714087, test loss: 0.4885192592849015\n",
      "Iteration 1333 training loss: 0.4533943483565489, test loss: 0.48970520016377606\n",
      "Iteration 1334 training loss: 0.4499057771695638, test loss: 0.4883206359136002\n",
      "Iteration 1335 training loss: 0.45316253915338456, test loss: 0.4894947323655125\n",
      "Iteration 1336 training loss: 0.44969329876826747, test loss: 0.4881235443375154\n",
      "Iteration 1337 training loss: 0.4529353310100934, test loss: 0.4892897163793931\n",
      "Iteration 1338 training loss: 0.4494731062008357, test loss: 0.4879191405694679\n",
      "Iteration 1339 training loss: 0.4526971967917541, test loss: 0.4890748022315005\n",
      "Iteration 1340 training loss: 0.4492511569389311, test loss: 0.48771327134212145\n",
      "Iteration 1341 training loss: 0.45245002419096486, test loss: 0.4888513215559371\n",
      "Iteration 1342 training loss: 0.4490303364208932, test loss: 0.48750879752431137\n",
      "Iteration 1343 training loss: 0.45221543584190327, test loss: 0.48864148319227524\n",
      "Iteration 1344 training loss: 0.44882295354647195, test loss: 0.4873186522700261\n",
      "Iteration 1345 training loss: 0.45199165048014917, test loss: 0.4884425846126513\n",
      "Iteration 1346 training loss: 0.44862308477731244, test loss: 0.4871366815820099\n",
      "Iteration 1347 training loss: 0.45178114862514573, test loss: 0.4882569119499044\n",
      "Iteration 1348 training loss: 0.4484356435825641, test loss: 0.48696711797731457\n",
      "Iteration 1349 training loss: 0.45157725620029726, test loss: 0.48807753399040393\n",
      "Iteration 1350 training loss: 0.4482511891543921, test loss: 0.4868008967193078\n",
      "Iteration 1351 training loss: 0.4513746990692345, test loss: 0.48789936629008535\n",
      "Iteration 1352 training loss: 0.448078122267661, test loss: 0.48664525582561363\n",
      "Iteration 1353 training loss: 0.4511860129212135, test loss: 0.48773490523387053\n",
      "Iteration 1354 training loss: 0.4479106278770627, test loss: 0.48649546372710656\n",
      "Iteration 1355 training loss: 0.4509959056713458, test loss: 0.4875690535879172\n",
      "Iteration 1356 training loss: 0.4477507360367927, test loss: 0.4863525591484317\n",
      "Iteration 1357 training loss: 0.4508127686035304, test loss: 0.4874103441803345\n",
      "Iteration 1358 training loss: 0.44759640429984215, test loss: 0.4862151797356524\n",
      "Iteration 1359 training loss: 0.45064535687122154, test loss: 0.4872680053531833\n",
      "Iteration 1360 training loss: 0.4474652032061653, test loss: 0.4861026259183219\n",
      "Iteration 1361 training loss: 0.45049245017694955, test loss: 0.4871403150800438\n",
      "Iteration 1362 training loss: 0.4473432741407935, test loss: 0.48599819898666663\n",
      "Iteration 1363 training loss: 0.4503392262986889, test loss: 0.4870136815650406\n",
      "Iteration 1364 training loss: 0.4472367925242581, test loss: 0.485909335754713\n",
      "Iteration 1365 training loss: 0.45019656166337946, test loss: 0.48689801859890985\n",
      "Iteration 1366 training loss: 0.44714780037657864, test loss: 0.4858376935237936\n",
      "Iteration 1367 training loss: 0.45006036772970154, test loss: 0.48678910620130483\n",
      "Iteration 1368 training loss: 0.44708507629788985, test loss: 0.48579164618741233\n",
      "Iteration 1369 training loss: 0.4499531556442623, test loss: 0.4867114001600391\n",
      "Iteration 1370 training loss: 0.4470576269609705, test loss: 0.4857825269412666\n",
      "Iteration 1371 training loss: 0.44985735525822385, test loss: 0.48664660785857694\n",
      "Iteration 1372 training loss: 0.4470662135573771, test loss: 0.48580985104626806\n",
      "Iteration 1373 training loss: 0.44977602174594705, test loss: 0.4865969905830794\n",
      "Iteration 1374 training loss: 0.4471163596737035, test loss: 0.48587730774622423\n",
      "Iteration 1375 training loss: 0.4497155206384198, test loss: 0.4865677927707161\n",
      "Iteration 1376 training loss: 0.44721667800216136, test loss: 0.48599289814928387\n",
      "Iteration 1377 training loss: 0.44969421752331107, test loss: 0.4865791645343641\n",
      "Iteration 1378 training loss: 0.44739794950821926, test loss: 0.4861881137234446\n",
      "Iteration 1379 training loss: 0.44970818555929226, test loss: 0.4866254528454156\n",
      "Iteration 1380 training loss: 0.4476362344647862, test loss: 0.4864356855391273\n",
      "Iteration 1381 training loss: 0.4497184588062953, test loss: 0.48666880460837136\n",
      "Iteration 1382 training loss: 0.4479161806869759, test loss: 0.4867210581913203\n",
      "Iteration 1383 training loss: 0.4497464547613693, test loss: 0.4867320007160213\n",
      "Iteration 1384 training loss: 0.4482645978928643, test loss: 0.487072657433758\n",
      "Iteration 1385 training loss: 0.4497931042687032, test loss: 0.4868154911904966\n",
      "Iteration 1386 training loss: 0.44867056820268053, test loss: 0.4874778333025487\n",
      "Iteration 1387 training loss: 0.4498435924575352, test loss: 0.4869040663092437\n",
      "Iteration 1388 training loss: 0.44910784391219255, test loss: 0.4879100043135719\n",
      "Iteration 1389 training loss: 0.44986302841110554, test loss: 0.4869629742181016\n",
      "Iteration 1390 training loss: 0.44951547776851364, test loss: 0.48830378560807897\n",
      "Iteration 1391 training loss: 0.44982865504080694, test loss: 0.4869712277570512\n",
      "Iteration 1392 training loss: 0.449900637354631, test loss: 0.48867423655417863\n",
      "Iteration 1393 training loss: 0.4497595716381185, test loss: 0.48694654403901705\n",
      "Iteration 1394 training loss: 0.4502391769813511, test loss: 0.4889987483042938\n",
      "Iteration 1395 training loss: 0.4496266760520855, test loss: 0.48685895842418925\n",
      "Iteration 1396 training loss: 0.45042678220527765, test loss: 0.4891693799591609\n",
      "Iteration 1397 training loss: 0.4493648343901118, test loss: 0.4866414635418636\n",
      "Iteration 1398 training loss: 0.4504280186513134, test loss: 0.4891492243737343\n",
      "Iteration 1399 training loss: 0.44899607012263115, test loss: 0.486319451725163\n",
      "Iteration 1400 training loss: 0.4502628450341605, test loss: 0.48896252058343725\n",
      "Iteration 1401 training loss: 0.44851353827357004, test loss: 0.48588304272774957\n",
      "Iteration 1402 training loss: 0.44988109997010495, test loss: 0.4885583434757602\n",
      "Iteration 1403 training loss: 0.44791915174917024, test loss: 0.4853378143077295\n",
      "Iteration 1404 training loss: 0.4493321924987814, test loss: 0.4879875933547726\n",
      "Iteration 1405 training loss: 0.4472491553457603, test loss: 0.48471550616937603\n",
      "Iteration 1406 training loss: 0.4486385880178285, test loss: 0.48727209450776693\n",
      "Iteration 1407 training loss: 0.4464882331782421, test loss: 0.4840001830165624\n",
      "Iteration 1408 training loss: 0.447838457935806, test loss: 0.4864534321388304\n",
      "Iteration 1409 training loss: 0.44571924674213914, test loss: 0.4832770956248945\n",
      "Iteration 1410 training loss: 0.4470118588257638, test loss: 0.4856158238034064\n",
      "Iteration 1411 training loss: 0.44494974150885874, test loss: 0.48255489801778756\n",
      "Iteration 1412 training loss: 0.4461837304008852, test loss: 0.48478273116114207\n",
      "Iteration 1413 training loss: 0.4442104660135086, test loss: 0.4818620002266775\n",
      "Iteration 1414 training loss: 0.44539508250852244, test loss: 0.4839956860779348\n",
      "Iteration 1415 training loss: 0.4435381136734896, test loss: 0.48123266956235955\n",
      "Iteration 1416 training loss: 0.44467035998282584, test loss: 0.48327981668366954\n",
      "Iteration 1417 training loss: 0.4429289371714762, test loss: 0.4806621166191761\n",
      "Iteration 1418 training loss: 0.4440047837618159, test loss: 0.48262991699044316\n",
      "Iteration 1419 training loss: 0.4423805482760231, test loss: 0.4801498965804462\n",
      "Iteration 1420 training loss: 0.443405375279871, test loss: 0.48205439313209997\n",
      "Iteration 1421 training loss: 0.44190857399868194, test loss: 0.4797108658280962\n",
      "Iteration 1422 training loss: 0.44289415029490514, test loss: 0.481574780478699\n",
      "Iteration 1423 training loss: 0.44152262309764867, test loss: 0.4793535774054094\n",
      "Iteration 1424 training loss: 0.4424677385076243, test loss: 0.4811857761346369\n",
      "Iteration 1425 training loss: 0.44121928662299686, test loss: 0.47907305307600473\n",
      "Iteration 1426 training loss: 0.44210445908181784, test loss: 0.48086615139675004\n",
      "Iteration 1427 training loss: 0.44098910156504617, test loss: 0.47885848548117127\n",
      "Iteration 1428 training loss: 0.4418233028817984, test loss: 0.4806364373264205\n",
      "Iteration 1429 training loss: 0.44088218644808186, test loss: 0.4787582719662364\n",
      "Iteration 1430 training loss: 0.4416381862985023, test loss: 0.4805126227606275\n",
      "Iteration 1431 training loss: 0.4409117675946556, test loss: 0.4787856415853411\n",
      "Iteration 1432 training loss: 0.44154177616528006, test loss: 0.48048641155173866\n",
      "Iteration 1433 training loss: 0.44111098644588903, test loss: 0.478976853278713\n",
      "Iteration 1434 training loss: 0.4415653328284091, test loss: 0.48059221258205215\n",
      "Iteration 1435 training loss: 0.44154482136126416, test loss: 0.47939414655079127\n",
      "Iteration 1436 training loss: 0.44172424068322447, test loss: 0.48084503911084103\n",
      "Iteration 1437 training loss: 0.44227794375936164, test loss: 0.48010360141896236\n",
      "Iteration 1438 training loss: 0.442010745751724, test loss: 0.48123351553262317\n",
      "Iteration 1439 training loss: 0.44331030198198884, test loss: 0.48110458490262054\n",
      "Iteration 1440 training loss: 0.4423993270748414, test loss: 0.48172322955136015\n",
      "Iteration 1441 training loss: 0.4446162063650918, test loss: 0.48237004406980677\n",
      "Iteration 1442 training loss: 0.4428533826294761, test loss: 0.4822760664321606\n",
      "Iteration 1443 training loss: 0.4461368632462211, test loss: 0.48384664435435387\n",
      "Iteration 1444 training loss: 0.44331527073317495, test loss: 0.48282246535979184\n",
      "Iteration 1445 training loss: 0.44766171522161086, test loss: 0.48532216579162823\n",
      "Iteration 1446 training loss: 0.4437202740737259, test loss: 0.48328137728347803\n",
      "Iteration 1447 training loss: 0.44899307625338986, test loss: 0.48660569590346003\n",
      "Iteration 1448 training loss: 0.4440901740458818, test loss: 0.4836742637447716\n",
      "Iteration 1449 training loss: 0.4500088654478265, test loss: 0.4875735232813507\n",
      "Iteration 1450 training loss: 0.44447648671472234, test loss: 0.4840606919756402\n",
      "Iteration 1451 training loss: 0.45072061976412325, test loss: 0.4882390830192022\n",
      "Iteration 1452 training loss: 0.4450526120587178, test loss: 0.48462726685343605\n",
      "Iteration 1453 training loss: 0.45129629922291664, test loss: 0.48876982517939704\n",
      "Iteration 1454 training loss: 0.44584804669112243, test loss: 0.4854023709814476\n",
      "Iteration 1455 training loss: 0.45176360243544805, test loss: 0.4892028600249796\n",
      "Iteration 1456 training loss: 0.44686217762858294, test loss: 0.48638473060950044\n",
      "Iteration 1457 training loss: 0.4521587654321506, test loss: 0.48957638756681443\n",
      "Iteration 1458 training loss: 0.4479737326111227, test loss: 0.48745775668528907\n",
      "Iteration 1459 training loss: 0.45244225478829586, test loss: 0.4898598216037025\n",
      "Iteration 1460 training loss: 0.44903132175289573, test loss: 0.4884706181489825\n",
      "Iteration 1461 training loss: 0.45256064445196387, test loss: 0.48999515757708695\n",
      "Iteration 1462 training loss: 0.44977645083334966, test loss: 0.4891671735709033\n",
      "Iteration 1463 training loss: 0.4523442762439045, test loss: 0.4898005416353061\n",
      "Iteration 1464 training loss: 0.4500661206055376, test loss: 0.48940445015311707\n",
      "Iteration 1465 training loss: 0.45169990064244475, test loss: 0.4891851615763053\n",
      "Iteration 1466 training loss: 0.449741702766376, test loss: 0.4890303106443791\n",
      "Iteration 1467 training loss: 0.4506226895891348, test loss: 0.4881428145280357\n",
      "Iteration 1468 training loss: 0.4488594999015815, test loss: 0.48810365845292925\n",
      "Iteration 1469 training loss: 0.44916966620019755, test loss: 0.4867235470288164\n",
      "Iteration 1470 training loss: 0.4475506955865097, test loss: 0.48675656511816034\n",
      "Iteration 1471 training loss: 0.4474878624576882, test loss: 0.485077692030829\n",
      "Iteration 1472 training loss: 0.4459820513215066, test loss: 0.4851641040420237\n",
      "Iteration 1473 training loss: 0.44575368700486157, test loss: 0.4833840137417765\n",
      "Iteration 1474 training loss: 0.4443596278883566, test loss: 0.48352979212664077\n",
      "Iteration 1475 training loss: 0.4441209042850139, test loss: 0.48179147526593796\n",
      "Iteration 1476 training loss: 0.4428318506142334, test loss: 0.4820034406115704\n",
      "Iteration 1477 training loss: 0.4426449403640326, test loss: 0.48035728591692034\n",
      "Iteration 1478 training loss: 0.4414590368026136, test loss: 0.4806440762197041\n",
      "Iteration 1479 training loss: 0.44137279505721133, test loss: 0.47912425353103927\n",
      "Iteration 1480 training loss: 0.4402566843669422, test loss: 0.47946297901416546\n",
      "Iteration 1481 training loss: 0.44031384091820447, test loss: 0.47810292427855183\n",
      "Iteration 1482 training loss: 0.4392375167981699, test loss: 0.4784752968855719\n",
      "Iteration 1483 training loss: 0.4394524800964414, test loss: 0.4772758033910468\n",
      "Iteration 1484 training loss: 0.43840163332540144, test loss: 0.47767702164779974\n",
      "Iteration 1485 training loss: 0.4387693280007788, test loss: 0.47661888902213995\n",
      "Iteration 1486 training loss: 0.4377025721526775, test loss: 0.4770217447788785\n",
      "Iteration 1487 training loss: 0.4382627131340893, test loss: 0.4761367886782045\n",
      "Iteration 1488 training loss: 0.4371530495980149, test loss: 0.4765199655405026\n",
      "Iteration 1489 training loss: 0.4379108623708339, test loss: 0.4758076315197119\n",
      "Iteration 1490 training loss: 0.43672800554473945, test loss: 0.47614587685300075\n",
      "Iteration 1491 training loss: 0.43771119330172775, test loss: 0.47562845077552157\n",
      "Iteration 1492 training loss: 0.43641398578548435, test loss: 0.4758842166923547\n",
      "Iteration 1493 training loss: 0.43764135117964736, test loss: 0.4755771902534246\n",
      "Iteration 1494 training loss: 0.43618723876596244, test loss: 0.4757108612889477\n",
      "Iteration 1495 training loss: 0.43769100604103583, test loss: 0.4756433773953005\n",
      "Iteration 1496 training loss: 0.43605299760109617, test loss: 0.4756287503947985\n",
      "Iteration 1497 training loss: 0.4378448336779934, test loss: 0.47580871889632154\n",
      "Iteration 1498 training loss: 0.43596407963100664, test loss: 0.4755874096852624\n",
      "Iteration 1499 training loss: 0.4380652162467075, test loss: 0.47604147794953716\n",
      "Iteration 1500 training loss: 0.4359191534440873, test loss: 0.47558687912001335\n",
      "Iteration 1501 training loss: 0.4383304121752726, test loss: 0.4763184510243544\n",
      "Iteration 1502 training loss: 0.4359012864563553, test loss: 0.4756067266533072\n",
      "Iteration 1503 training loss: 0.43863177568227674, test loss: 0.47663500223063027\n",
      "Iteration 1504 training loss: 0.4359094562461755, test loss: 0.47564423500478414\n",
      "Iteration 1505 training loss: 0.43893450802390055, test loss: 0.4769547338566229\n",
      "Iteration 1506 training loss: 0.43594663235499204, test loss: 0.47570651307330636\n",
      "Iteration 1507 training loss: 0.43922778818894864, test loss: 0.47726935917750435\n",
      "Iteration 1508 training loss: 0.4360173541085383, test loss: 0.47579663303622954\n",
      "Iteration 1509 training loss: 0.43949987848986394, test loss: 0.47756306522615444\n",
      "Iteration 1510 training loss: 0.43613320348299583, test loss: 0.4759254965671882\n",
      "Iteration 1511 training loss: 0.4397882535835934, test loss: 0.4778786060810301\n",
      "Iteration 1512 training loss: 0.4363702433882278, test loss: 0.47617250788683624\n",
      "Iteration 1513 training loss: 0.44012347395251183, test loss: 0.47824270205254726\n",
      "Iteration 1514 training loss: 0.43671554823170516, test loss: 0.4765209601257621\n",
      "Iteration 1515 training loss: 0.4404443201630559, test loss: 0.47859261642198636\n",
      "Iteration 1516 training loss: 0.43715223100927003, test loss: 0.47695242541817606\n",
      "Iteration 1517 training loss: 0.4407311685880201, test loss: 0.4789091317682827\n",
      "Iteration 1518 training loss: 0.4376667495158047, test loss: 0.47745392992089136\n",
      "Iteration 1519 training loss: 0.4409988271865178, test loss: 0.4792099680570552\n",
      "Iteration 1520 training loss: 0.4382882024555792, test loss: 0.47806123910629106\n",
      "Iteration 1521 training loss: 0.4412489201104382, test loss: 0.4794988372096508\n",
      "Iteration 1522 training loss: 0.4389623822739942, test loss: 0.4787202469022119\n",
      "Iteration 1523 training loss: 0.4414146041110539, test loss: 0.4797004380952657\n",
      "Iteration 1524 training loss: 0.4396214163902524, test loss: 0.4793572978441768\n",
      "Iteration 1525 training loss: 0.44150221101668624, test loss: 0.47982486036259925\n",
      "Iteration 1526 training loss: 0.4402166142062846, test loss: 0.4799284442241035\n",
      "Iteration 1527 training loss: 0.44148532989067996, test loss: 0.479843979720291\n",
      "Iteration 1528 training loss: 0.4406168794337998, test loss: 0.48030251262560303\n",
      "Iteration 1529 training loss: 0.44126510385244333, test loss: 0.4796607946868873\n",
      "Iteration 1530 training loss: 0.4407357939646793, test loss: 0.48039578288114126\n",
      "Iteration 1531 training loss: 0.44082079054704804, test loss: 0.4792477868157072\n",
      "Iteration 1532 training loss: 0.44055634753792056, test loss: 0.4801917890205098\n",
      "Iteration 1533 training loss: 0.4402113224173132, test loss: 0.478668364899052\n",
      "Iteration 1534 training loss: 0.4401240786028263, test loss: 0.47973744913587235\n",
      "Iteration 1535 training loss: 0.43944919407608163, test loss: 0.47793641102442896\n",
      "Iteration 1536 training loss: 0.4394723828628774, test loss: 0.47906640167725845\n",
      "Iteration 1537 training loss: 0.43857437469717514, test loss: 0.47709041760764415\n",
      "Iteration 1538 training loss: 0.4386514391691586, test loss: 0.47823227831764437\n",
      "Iteration 1539 training loss: 0.437692519122193, test loss: 0.4762423605846552\n",
      "Iteration 1540 training loss: 0.4377726534910405, test loss: 0.47735387275925495\n",
      "Iteration 1541 training loss: 0.4368108585454795, test loss: 0.4753935358390222\n",
      "Iteration 1542 training loss: 0.43687914571130276, test loss: 0.4764684484752067\n",
      "Iteration 1543 training loss: 0.43600489902960915, test loss: 0.4746202819070788\n",
      "Iteration 1544 training loss: 0.4360098990501221, test loss: 0.4756159400942375\n",
      "Iteration 1545 training loss: 0.4352580569273391, test loss: 0.4739085393765513\n",
      "Iteration 1546 training loss: 0.4352135578406609, test loss: 0.47484463878019667\n",
      "Iteration 1547 training loss: 0.4346157714985361, test loss: 0.47329938931573706\n",
      "Iteration 1548 training loss: 0.43451219011839237, test loss: 0.4741717959232565\n",
      "Iteration 1549 training loss: 0.4340591381917366, test loss: 0.4727723771467835\n",
      "Iteration 1550 training loss: 0.43388350167523926, test loss: 0.47357575998097656\n",
      "Iteration 1551 training loss: 0.43361128959683554, test loss: 0.472351885756564\n",
      "Iteration 1552 training loss: 0.43337483004651683, test loss: 0.4731044478236862\n",
      "Iteration 1553 training loss: 0.4332955211246752, test loss: 0.4720598003495834\n",
      "Iteration 1554 training loss: 0.43295548104288856, test loss: 0.47272618826092666\n",
      "Iteration 1555 training loss: 0.43308411330576096, test loss: 0.47187009496551036\n",
      "Iteration 1556 training loss: 0.43262660542144055, test loss: 0.47244299460836425\n",
      "Iteration 1557 training loss: 0.4329578809461131, test loss: 0.4717626002996216\n",
      "Iteration 1558 training loss: 0.4323556612890758, test loss: 0.47221758520185314\n",
      "Iteration 1559 training loss: 0.4329195343741006, test loss: 0.47174145698030634\n",
      "Iteration 1560 training loss: 0.4321531964038841, test loss: 0.47206264566649786\n",
      "Iteration 1561 training loss: 0.43296273261127743, test loss: 0.4717994878667282\n",
      "Iteration 1562 training loss: 0.4320032088627599, test loss: 0.47195977704400865\n",
      "Iteration 1563 training loss: 0.43306985856392416, test loss: 0.47191838783946993\n",
      "Iteration 1564 training loss: 0.43188038081743574, test loss: 0.4718815490859581\n",
      "Iteration 1565 training loss: 0.4332209929534028, test loss: 0.4720797567496683\n",
      "Iteration 1566 training loss: 0.4317867925170661, test loss: 0.47183225928444855\n",
      "Iteration 1567 training loss: 0.43339933728488717, test loss: 0.47226511296828366\n",
      "Iteration 1568 training loss: 0.43169219265494657, test loss: 0.471778243691004\n",
      "Iteration 1569 training loss: 0.4335730251914781, test loss: 0.47244507394929514\n",
      "Iteration 1570 training loss: 0.4315927770343013, test loss: 0.47171731250884114\n",
      "Iteration 1571 training loss: 0.43372638134735275, test loss: 0.47260181850996485\n",
      "Iteration 1572 training loss: 0.4314670430178585, test loss: 0.4716251952163555\n",
      "Iteration 1573 training loss: 0.43383652589866534, test loss: 0.47271438734243676\n",
      "Iteration 1574 training loss: 0.43131342163685493, test loss: 0.47150218994469945\n",
      "Iteration 1575 training loss: 0.4338834477572948, test loss: 0.4727633812951656\n",
      "Iteration 1576 training loss: 0.4311391263286, test loss: 0.47135559068026234\n",
      "Iteration 1577 training loss: 0.4338812555143973, test loss: 0.47276251413766074\n",
      "Iteration 1578 training loss: 0.43096496131163414, test loss: 0.4712053325434699\n",
      "Iteration 1579 training loss: 0.4338646662273477, test loss: 0.47275029136074537\n",
      "Iteration 1580 training loss: 0.43079933779313917, test loss: 0.47106292505552\n",
      "Iteration 1581 training loss: 0.43381912147234386, test loss: 0.47270819645873224\n",
      "Iteration 1582 training loss: 0.4306512253466889, test loss: 0.47093564634178436\n",
      "Iteration 1583 training loss: 0.43377231591508597, test loss: 0.4726638797293202\n",
      "Iteration 1584 training loss: 0.43052542707177055, test loss: 0.47083041461560887\n",
      "Iteration 1585 training loss: 0.43369824317974853, test loss: 0.472592449035837\n",
      "Iteration 1586 training loss: 0.4304094605424669, test loss: 0.4707327410113885\n",
      "Iteration 1587 training loss: 0.4335911293662776, test loss: 0.47248653658107626\n",
      "Iteration 1588 training loss: 0.43031637912655846, test loss: 0.4706551813448703\n",
      "Iteration 1589 training loss: 0.4334980817645028, test loss: 0.4723971576254327\n",
      "Iteration 1590 training loss: 0.4302568560167651, test loss: 0.47061138698207117\n",
      "Iteration 1591 training loss: 0.4334013127434318, test loss: 0.47230293603237467\n",
      "Iteration 1592 training loss: 0.4302252720464411, test loss: 0.47059487374105213\n",
      "Iteration 1593 training loss: 0.43330569502879274, test loss: 0.47221064314686384\n",
      "Iteration 1594 training loss: 0.4302384391953396, test loss: 0.47062049800753086\n",
      "Iteration 1595 training loss: 0.4332272980449466, test loss: 0.4721370927685927\n",
      "Iteration 1596 training loss: 0.4302974776829434, test loss: 0.47069323021019643\n",
      "Iteration 1597 training loss: 0.43317349309432296, test loss: 0.47209031667520324\n",
      "Iteration 1598 training loss: 0.43039564297244287, test loss: 0.47080547480877305\n",
      "Iteration 1599 training loss: 0.43312547106329247, test loss: 0.47205044798855034\n",
      "Iteration 1600 training loss: 0.4305354006228341, test loss: 0.4709567248272695\n",
      "Iteration 1601 training loss: 0.4331010844690998, test loss: 0.47203602187775767\n",
      "Iteration 1602 training loss: 0.4307044427443481, test loss: 0.47113705222536156\n",
      "Iteration 1603 training loss: 0.43307478153600837, test loss: 0.47201939718812\n",
      "Iteration 1604 training loss: 0.43087100279331514, test loss: 0.4713098668924475\n",
      "Iteration 1605 training loss: 0.43301446028490737, test loss: 0.4719694055976379\n",
      "Iteration 1606 training loss: 0.4310070000535591, test loss: 0.4714448502838528\n",
      "Iteration 1607 training loss: 0.432921905948476, test loss: 0.4718904013909538\n",
      "Iteration 1608 training loss: 0.43111137671778677, test loss: 0.4715452947223415\n",
      "Iteration 1609 training loss: 0.43276621608569343, test loss: 0.4717489588478915\n",
      "Iteration 1610 training loss: 0.4311352799019034, test loss: 0.47156513902551767\n",
      "Iteration 1611 training loss: 0.4325480642934785, test loss: 0.47154903713257135\n",
      "Iteration 1612 training loss: 0.4311046360720408, test loss: 0.4715301615419236\n",
      "Iteration 1613 training loss: 0.4322628386971542, test loss: 0.47128360211860465\n",
      "Iteration 1614 training loss: 0.4309688844855298, test loss: 0.47139059606489236\n",
      "Iteration 1615 training loss: 0.43189168745098333, test loss: 0.47093603279801827\n",
      "Iteration 1616 training loss: 0.4307237520559635, test loss: 0.4711386265811238\n",
      "Iteration 1617 training loss: 0.43145518598108046, test loss: 0.47052496975401065\n",
      "Iteration 1618 training loss: 0.4304018909588333, test loss: 0.47081171410108563\n",
      "Iteration 1619 training loss: 0.43097367116624213, test loss: 0.47007199590890475\n",
      "Iteration 1620 training loss: 0.4300243973940916, test loss: 0.47042944302005224\n",
      "Iteration 1621 training loss: 0.43046949146942304, test loss: 0.4695972134854239\n",
      "Iteration 1622 training loss: 0.4296075232538357, test loss: 0.47001403636674177\n",
      "Iteration 1623 training loss: 0.4299525385539118, test loss: 0.46911003449795863\n",
      "Iteration 1624 training loss: 0.42915937964199485, test loss: 0.46956887161759703\n",
      "Iteration 1625 training loss: 0.42943150663344487, test loss: 0.4686184437830063\n",
      "Iteration 1626 training loss: 0.4286845290248042, test loss: 0.46909956703703426\n",
      "Iteration 1627 training loss: 0.42891194425207557, test loss: 0.46812848326126105\n",
      "Iteration 1628 training loss: 0.42820919993696205, test loss: 0.46863061670933814\n",
      "Iteration 1629 training loss: 0.4283901152848166, test loss: 0.4676316947234095\n",
      "Iteration 1630 training loss: 0.4277054082740467, test loss: 0.4681317512792868\n",
      "Iteration 1631 training loss: 0.4278715395566269, test loss: 0.46713820191914374\n",
      "Iteration 1632 training loss: 0.4272056552979856, test loss: 0.46764053130241107\n",
      "Iteration 1633 training loss: 0.4273763784516895, test loss: 0.4666668663256636\n",
      "Iteration 1634 training loss: 0.4267114361476644, test loss: 0.46715951918346227\n",
      "Iteration 1635 training loss: 0.42691122489005895, test loss: 0.4662259853554866\n",
      "Iteration 1636 training loss: 0.4262510451889669, test loss: 0.466717766461049\n",
      "Iteration 1637 training loss: 0.4265000638665692, test loss: 0.4658381330748486\n",
      "Iteration 1638 training loss: 0.4258342979878022, test loss: 0.46632293120567003\n",
      "Iteration 1639 training loss: 0.42615492242090536, test loss: 0.4655126246532439\n",
      "Iteration 1640 training loss: 0.42546055293360063, test loss: 0.4659726026482981\n",
      "Iteration 1641 training loss: 0.42587486156449955, test loss: 0.46524928944477634\n",
      "Iteration 1642 training loss: 0.4251296715830164, test loss: 0.46566886687160713\n",
      "Iteration 1643 training loss: 0.425656582531094, test loss: 0.46504350785980414\n",
      "Iteration 1644 training loss: 0.4248324002115405, test loss: 0.4654014052701849\n",
      "Iteration 1645 training loss: 0.4255122533416493, test loss: 0.46491176589501093\n",
      "Iteration 1646 training loss: 0.42459531247117865, test loss: 0.465197629059887\n",
      "Iteration 1647 training loss: 0.4254733173664553, test loss: 0.46488750062728457\n",
      "Iteration 1648 training loss: 0.42442349673653373, test loss: 0.46506261490034256\n",
      "Iteration 1649 training loss: 0.4255446685097562, test loss: 0.4649716766051466\n",
      "Iteration 1650 training loss: 0.42431787513874003, test loss: 0.4649983460557235\n",
      "Iteration 1651 training loss: 0.42573229270322427, test loss: 0.46517316341348314\n",
      "Iteration 1652 training loss: 0.4242878890204043, test loss: 0.4650102396607506\n",
      "Iteration 1653 training loss: 0.42604528246255585, test loss: 0.4655026879499599\n",
      "Iteration 1654 training loss: 0.42435637489707406, test loss: 0.4651177780160244\n",
      "Iteration 1655 training loss: 0.42654538375947093, test loss: 0.46602378339061923\n",
      "Iteration 1656 training loss: 0.42455825941330444, test loss: 0.4653573135140245\n",
      "Iteration 1657 training loss: 0.4272216337929111, test loss: 0.4667200513659461\n",
      "Iteration 1658 training loss: 0.42492701783415193, test loss: 0.46575518114873776\n",
      "Iteration 1659 training loss: 0.42812444311054393, test loss: 0.4676463265251994\n",
      "Iteration 1660 training loss: 0.4255108554348455, test loss: 0.4663602348975326\n",
      "Iteration 1661 training loss: 0.42919301471300725, test loss: 0.4687399311948805\n",
      "Iteration 1662 training loss: 0.4263376582249589, test loss: 0.46719460515891587\n",
      "Iteration 1663 training loss: 0.4304538720345104, test loss: 0.47002814945829796\n",
      "Iteration 1664 training loss: 0.427525976988343, test loss: 0.46838190668333907\n",
      "Iteration 1665 training loss: 0.431844114850021, test loss: 0.47144359225721555\n",
      "Iteration 1666 training loss: 0.4290612719685297, test loss: 0.4699003825738231\n",
      "Iteration 1667 training loss: 0.43331660628941726, test loss: 0.47294635976109256\n",
      "Iteration 1668 training loss: 0.4309915143972474, test loss: 0.4718031772278183\n",
      "Iteration 1669 training loss: 0.4347894763639328, test loss: 0.474448403273912\n",
      "Iteration 1670 training loss: 0.43313730520941435, test loss: 0.4739096314301537\n",
      "Iteration 1671 training loss: 0.43595795696563266, test loss: 0.475635829601804\n",
      "Iteration 1672 training loss: 0.43506618265042435, test loss: 0.4757794029664286\n",
      "Iteration 1673 training loss: 0.4365598336058003, test loss: 0.4762617080653564\n",
      "Iteration 1674 training loss: 0.4364300308255007, test loss: 0.47707412460278503\n",
      "Iteration 1675 training loss: 0.43650769023301533, test loss: 0.4762278191966516\n",
      "Iteration 1676 training loss: 0.4369107747716426, test loss: 0.47749145803937393\n",
      "Iteration 1677 training loss: 0.435733558172204, test loss: 0.47547405070167914\n",
      "Iteration 1678 training loss: 0.4364812147084852, test loss: 0.47699875517666895\n",
      "Iteration 1679 training loss: 0.43439902457276197, test loss: 0.47415443862245027\n",
      "Iteration 1680 training loss: 0.4352080386359958, test loss: 0.47567573132839136\n",
      "Iteration 1681 training loss: 0.43267477950526856, test loss: 0.4724427290969516\n",
      "Iteration 1682 training loss: 0.4333968660525577, test loss: 0.4738255165078799\n",
      "Iteration 1683 training loss: 0.4307569336787821, test loss: 0.47054121372647906\n",
      "Iteration 1684 training loss: 0.43133599680520934, test loss: 0.47174145920614174\n",
      "Iteration 1685 training loss: 0.4288846316774545, test loss: 0.4686922641924112\n",
      "Iteration 1686 training loss: 0.4293397428381895, test loss: 0.46973873313047404\n",
      "Iteration 1687 training loss: 0.4272164497442013, test loss: 0.46705647320720617\n",
      "Iteration 1688 training loss: 0.4275753511210074, test loss: 0.46798295876287704\n",
      "Iteration 1689 training loss: 0.4258032327088938, test loss: 0.46568034938780795\n",
      "Iteration 1690 training loss: 0.4260990855206716, test loss: 0.46652833974036995\n",
      "Iteration 1691 training loss: 0.4246511375685128, test loss: 0.46456555537049227\n",
      "Iteration 1692 training loss: 0.4248907501338335, test loss: 0.4653552387033713\n",
      "Iteration 1693 training loss: 0.4237422708377789, test loss: 0.4636955749416531\n",
      "Iteration 1694 training loss: 0.4239284877747568, test loss: 0.4644364029110951\n",
      "Iteration 1695 training loss: 0.4230560182713793, test loss: 0.46305067983566295\n",
      "Iteration 1696 training loss: 0.4232221712321268, test loss: 0.4637793974680323\n",
      "Iteration 1697 training loss: 0.42258058754424926, test loss: 0.46261077508785003\n",
      "Iteration 1698 training loss: 0.4226902093872003, test loss: 0.4632986502317379\n",
      "Iteration 1699 training loss: 0.42225458458521564, test loss: 0.46231812673173206\n",
      "Iteration 1700 training loss: 0.42231386555783884, test loss: 0.4629779999550152\n",
      "Iteration 1701 training loss: 0.4220836498351974, test loss: 0.4621784378411181\n",
      "Iteration 1702 training loss: 0.42206950133577886, test loss: 0.46279094877495375\n",
      "Iteration 1703 training loss: 0.4220354733636284, test loss: 0.4621583707135571\n",
      "Iteration 1704 training loss: 0.4219415598853193, test loss: 0.4627203335503512\n",
      "Iteration 1705 training loss: 0.4220982977690508, test loss: 0.46224656099785305\n",
      "Iteration 1706 training loss: 0.4218983799606488, test loss: 0.46273192658889695\n",
      "Iteration 1707 training loss: 0.4222646563522846, test loss: 0.46243592531344574\n",
      "Iteration 1708 training loss: 0.4219202129213773, test loss: 0.46280723960042713\n",
      "Iteration 1709 training loss: 0.42248553190473553, test loss: 0.4626780945252342\n",
      "Iteration 1710 training loss: 0.42196948568944326, test loss: 0.46290484601948156\n",
      "Iteration 1711 training loss: 0.4227250593235992, test loss: 0.4629346498403054\n",
      "Iteration 1712 training loss: 0.42201212897225876, test loss: 0.4629919819768196\n",
      "Iteration 1713 training loss: 0.4229748085792346, test loss: 0.46319735348186886\n",
      "Iteration 1714 training loss: 0.422044907815205, test loss: 0.4630637544463038\n",
      "Iteration 1715 training loss: 0.42322352107064537, test loss: 0.46345899103590327\n",
      "Iteration 1716 training loss: 0.4220560624947175, test loss: 0.46311056598811184\n",
      "Iteration 1717 training loss: 0.4234172638566333, test loss: 0.4636613877062244\n",
      "Iteration 1718 training loss: 0.42201718053406556, test loss: 0.46310001037640797\n",
      "Iteration 1719 training loss: 0.42354093062942716, test loss: 0.46379011925231906\n",
      "Iteration 1720 training loss: 0.4219121208215895, test loss: 0.4630183253383527\n",
      "Iteration 1721 training loss: 0.42359022842317134, test loss: 0.463845861947661\n",
      "Iteration 1722 training loss: 0.42174433022147556, test loss: 0.46287098130629994\n",
      "Iteration 1723 training loss: 0.42352865615472157, test loss: 0.4637879730379721\n",
      "Iteration 1724 training loss: 0.4215068396249581, test loss: 0.46264886400788985\n",
      "Iteration 1725 training loss: 0.42337522352838813, test loss: 0.46363871377975846\n",
      "Iteration 1726 training loss: 0.4212119782831386, test loss: 0.46236636260637803\n",
      "Iteration 1727 training loss: 0.42314182674891543, test loss: 0.4634107354326883\n",
      "Iteration 1728 training loss: 0.4208769607746821, test loss: 0.4620447329357158\n",
      "Iteration 1729 training loss: 0.4228526881916216, test loss: 0.4631298455963831\n",
      "Iteration 1730 training loss: 0.420533780765803, test loss: 0.4617166167085137\n",
      "Iteration 1731 training loss: 0.4225583126820571, test loss: 0.4628442065046821\n",
      "Iteration 1732 training loss: 0.42018988220215137, test loss: 0.46138821883588527\n",
      "Iteration 1733 training loss: 0.4222638014950912, test loss: 0.462560561629021\n",
      "Iteration 1734 training loss: 0.419883655715257, test loss: 0.4611024084233035\n",
      "Iteration 1735 training loss: 0.4220007437382474, test loss: 0.4623106035774211\n",
      "Iteration 1736 training loss: 0.419605075537409, test loss: 0.46084266091249326\n",
      "Iteration 1737 training loss: 0.4217640048968492, test loss: 0.46208607209051505\n",
      "Iteration 1738 training loss: 0.41935575547250076, test loss: 0.46061229840656753\n",
      "Iteration 1739 training loss: 0.4215524099989661, test loss: 0.4618874836045715\n",
      "Iteration 1740 training loss: 0.41913008429021537, test loss: 0.4604054955674844\n",
      "Iteration 1741 training loss: 0.4213767256436899, test loss: 0.4617256303677192\n",
      "Iteration 1742 training loss: 0.4189475263930815, test loss: 0.46024473914801495\n",
      "Iteration 1743 training loss: 0.42124756611958186, test loss: 0.46160823960039454\n",
      "Iteration 1744 training loss: 0.4188133942641088, test loss: 0.46013407082267305\n",
      "Iteration 1745 training loss: 0.4211687918504919, test loss: 0.461540224126298\n",
      "Iteration 1746 training loss: 0.41870870964982376, test loss: 0.4600520465495386\n",
      "Iteration 1747 training loss: 0.4211231923868249, test loss: 0.4615065843892607\n",
      "Iteration 1748 training loss: 0.4186378367363043, test loss: 0.46000476461714207\n",
      "Iteration 1749 training loss: 0.42110553245731147, test loss: 0.4615007172317407\n",
      "Iteration 1750 training loss: 0.41858629294222555, test loss: 0.45997429616847985\n",
      "Iteration 1751 training loss: 0.4211072798184724, test loss: 0.46151316891478356\n",
      "Iteration 1752 training loss: 0.4185506843771703, test loss: 0.45996037980099047\n",
      "Iteration 1753 training loss: 0.4211025073819043, test loss: 0.4615173537141971\n",
      "Iteration 1754 training loss: 0.41852590463525935, test loss: 0.45995503212162564\n",
      "Iteration 1755 training loss: 0.4210916182109437, test loss: 0.4615141408336505\n",
      "Iteration 1756 training loss: 0.4185043982568931, test loss: 0.45995133784400194\n",
      "Iteration 1757 training loss: 0.4210482969141695, test loss: 0.46147537162939617\n",
      "Iteration 1758 training loss: 0.4184855509864818, test loss: 0.4599488445582805\n",
      "Iteration 1759 training loss: 0.42101202466161675, test loss: 0.46144378663938534\n",
      "Iteration 1760 training loss: 0.41848401986356165, test loss: 0.4599635603618188\n",
      "Iteration 1761 training loss: 0.42097103220997867, test loss: 0.4614073662750133\n",
      "Iteration 1762 training loss: 0.41850862311488574, test loss: 0.46000483469991005\n",
      "Iteration 1763 training loss: 0.4209263903429031, test loss: 0.46136711076957426\n",
      "Iteration 1764 training loss: 0.4185489798680685, test loss: 0.4600599778985783\n",
      "Iteration 1765 training loss: 0.4208777547451549, test loss: 0.4613213336194234\n",
      "Iteration 1766 training loss: 0.41860163566989783, test loss: 0.460124522964715\n",
      "Iteration 1767 training loss: 0.420821058666687, test loss: 0.46126763192233555\n",
      "Iteration 1768 training loss: 0.41867853746400785, test loss: 0.4602120682036356\n",
      "Iteration 1769 training loss: 0.4207722245114246, test loss: 0.4612238788325064\n",
      "Iteration 1770 training loss: 0.41879410954438195, test loss: 0.4603391884803108\n",
      "Iteration 1771 training loss: 0.42074089919654334, test loss: 0.46119883707286047\n",
      "Iteration 1772 training loss: 0.4189476596691567, test loss: 0.46050310667238464\n",
      "Iteration 1773 training loss: 0.42073702389089984, test loss: 0.4612015551157169\n",
      "Iteration 1774 training loss: 0.41912503163164594, test loss: 0.460690801535965\n",
      "Iteration 1775 training loss: 0.42073359461651366, test loss: 0.4612080110683488\n",
      "Iteration 1776 training loss: 0.4193075008281529, test loss: 0.46088055706391207\n",
      "Iteration 1777 training loss: 0.4207398905644958, test loss: 0.4612266143026611\n",
      "Iteration 1778 training loss: 0.4195125211067152, test loss: 0.4610929069402048\n",
      "Iteration 1779 training loss: 0.42074976840617606, test loss: 0.4612471511274979\n",
      "Iteration 1780 training loss: 0.41970392363491044, test loss: 0.4612866209823312\n",
      "Iteration 1781 training loss: 0.42072988765187835, test loss: 0.461238564203215\n",
      "Iteration 1782 training loss: 0.4198728832059208, test loss: 0.461455633549383\n",
      "Iteration 1783 training loss: 0.4206893323947626, test loss: 0.4612111500812683\n",
      "Iteration 1784 training loss: 0.4199797145246085, test loss: 0.4615610324889295\n",
      "Iteration 1785 training loss: 0.4205871261101575, test loss: 0.4611234115781945\n",
      "Iteration 1786 training loss: 0.42000550584305274, test loss: 0.46158352041830575\n",
      "Iteration 1787 training loss: 0.42040412614015, test loss: 0.460955914585712\n",
      "Iteration 1788 training loss: 0.41991303676757696, test loss: 0.4614856791123807\n",
      "Iteration 1789 training loss: 0.4201143435684034, test loss: 0.460683597778844\n",
      "Iteration 1790 training loss: 0.41968400352231405, test loss: 0.4612507354998894\n",
      "Iteration 1791 training loss: 0.4197229862685068, test loss: 0.46031529064366733\n",
      "Iteration 1792 training loss: 0.4193421172766941, test loss: 0.46089750100502735\n",
      "Iteration 1793 training loss: 0.41925737843903166, test loss: 0.45987108653407816\n",
      "Iteration 1794 training loss: 0.41892218923679253, test loss: 0.4604666138010325\n",
      "Iteration 1795 training loss: 0.41873337263334565, test loss: 0.45937092615330455\n",
      "Iteration 1796 training loss: 0.4184249795831794, test loss: 0.4599625710901251\n",
      "Iteration 1797 training loss: 0.4181698412192081, test loss: 0.4588355436186097\n",
      "Iteration 1798 training loss: 0.4178832277720429, test loss: 0.45941829891104535\n",
      "Iteration 1799 training loss: 0.4175835738042116, test loss: 0.45827812622261666\n",
      "Iteration 1800 training loss: 0.4173212986963003, test loss: 0.4588535912281039\n",
      "Iteration 1801 training loss: 0.4169981094709, test loss: 0.4577244127592681\n",
      "Iteration 1802 training loss: 0.416766730367022, test loss: 0.45830251654666865\n",
      "Iteration 1803 training loss: 0.4164426272747905, test loss: 0.4572022732771421\n",
      "Iteration 1804 training loss: 0.41624889607732934, test loss: 0.45778674294324784\n",
      "Iteration 1805 training loss: 0.4159393411167376, test loss: 0.45673275257630014\n",
      "Iteration 1806 training loss: 0.4157598799965498, test loss: 0.4573021922057221\n",
      "Iteration 1807 training loss: 0.41545465168165985, test loss: 0.45627960577609666\n",
      "Iteration 1808 training loss: 0.41529395421069804, test loss: 0.456839991233107\n",
      "Iteration 1809 training loss: 0.41500828108807936, test loss: 0.45586671251440347\n",
      "Iteration 1810 training loss: 0.4148626808910368, test loss: 0.45641509866514296\n",
      "Iteration 1811 training loss: 0.4145907778215932, test loss: 0.45548433295772084\n",
      "Iteration 1812 training loss: 0.4144579123643616, test loss: 0.45601745786797865\n",
      "Iteration 1813 training loss: 0.41420812378153743, test loss: 0.45513657963020004\n",
      "Iteration 1814 training loss: 0.4140974220725572, test loss: 0.45566254449319576\n",
      "Iteration 1815 training loss: 0.41388719392827916, test loss: 0.45485254266580816\n",
      "Iteration 1816 training loss: 0.41378369889443345, test loss: 0.45535531485626135\n",
      "Iteration 1817 training loss: 0.4136175776926856, test loss: 0.4546207902807759\n",
      "Iteration 1818 training loss: 0.41352144504475075, test loss: 0.4551009934327316\n",
      "Iteration 1819 training loss: 0.41340373567061445, test loss: 0.45444716393761675\n",
      "Iteration 1820 training loss: 0.4132990593396138, test loss: 0.45488461578518175\n",
      "Iteration 1821 training loss: 0.41325078144045724, test loss: 0.4543367467125992\n",
      "Iteration 1822 training loss: 0.413130185011904, test loss: 0.45471909495731644\n",
      "Iteration 1823 training loss: 0.41317052794745457, test loss: 0.4543023845047932\n",
      "Iteration 1824 training loss: 0.4130323900918144, test loss: 0.45462050671579035\n",
      "Iteration 1825 training loss: 0.41319952987914316, test loss: 0.45438212289533203\n",
      "Iteration 1826 training loss: 0.41302704064198587, test loss: 0.4546156983729705\n",
      "Iteration 1827 training loss: 0.41337441649363965, test loss: 0.45461500592287457\n",
      "Iteration 1828 training loss: 0.41314565319738816, test loss: 0.4547334115954782\n",
      "Iteration 1829 training loss: 0.4137507700455246, test loss: 0.4550605447893293\n",
      "Iteration 1830 training loss: 0.41343802459207873, test loss: 0.455024278384798\n",
      "Iteration 1831 training loss: 0.41442798310292017, test loss: 0.45582141345388283\n",
      "Iteration 1832 training loss: 0.41401249381349076, test loss: 0.45559309667858433\n",
      "Iteration 1833 training loss: 0.4155545794970282, test loss: 0.4570448510803175\n",
      "Iteration 1834 training loss: 0.41502789594690237, test loss: 0.4565973076893098\n",
      "Iteration 1835 training loss: 0.41734190395326615, test loss: 0.45894561232981457\n",
      "Iteration 1836 training loss: 0.41678552198844954, test loss: 0.45835356928574544\n",
      "Iteration 1837 training loss: 0.42003433721278377, test loss: 0.4617665347444904\n",
      "Iteration 1838 training loss: 0.41950821647327735, test loss: 0.46108589720699455\n",
      "Iteration 1839 training loss: 0.42352858343377064, test loss: 0.4653819348346593\n",
      "Iteration 1840 training loss: 0.42330129995114496, test loss: 0.4648890050772205\n",
      "Iteration 1841 training loss: 0.4273799301686346, test loss: 0.46932104414559306\n",
      "Iteration 1842 training loss: 0.4276499820507754, test loss: 0.4692587910152447\n",
      "Iteration 1843 training loss: 0.43028550358842454, test loss: 0.47224447178794215\n",
      "Iteration 1844 training loss: 0.4308114320392076, test loss: 0.4724089675547675\n",
      "Iteration 1845 training loss: 0.4308346796625185, test loss: 0.47274287473583\n",
      "Iteration 1846 training loss: 0.4312951971594784, test loss: 0.4728102788431258\n",
      "Iteration 1847 training loss: 0.4288256073036042, test loss: 0.470631067242479\n",
      "Iteration 1848 training loss: 0.4290283832827504, test loss: 0.4704244073980387\n",
      "Iteration 1849 training loss: 0.42532449100217307, test loss: 0.467016109420312\n",
      "Iteration 1850 training loss: 0.42545131083990423, test loss: 0.4667425153019342\n",
      "Iteration 1851 training loss: 0.4216317447132176, test loss: 0.4632325357904183\n",
      "Iteration 1852 training loss: 0.4218191404490187, test loss: 0.4630415650527883\n",
      "Iteration 1853 training loss: 0.41842209719312745, test loss: 0.45998035343100147\n",
      "Iteration 1854 training loss: 0.41879989454227373, test loss: 0.45999825635938324\n",
      "Iteration 1855 training loss: 0.41592583985084813, test loss: 0.4574888564739178\n",
      "Iteration 1856 training loss: 0.41652085429760316, test loss: 0.45773166583971725\n",
      "Iteration 1857 training loss: 0.41410911965210895, test loss: 0.45570453137045713\n",
      "Iteration 1858 training loss: 0.4148876359184446, test loss: 0.4561243074818015\n",
      "Iteration 1859 training loss: 0.412801049980226, test loss: 0.45444230194026547\n",
      "Iteration 1860 training loss: 0.4137315248071961, test loss: 0.4550042821120657\n",
      "Iteration 1861 training loss: 0.41189483363902935, test loss: 0.45359707020926227\n",
      "Iteration 1862 training loss: 0.4129858942435168, test loss: 0.4543019291942365\n",
      "Iteration 1863 training loss: 0.41129408234644493, test loss: 0.4530620773204373\n",
      "Iteration 1864 training loss: 0.41251758788313525, test loss: 0.4538751092292743\n",
      "Iteration 1865 training loss: 0.41090862238178416, test loss: 0.45273528107485744\n",
      "Iteration 1866 training loss: 0.41228592854442253, test loss: 0.4536792686987087\n",
      "Iteration 1867 training loss: 0.41070697825494873, test loss: 0.4525909473512544\n",
      "Iteration 1868 training loss: 0.41224628903050514, test loss: 0.4536734825905059\n",
      "Iteration 1869 training loss: 0.4106178360064209, test loss: 0.4525583042344359\n",
      "Iteration 1870 training loss: 0.4123250814249461, test loss: 0.45378813828107295\n",
      "Iteration 1871 training loss: 0.410646785410141, test loss: 0.4526401167802328\n",
      "Iteration 1872 training loss: 0.41250768539902916, test loss: 0.45400114978541534\n",
      "Iteration 1873 training loss: 0.41072003060988177, test loss: 0.45275816927717977\n",
      "Iteration 1874 training loss: 0.41274077846927526, test loss: 0.45426178294078945\n",
      "Iteration 1875 training loss: 0.4108293058275034, test loss: 0.4529059446223427\n",
      "Iteration 1876 training loss: 0.4129942140336154, test loss: 0.4545409001630071\n",
      "Iteration 1877 training loss: 0.4109300720415274, test loss: 0.4530371081116964\n",
      "Iteration 1878 training loss: 0.4132124598055938, test loss: 0.4547817973172301\n",
      "Iteration 1879 training loss: 0.4109892610555775, test loss: 0.4531239948419726\n",
      "Iteration 1880 training loss: 0.4133486081661788, test loss: 0.4549385395962648\n",
      "Iteration 1881 training loss: 0.4109915571104752, test loss: 0.4531483859417598\n",
      "Iteration 1882 training loss: 0.4133922144049594, test loss: 0.45500008571893047\n",
      "Iteration 1883 training loss: 0.41094250168821805, test loss: 0.45311831697201316\n",
      "Iteration 1884 training loss: 0.4133513371776023, test loss: 0.454976151596989\n",
      "Iteration 1885 training loss: 0.41083704535477417, test loss: 0.45302713928686633\n",
      "Iteration 1886 training loss: 0.4132261227904184, test loss: 0.45486689758834764\n",
      "Iteration 1887 training loss: 0.41069878947777005, test loss: 0.45290319554428704\n",
      "Iteration 1888 training loss: 0.4130621239090052, test loss: 0.45471959261692535\n",
      "Iteration 1889 training loss: 0.41053755897949595, test loss: 0.4527546871346593\n",
      "Iteration 1890 training loss: 0.4128626989327572, test loss: 0.45453835215750077\n",
      "Iteration 1891 training loss: 0.4103518927514164, test loss: 0.4525801442088514\n",
      "Iteration 1892 training loss: 0.41263821707890996, test loss: 0.45433268744588884\n",
      "Iteration 1893 training loss: 0.4101526616899025, test loss: 0.45239046375809594\n",
      "Iteration 1894 training loss: 0.4123957598918451, test loss: 0.45411030179775896\n",
      "Iteration 1895 training loss: 0.40994431531110986, test loss: 0.4521943331627166\n",
      "Iteration 1896 training loss: 0.4121334893816827, test loss: 0.4538684856400099\n",
      "Iteration 1897 training loss: 0.4097227538500293, test loss: 0.4519866015744298\n",
      "Iteration 1898 training loss: 0.4118686882293163, test loss: 0.45362529329571527\n",
      "Iteration 1899 training loss: 0.4095082608661942, test loss: 0.45178485755065384\n",
      "Iteration 1900 training loss: 0.4116129439783896, test loss: 0.4533916759037026\n",
      "Iteration 1901 training loss: 0.4092980695963685, test loss: 0.4515883911008062\n",
      "Iteration 1902 training loss: 0.41136978266896035, test loss: 0.45317146762906557\n",
      "Iteration 1903 training loss: 0.4091043291623263, test loss: 0.45140988527306347\n",
      "Iteration 1904 training loss: 0.41114693689568155, test loss: 0.452973106434976\n",
      "Iteration 1905 training loss: 0.4089308920743065, test loss: 0.4512506511855275\n",
      "Iteration 1906 training loss: 0.410950615910635, test loss: 0.4528015778745904\n",
      "Iteration 1907 training loss: 0.4087788455895825, test loss: 0.45111379707344557\n",
      "Iteration 1908 training loss: 0.4107928649005967, test loss: 0.45266992666299055\n",
      "Iteration 1909 training loss: 0.4086570772427356, test loss: 0.451008233112396\n",
      "Iteration 1910 training loss: 0.41066533474039496, test loss: 0.4525669401691467\n",
      "Iteration 1911 training loss: 0.40854290822511186, test loss: 0.4509090792605528\n",
      "Iteration 1912 training loss: 0.4105426189599494, test loss: 0.45246896917389934\n",
      "Iteration 1913 training loss: 0.408431126135014, test loss: 0.45081118452600494\n",
      "Iteration 1914 training loss: 0.41042857233303603, test loss: 0.4523791627085089\n",
      "Iteration 1915 training loss: 0.4083294165540069, test loss: 0.4507227614654572\n",
      "Iteration 1916 training loss: 0.41032583371060155, test loss: 0.4523011792901213\n",
      "Iteration 1917 training loss: 0.40823603755690285, test loss: 0.45064247269939134\n",
      "Iteration 1918 training loss: 0.41023006770575593, test loss: 0.4522292069222197\n",
      "Iteration 1919 training loss: 0.4081404522357042, test loss: 0.45055945695798744\n",
      "Iteration 1920 training loss: 0.4101242656730646, test loss: 0.45214745090330305\n",
      "Iteration 1921 training loss: 0.40804525872969566, test loss: 0.4504766124597082\n",
      "Iteration 1922 training loss: 0.41001663290384166, test loss: 0.4520632790116649\n",
      "Iteration 1923 training loss: 0.4079519893697355, test loss: 0.4503945627894789\n",
      "Iteration 1924 training loss: 0.40990941822572924, test loss: 0.4519792540155745\n",
      "Iteration 1925 training loss: 0.4078591630167719, test loss: 0.45031286888438427\n",
      "Iteration 1926 training loss: 0.4098011826981524, test loss: 0.4518948766134645\n",
      "Iteration 1927 training loss: 0.4077707706184165, test loss: 0.45023646597300576\n",
      "Iteration 1928 training loss: 0.4096977467558147, test loss: 0.4518152098557464\n",
      "Iteration 1929 training loss: 0.40769281171523347, test loss: 0.45016930326896015\n",
      "Iteration 1930 training loss: 0.4096037526390647, test loss: 0.4517454494964028\n",
      "Iteration 1931 training loss: 0.4076119954252036, test loss: 0.4500980233850985\n",
      "Iteration 1932 training loss: 0.4094922969766386, test loss: 0.45165721935907177\n",
      "Iteration 1933 training loss: 0.407519839339013, test loss: 0.45001470331702814\n",
      "Iteration 1934 training loss: 0.4093576569939391, test loss: 0.4515449238203006\n",
      "Iteration 1935 training loss: 0.4074248338026418, test loss: 0.4499277231771459\n",
      "Iteration 1936 training loss: 0.4092287331429963, test loss: 0.4514392929615565\n",
      "Iteration 1937 training loss: 0.4073503637361309, test loss: 0.4498626118674702\n",
      "Iteration 1938 training loss: 0.4091129346483871, test loss: 0.45134759884696896\n",
      "Iteration 1939 training loss: 0.4072834400845681, test loss: 0.44980389121585496\n",
      "Iteration 1940 training loss: 0.40899534484042294, test loss: 0.4512538999379809\n",
      "Iteration 1941 training loss: 0.40722289880513707, test loss: 0.44975185646741456\n",
      "Iteration 1942 training loss: 0.40888320524283794, test loss: 0.45116772950955836\n",
      "Iteration 1943 training loss: 0.40718745404387846, test loss: 0.4497248463979376\n",
      "Iteration 1944 training loss: 0.4087915850096616, test loss: 0.45110237176953577\n",
      "Iteration 1945 training loss: 0.40716904966339873, test loss: 0.44971449645857275\n",
      "Iteration 1946 training loss: 0.4087097525470092, test loss: 0.45104598337267743\n",
      "Iteration 1947 training loss: 0.4071615137192058, test loss: 0.4497151671927185\n",
      "Iteration 1948 training loss: 0.4086291743137834, test loss: 0.4509898404425999\n",
      "Iteration 1949 training loss: 0.4071630833530724, test loss: 0.4497227812967685\n",
      "Iteration 1950 training loss: 0.408541769000358, test loss: 0.45092580325090176\n",
      "Iteration 1951 training loss: 0.40716886751216735, test loss: 0.44973487880582413\n",
      "Iteration 1952 training loss: 0.40845902753800784, test loss: 0.4508677889337352\n",
      "Iteration 1953 training loss: 0.4071996531191123, test loss: 0.44977281431454014\n",
      "Iteration 1954 training loss: 0.408385782566215, test loss: 0.45081937476523465\n",
      "Iteration 1955 training loss: 0.4072488776343568, test loss: 0.44982989249741534\n",
      "Iteration 1956 training loss: 0.40830486506800345, test loss: 0.4507629995371903\n",
      "Iteration 1957 training loss: 0.4072844592188056, test loss: 0.44987253717098347\n",
      "Iteration 1958 training loss: 0.4082102410407276, test loss: 0.45069296240706963\n",
      "Iteration 1959 training loss: 0.4073318941550819, test loss: 0.44992513622927616\n",
      "Iteration 1960 training loss: 0.40811175791620413, test loss: 0.4506206135118091\n",
      "Iteration 1961 training loss: 0.40736995456740405, test loss: 0.4499692842044068\n",
      "Iteration 1962 training loss: 0.4079944189717112, test loss: 0.45052988781738995\n",
      "Iteration 1963 training loss: 0.4074170923380378, test loss: 0.4500238954508474\n",
      "Iteration 1964 training loss: 0.40786902531823793, test loss: 0.45043096439556596\n",
      "Iteration 1965 training loss: 0.40746132473405355, test loss: 0.4500754918273073\n",
      "Iteration 1966 training loss: 0.40776256515120196, test loss: 0.4503509433301366\n",
      "Iteration 1967 training loss: 0.40752166954717467, test loss: 0.45014540970283234\n",
      "Iteration 1968 training loss: 0.4076536444476791, test loss: 0.4502693326897318\n",
      "Iteration 1969 training loss: 0.4075948756470352, test loss: 0.4502273958039029\n",
      "Iteration 1970 training loss: 0.4075354993678272, test loss: 0.4501798113129961\n",
      "Iteration 1971 training loss: 0.4076530330517769, test loss: 0.4502954775955479\n",
      "Iteration 1972 training loss: 0.4074163088774065, test loss: 0.450090124802934\n",
      "Iteration 1973 training loss: 0.40772159659313756, test loss: 0.4503760150420398\n",
      "Iteration 1974 training loss: 0.407309309926836, test loss: 0.4500140001846774\n",
      "Iteration 1975 training loss: 0.4077862471505244, test loss: 0.45045496210523006\n",
      "Iteration 1976 training loss: 0.40719552306010226, test loss: 0.4499303740528232\n",
      "Iteration 1977 training loss: 0.4078460175634751, test loss: 0.4505288545357682\n",
      "Iteration 1978 training loss: 0.4070706301927775, test loss: 0.44983082496041027\n",
      "Iteration 1979 training loss: 0.40785681345222996, test loss: 0.4505523171117837\n",
      "Iteration 1980 training loss: 0.4069149789440364, test loss: 0.44969901601079454\n",
      "Iteration 1981 training loss: 0.4078046975802094, test loss: 0.45051008124868613\n",
      "Iteration 1982 training loss: 0.4067250524162288, test loss: 0.4495326254598696\n",
      "Iteration 1983 training loss: 0.4076969303928181, test loss: 0.4504141947545296\n",
      "Iteration 1984 training loss: 0.40650063212587373, test loss: 0.44933153165856654\n",
      "Iteration 1985 training loss: 0.4075513034251207, test loss: 0.4502834974632549\n",
      "Iteration 1986 training loss: 0.4062787693267988, test loss: 0.44913453913910534\n",
      "Iteration 1987 training loss: 0.4074023850045529, test loss: 0.45015262857085225\n",
      "Iteration 1988 training loss: 0.4060904895905794, test loss: 0.4489743622569619\n",
      "Iteration 1989 training loss: 0.4072677335893788, test loss: 0.45003617669395296\n",
      "Iteration 1990 training loss: 0.40592512113691015, test loss: 0.44883757276570324\n",
      "Iteration 1991 training loss: 0.40713593763494643, test loss: 0.44992324682433193\n",
      "Iteration 1992 training loss: 0.4057996821015364, test loss: 0.4487410300483537\n",
      "Iteration 1993 training loss: 0.4070288522935447, test loss: 0.4498352134548552\n",
      "Iteration 1994 training loss: 0.4057211164488528, test loss: 0.44869239958321966\n",
      "Iteration 1995 training loss: 0.40695430420952333, test loss: 0.4497815994494989\n",
      "Iteration 1996 training loss: 0.4057008281707737, test loss: 0.44870320392837837\n",
      "Iteration 1997 training loss: 0.4069290176870824, test loss: 0.4497787469281605\n",
      "Iteration 1998 training loss: 0.4057700224031901, test loss: 0.44880477846906813\n",
      "Iteration 1999 training loss: 0.4069550976002859, test loss: 0.4498275816193078\n",
      "Iteration 2000 training loss: 0.4059330027356944, test loss: 0.44900148427752445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7492131db110>,\n",
       " <matplotlib.lines.Line2D at 0x7492131dd520>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### START CODE HERE ### (≈ 10 lines of code)\n",
    "layer_sizes = (features_train.shape[1], 128, 10)\n",
    "params = init_params(layer_sizes)\n",
    "num_iters = 2000\n",
    "learning_rate = 2e-6\n",
    "losses_train, losses_test = [], []\n",
    "# Optimization loop\n",
    "for i in range(num_iters):\n",
    "    preds_train, cache = forward(features_train, params)\n",
    "    preds_test, _ = forward(features_test, params)\n",
    "    loss_train = ce_loss(preds_train, labels_train)\n",
    "    loss_test = ce_loss(preds_test, labels_test)\n",
    "    print(f\"Iteration {i+1} training loss: {loss_train}, test loss: {loss_test}\")\n",
    "    losses_train.append(loss_train)\n",
    "    losses_test.append(loss_test)\n",
    "    grads = grad(preds_train, labels_train, params, cache)\n",
    "    for j in range(len(layer_sizes) -1):\n",
    "        params['W'+str(j+1)] = params['W'+str(j+1)] - learning_rate * grads['dW'+str(j+1)]\n",
    "        params['b'+str(j+1)] = params['b'+str(j+1)] - learning_rate * grads['db'+str(j+1)]\n",
    "### END CODE HERE ###\n",
    "\n",
    "plt.plot(range(num_iters), losses_train, 'b--', range(num_iters), losses_test, 'r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "Besides cross entropy loss, accuracy is usually used to evaluate the performance of a model. We can predict an image's class based on the index of the maximum value in the prediction.\n",
    "### $\\color{violet}{\\textbf{(10\\%) Exercise 7: Accuracy Evaluation}}$\n",
    "- **Bring test accuracy up to be higher than 80%**\n",
    "- You'll find the index of the maximum value in an array using `np.argmax()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 8 lines of code)\n",
    "# Evaluate with train\n",
    "categorize_pred_train = None  # Get indices of max values in Yhat_train row-wise\n",
    "correct_train = None  # Find out which predictions are correct\n",
    "num_correct_train = None  # Calculate how many correct predictions are made\n",
    "accuracy_train = None  # Calculate accuracy rate: correct # / total #\n",
    "# Evaluate with test\n",
    "categorize_pred_test = None  # # Get indices of max values in Yhat_test row-wise\n",
    "correct_test = None  # Find out which predictions are correct\n",
    "num_correct_test = None  # Calculate how many correct predictions are made\n",
    "accuracy_test = None  # Calculate accuracy rate: correct # / total #\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(f\"prediction accuracy on train set: {accuracy_train}\")\n",
    "print(f\"prediction accuracy on test set: {accuracy_test}\")\n",
    "\n",
    "# Compare prediction and target\n",
    "figure = plt.figure(figsize=(10, 10))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = np.random.randint(0, len(test_set))\n",
    "    img, label_true = test_set[sample_idx]\n",
    "    label_pred = categorize_pred_test[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(f\"true: {category_keys[label_true]}, pred: {category_keys[label_pred]}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Congrats on finishing this assignment! "
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "3321",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
